# verl å®æˆ˜å­¦ä¹ æŒ‡å—

æœ¬æ–‡æ¡£ä¸º verl é¡¹ç›®çš„**åº”ç”¨å±‚é¢**å­¦ä¹ è·¯çº¿ï¼Œä¸“æ³¨äºæ•°æ®å‡†å¤‡ã€RL ç®—æ³•ä½¿ç”¨å’Œ Agent è®­ç»ƒï¼Œè·³è¿‡åº•å±‚åˆ†å¸ƒå¼å®ç°ç»†èŠ‚ã€‚

é…åˆå®˜æ–¹æ–‡æ¡£ https://verl.readthedocs.io/en/latest/index.html ä½¿ç”¨ã€‚

---

## ç›®å½•

- [å­¦ä¹ ç›®æ ‡](#å­¦ä¹ ç›®æ ‡)
- [å¿«é€Ÿä¸Šæ‰‹ï¼ˆ1å¤©ï¼‰](#å¿«é€Ÿä¸Šæ‰‹1å¤©)
- [æ•°æ®å‡†å¤‡ï¼ˆ1-2å¤©ï¼‰](#æ•°æ®å‡†å¤‡1-2å¤©)
- [RL ç®—æ³•å®æˆ˜ï¼ˆ2-3å¤©ï¼‰](#rl-ç®—æ³•å®æˆ˜2-3å¤©)
- [Agent RL è®­ç»ƒï¼ˆ3-5å¤©ï¼‰](#agent-rl-è®­ç»ƒ3-5å¤©)
- [è¿›é˜¶æŠ€å·§](#è¿›é˜¶æŠ€å·§)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬æŒ‡å—åï¼Œä½ å°†èƒ½å¤Ÿï¼š

âœ… å‡†å¤‡å’Œå¤„ç† RL è®­ç»ƒæ•°æ®
âœ… ä½¿ç”¨ä¸åŒçš„ RL ç®—æ³•ï¼ˆPPOã€GRPOã€RLOO ç­‰ï¼‰è¿›è¡Œè®­ç»ƒ
âœ… è®¾è®¡å’Œå®ç°è‡ªå®šä¹‰çš„ Reward å‡½æ•°
âœ… è®­ç»ƒå¤šè½®å¯¹è¯å’Œå·¥å…·è°ƒç”¨çš„ Agent
âœ… è°ƒä¼˜è®­ç»ƒå‚æ•°è·å¾—æ›´å¥½æ•ˆæœ

---

## ğŸ¯ å­¦ä¹ å±‚æ¬¡è¯´æ˜

æœ¬æŒ‡å—åˆ†ä¸ºä¸¤ä¸ªå±‚æ¬¡ï¼š

### åº”ç”¨å±‚ï¼ˆç¬¬ 1-5 èŠ‚ï¼‰
- å¿«é€Ÿä¸Šæ‰‹ã€æ•°æ®å‡†å¤‡ã€ç®—æ³•ä½¿ç”¨ã€å‚æ•°è°ƒä¼˜
- é¢å‘ç”¨æˆ·çš„å®æˆ˜æ“ä½œ
- ä¸éœ€è¦äº†è§£åº•å±‚å®ç°

### åŸç†å±‚ï¼ˆç¬¬ 6-7 èŠ‚ï¼‰â­ æ–°å¢
- è®­ç»ƒæµç¨‹æ·±åº¦è§£æï¼ˆRayPPOTrainerï¼‰
- Reward ç³»ç»Ÿæ¶æ„æ·±åº¦è§£æ
- å¤šæ–‡ä»¶åä½œåŸç†
- é¢å‘æƒ³æ·±å…¥ç†è§£ä»£ç çš„å¼€å‘è€…

---

## å¿«é€Ÿä¸Šæ‰‹ï¼ˆ1å¤©ï¼‰

### ç›®æ ‡
è·‘é€šç¬¬ä¸€ä¸ªç¤ºä¾‹ï¼Œå»ºç«‹ç›´è§‚æ„Ÿè§‰ã€‚

### 1.1 å®‰è£…ç¯å¢ƒ

```bash
# å®‰è£… verl + vLLM
pip install -e .[test,vllm]

# æˆ–è€…å®‰è£… SGLangï¼ˆæ¨èç”¨äº Agentï¼‰
pip install -e .[test,sglang]
```

### 1.2 ä¸‹è½½æ¨¡å‹å’Œæ•°æ®

```bash
# ä¸‹è½½å°æ¨¡å‹ï¼ˆ7Bï¼Œé€‚åˆå…¥é—¨ï¼‰
huggingface-cli download Qwen/Qwen2.5-7B-Instruct --local-dir ~/models/Qwen2.5-7B-Instruct

# å‡†å¤‡ GSM8K æ•°æ®
python examples/data_preprocess/gsm8k.py --local_dir ~/data/gsm8k
```

### 1.3 è¿è¡Œç¬¬ä¸€ä¸ªè®­ç»ƒ

```bash
# GRPO è®­ç»ƒï¼ˆæœ€ç®€å•çš„ RL ç®—æ³•ï¼‰
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    data.train_files="['~/data/gsm8k/train.parquet']" \
    data.val_files="['~/data/gsm8k/test.parquet']" \
    actor_rollout_ref.model.path=~/models/Qwen2.5-7B-Instruct \
    actor_rollout_ref.rollout.name=vllm \
    trainer.total_epochs=3 \
    trainer.logger='["tensorboard"]'
```

**è§‚å¯Ÿè¾“å‡ºæŒ‡æ ‡ï¼š**
- `reward/mean` - å¹³å‡å¥–åŠ±ï¼Œåº”è¯¥é€æ­¥ä¸Šå‡
- `accuracy` - å‡†ç¡®ç‡ï¼ˆå¯¹äº GSM8Kï¼‰
- `response_length/mean` - å¹³å‡å“åº”é•¿åº¦

### 1.4 æŸ¥çœ‹è®­ç»ƒæ›²çº¿

```bash
tensorboard --logdir=./outputs
# æµè§ˆå™¨æ‰“å¼€ http://localhost:6006
```

---

## æ•°æ®å‡†å¤‡ï¼ˆ1-2å¤©ï¼‰

### 2.1 ç†è§£æ•°æ®æ ¼å¼

**å®˜æ–¹æ–‡æ¡£ï¼š** https://verl.readthedocs.io/en/latest/preparation/prepare_data.html

verl ä½¿ç”¨ **Parquet** æ ¼å¼ï¼Œå¿…é¡»åŒ…å«çš„å­—æ®µï¼š

```python
{
    "data_source": "gsm8k",           # æ•°æ®æ¥æºæ ‡è¯†
    "prompt": "é—®é¢˜å†…å®¹...",          # å¿…éœ€ï¼šè¾“å…¥æç¤º
    "ability": "math",                # å¯é€‰ï¼šèƒ½åŠ›ç±»å‹
    "reward_model": {                 # å¯é€‰ï¼šç”¨äº reward è®¡ç®—
        "ground_truth": "42",         # æ ‡å‡†ç­”æ¡ˆ
        "style": "short",             # ç­”æ¡ˆé£æ ¼
    }
}
```

### 2.2 æŸ¥çœ‹å†…ç½®æ•°æ®é›†ç¤ºä¾‹

```bash
# æŸ¥çœ‹æ•°æ®é¢„å¤„ç†è„šæœ¬
ls examples/data_preprocess/

# å¸¸ç”¨æ•°æ®é›†
gsm8k.py                  # GSM8K æ•°å­¦é¢˜
math_dataset.py           # MATH æ•°æ®é›†
geo3k.py                  # å‡ ä½•é¢˜
gsm8k_multiturn_w_tool.py # GSM8K + å·¥å…·è°ƒç”¨
```

### 2.3 å‡†å¤‡è‡ªå·±çš„æ•°æ®é›†

#### å•è½®æ•°æ®

```python
# my_data_prep.py
import pandas as pd

data = [
    {
        "data_source": "my_task",
        "prompt": "å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•",
        "reward_model": {
            "ground_truth": "def quick_sort(arr): ...",
            "test_cases": ["test1", "test2"]
        }
    },
    # ... æ›´å¤šæ•°æ®
]

df = pd.DataFrame(data)
df.to_parquet("my_data/train.parquet")
print(f"Saved {len(df)} samples")
```

#### å¤šè½®å¯¹è¯æ•°æ®

```python
# å¤šè½®å¯¹è¯æ ¼å¼
data = [
    {
        "data_source": "multiturn",
        "prompt": [
            {"role": "user", "content": "ç¬¬ä¸€è½®é—®é¢˜"},
            {"role": "assistant", "content": "ç¬¬ä¸€è½®å›ç­”"},
            {"role": "user", "content": "ç¬¬äºŒè½®é—®é¢˜"}
        ],
        "reward_model": {
            "ground_truth": "æœ€ç»ˆç­”æ¡ˆ"
        }
    }
]
```

### 2.4 å¤šæ¨¡æ€æ•°æ®ï¼ˆVLMï¼‰

```python
# è§†è§‰è¯­è¨€æ¨¡å‹æ•°æ®
data = [
    {
        "data_source": "vqa",
        "prompt": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "text", "text": "å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆï¼Ÿ"}
        ],
        "reward_model": {
            "ground_truth": "ä¸€åªçŒ«"
        }
    }
]
```

### 2.5 æ•°æ®è´¨é‡æ£€æŸ¥

```python
# check_data.py
import pandas as pd

df = pd.read_parquet("my_data/train.parquet")

print(f"æ€»æ ·æœ¬æ•°: {len(df)}")
print(f"å­—æ®µ: {df.columns.tolist()}")
print(f"\nå‰3æ¡æ ·æœ¬:")
print(df.head(3))

# æ£€æŸ¥ prompt é•¿åº¦åˆ†å¸ƒ
df['prompt_len'] = df['prompt'].str.len()
print(f"\nPrompt é•¿åº¦ç»Ÿè®¡:")
print(df['prompt_len'].describe())
```

---

## RL ç®—æ³•å®æˆ˜ï¼ˆ2-3å¤©ï¼‰

### 3.1 ç®—æ³•å¯¹æ¯”

**å®˜æ–¹æ–‡æ¡£ï¼š** https://verl.readthedocs.io/en/latest/algo/algo_intro.html

| ç®—æ³• | é€‚ç”¨åœºæ™¯ | ç‰¹ç‚¹ | é…ç½® |
|------|---------|------|------|
| **GRPO** | å…¥é—¨é¦–é€‰ | ç®€å•ç¨³å®šï¼Œä¸éœ€è¦ Critic | `algorithm.adv_estimator=grpo` |
| **PPO** | é€šç”¨ | ç»å…¸ç®—æ³•ï¼Œéœ€è¦ Critic | `algorithm.adv_estimator=gae` |
| **RLOO** | Best-of-N | åœ¨ N ä¸ªå€™é€‰ä¸­é€‰æœ€ä¼˜ | `algorithm.adv_estimator=rloo` |
| **ReMax** | é«˜è´¨é‡æ•°æ® | æœ€å¤§åŒ– reward æœŸæœ› | `algorithm.adv_estimator=remax` |
| **REINFORCE++** | ç®€å•ä»»åŠ¡ | æ”¹è¿›ç‰ˆ REINFORCE | `algorithm.adv_estimator=reinforce_plus_plus` |

### 3.2 GRPO è®­ç»ƒï¼ˆæ¨èå…¥é—¨ï¼‰

**å®˜æ–¹æ–‡æ¡£ï¼š** https://verl.readthedocs.io/en/latest/algo/grpo.html

```bash
# examples/grpo_trainer/run_qwen2-7b.sh
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    algorithm.group_size=4 \              # GRPO ç»„å¤§å°
    data.train_files="['~/data/gsm8k/train.parquet']" \
    actor_rollout_ref.model.path=~/models/Qwen2.5-7B-Instruct \
    actor_rollout_ref.actor.optim.lr=1e-6 \
    actor_rollout_ref.rollout.name=vllm \
    trainer.total_epochs=10
```

**æ ¸å¿ƒå‚æ•°ï¼š**
- `algorithm.group_size`: æ¯ä¸ª prompt ç”Ÿæˆå‡ ä¸ªå“åº”ï¼ˆé€šå¸¸ 4-8ï¼‰
- `algorithm.kl_penalty`: KL æƒ©ç½šç³»æ•°ï¼ˆé»˜è®¤ 0.001ï¼‰

### 3.3 PPO è®­ç»ƒ

**å®˜æ–¹æ–‡æ¡£ï¼š** https://verl.readthedocs.io/en/latest/algo/ppo.html

```bash
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=gae \          # ä½¿ç”¨ GAE
    algorithm.gamma=1.0 \                  # æŠ˜æ‰£å› å­
    algorithm.lam=0.95 \                   # GAE lambda
    critic.enable=true \                   # å¯ç”¨ Critic
    critic.optim.lr=1e-5 \                 # Critic å­¦ä¹ ç‡
    data.train_files="['~/data/gsm8k/train.parquet']" \
    actor_rollout_ref.model.path=~/models/Qwen2.5-7B-Instruct
```

**ä¸ GRPO çš„åŒºåˆ«ï¼š**
- PPO éœ€è¦é¢å¤–çš„ **Critic æ¨¡å‹**ï¼ˆé¢„æµ‹ valueï¼‰
- GRPO æ›´ç®€å•ï¼Œåªéœ€è¦ Actor æ¨¡å‹
- PPO ç†è®ºä¸Šæ›´ç¨³å®šï¼Œä½† GRPO å®è·µä¸­è¡¨ç°ä¹Ÿå¾ˆå¥½

### 3.4 è‡ªå®šä¹‰ Reward å‡½æ•°

**å®˜æ–¹æ–‡æ¡£ï¼š** https://verl.readthedocs.io/en/latest/preparation/reward_function.html

#### æ–¹å¼ä¸€ï¼šRule-based Reward

```python
# my_reward.py
def compute_score(data_source, solution_str, ground_truth, extra_info=None):
    """
    è‡ªå®šä¹‰ reward å‡½æ•°

    Args:
        data_source: æ•°æ®æ¥æºï¼ˆæ¥è‡ª Parquet çš„ data_source å­—æ®µï¼‰
        solution_str: æ¨¡å‹ç”Ÿæˆçš„å“åº”
        ground_truth: æ ‡å‡†ç­”æ¡ˆï¼ˆæ¥è‡ª reward_model.ground_truthï¼‰
        extra_info: é¢å¤–ä¿¡æ¯ï¼ˆæ¥è‡ª reward_model çš„å…¶ä»–å­—æ®µï¼‰

    Returns:
        float: reward åˆ†æ•°ï¼Œé€šå¸¸ 0-1 ä¹‹é—´
    """
    # ç¤ºä¾‹ï¼šç²¾ç¡®åŒ¹é…
    if solution_str.strip().lower() == ground_truth.strip().lower():
        return 1.0

    # ç¤ºä¾‹ï¼šåŒ…å«å…³é”®è¯
    if ground_truth.lower() in solution_str.lower():
        return 0.5

    return 0.0
```

```bash
# ä½¿ç”¨è‡ªå®šä¹‰ reward
python3 -m verl.trainer.main_ppo \
    custom_reward_function.path=/path/to/my_reward.py \
    custom_reward_function.name=compute_score \
    ...
```

#### æ–¹å¼äºŒï¼šä½¿ç”¨å†…ç½® Reward

```python
# å†…ç½® reward åœ¨ verl/utils/reward_score/
from verl.utils.reward_score import gsm8k, math_reward, geo3k

# GSM8K: æå–æ•°å­—ç­”æ¡ˆå¹¶æ¯”è¾ƒ
# MATH: æ•°å­¦è¡¨è¾¾å¼ç­‰ä»·æ€§åˆ¤æ–­
# Geo3K: å‡ ä½•é¢˜ç­”æ¡ˆåˆ¤æ–­
```

#### æ–¹å¼ä¸‰ï¼šReward Modelï¼ˆæ¨¡å‹æ‰“åˆ†ï¼‰

```python
# ä½¿ç”¨è®­ç»ƒå¥½çš„ Reward Model
reward_model:
  enable: true
  model_path: "Qwen/Qwen2-7B-Reward"
  batch_size: 64
```

### 3.5 ç®—æ³•å‚æ•°è°ƒä¼˜

**å…³é”®å‚æ•°é€ŸæŸ¥è¡¨ï¼š**

```yaml
# å­¦ä¹ ç‡ï¼ˆæœ€é‡è¦ï¼‰
actor_rollout_ref.actor.optim.lr: 1e-6       # å¤ªå¤§å®¹æ˜“å´©ï¼Œå¤ªå°æ”¶æ•›æ…¢
critic.optim.lr: 1e-5                         # Critic å­¦ä¹ ç‡é€šå¸¸æ¯” Actor å¤§

# KL æƒ©ç½šï¼ˆé˜²æ­¢åç¦»åŸå§‹æ¨¡å‹å¤ªè¿œï¼‰
algorithm.kl_penalty: 0.001                   # GRPO/RLOO ç”¨
algorithm.kl_ctrl.kl_coef: 0.01              # PPO ç”¨

# è®­ç»ƒç¨³å®šæ€§
actor_rollout_ref.actor.ppo_epochs: 1        # PPO epoch æ•°ï¼Œè¶Šå¤§è¶Šç¨³å®šä½†è¶Šæ…¢
actor_rollout_ref.actor.clip_ratio: 0.2      # PPO clip èŒƒå›´

# æ•°æ®ç›¸å…³
data.train_batch_size: 1024                   # å…¨å±€ batch size
algorithm.group_size: 4                       # GRPO æ¯ä¸ª prompt çš„å€™é€‰æ•°
```

**è°ƒä¼˜å»ºè®®ï¼š**

1. **å­¦ä¹ ç‡è¿‡å¤§**ç—‡çŠ¶ï¼šreward çªç„¶æ‰åˆ°è´Ÿæ•°ï¼Œloss çˆ†ç‚¸
   - è§£å†³ï¼šé™ä½ 10 å€ï¼Œå¦‚ 1e-6 â†’ 1e-7

2. **å­¦ä¹ ç‡è¿‡å°**ç—‡çŠ¶ï¼šè®­ç»ƒå¾ˆå¤š epoch ä»æ— æ”¹å–„
   - è§£å†³ï¼šå¢åŠ  3-5 å€

3. **KL divergence è¿‡å¤§**ç—‡çŠ¶ï¼šæ¨¡å‹è¾“å‡ºå˜å¾—å¾ˆå¥‡æ€ª
   - è§£å†³ï¼šå¢åŠ  `kl_penalty`

4. **ä¸æ”¶æ•›**ç—‡çŠ¶ï¼šreward ä¸Šä¸‹æ³¢åŠ¨ï¼Œä¸ç¨³å®š
   - å¢åŠ  `train_batch_size`
   - é™ä½å­¦ä¹ ç‡
   - å¢åŠ  `ppo_epochs`

### 3.6 å®éªŒå¯¹æ¯”

**åˆ›å»ºå®éªŒè„šæœ¬ï¼š**

```bash
# experiment.sh
#!/bin/bash

# å®éªŒ 1: GRPO baseline
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    actor_rollout_ref.actor.optim.lr=1e-6 \
    trainer.project_name=exp_grpo_lr1e6

# å®éªŒ 2: GRPO æ›´å¤§å­¦ä¹ ç‡
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    actor_rollout_ref.actor.optim.lr=5e-6 \
    trainer.project_name=exp_grpo_lr5e6

# å®éªŒ 3: PPO å¯¹æ¯”
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=gae \
    critic.enable=true \
    trainer.project_name=exp_ppo
```

---

## Agent RL è®­ç»ƒï¼ˆ3-5å¤©ï¼‰

### 4.1 ä»€ä¹ˆæ˜¯ Agent RL

**Agent RL** è®­ç»ƒæ™ºèƒ½ä½“è¿›è¡Œ**å¤šè½®å¯¹è¯**å’Œ**å·¥å…·è°ƒç”¨**ï¼Œä¸å•è½® RL çš„åŒºåˆ«ï¼š

| ç»´åº¦ | å•è½® RL | Agent RL |
|------|---------|----------|
| äº¤äº’ | ä¸€é—®ä¸€ç­” | å¤šè½®å¯¹è¯ |
| å·¥å…· | æ—  | è®¡ç®—å™¨ã€æœç´¢ã€ä»£ç æ‰§è¡Œç­‰ |
| çŠ¶æ€ | æ— çŠ¶æ€ | ç»´æŠ¤å¯¹è¯å†å² |
| Reward | ç«‹å³åé¦ˆ | æœ€ç»ˆç»“æœ |

### 4.2 Agent Loop æ¡†æ¶

**å®˜æ–¹æ–‡æ¡£ï¼š** https://verl.readthedocs.io/en/latest/advance/agent_loop.html

**æ ¸å¿ƒæ¦‚å¿µï¼š**

```
ç”¨æˆ·é—®é¢˜
    â†“
Agent ç”Ÿæˆå“åº”
    â†“
è§£ææ˜¯å¦è°ƒç”¨å·¥å…·ï¼Ÿ
    â”œâ”€ æ˜¯ â†’ æ‰§è¡Œå·¥å…· â†’ å°†ç»“æœåŠ å…¥å¯¹è¯ â†’ ç»§ç»­ç”Ÿæˆ
    â””â”€ å¦ â†’ è¿”å›æœ€ç»ˆç­”æ¡ˆ â†’ è®¡ç®— Reward
```

### 4.3 å‡†å¤‡å·¥å…·è°ƒç”¨æ•°æ®

```bash
# GSM8K + è®¡ç®—å™¨å·¥å…·
python examples/data_preprocess/gsm8k_multiturn_w_tool.py \
    --local_dir ~/data/gsm8k_tool

# Geo3K + å‡ ä½•å·¥å…·
python examples/data_preprocess/geo3k_multiturn_w_tool.py \
    --local_dir ~/data/geo3k_tool
```

**æ•°æ®æ ¼å¼ï¼š**

```python
{
    "data_source": "gsm8k",
    "prompt": [
        {"role": "user", "content": "è®¡ç®— 23 * 45 + 67"}
    ],
    "tool_config": {
        "available_tools": ["calculator"],
        "tool_format": "react"  # æˆ– "function_calling"
    },
    "reward_model": {
        "ground_truth": "1102"
    }
}
```

### 4.4 é…ç½®å·¥å…·

```yaml
# config/tool_config/gsm8k_tool_config.yaml
tools:
  - name: calculator
    description: "æ‰§è¡Œæ•°å­¦è®¡ç®—"
    parameters:
      expression:
        type: string
        description: "æ•°å­¦è¡¨è¾¾å¼ï¼Œå¦‚ '23 * 45 + 67'"

  - name: python
    description: "æ‰§è¡Œ Python ä»£ç "
    parameters:
      code:
        type: string
        description: "Python ä»£ç "
```

### 4.5 è¿è¡Œ Agent RL è®­ç»ƒ

```bash
# ä½¿ç”¨ SGLangï¼ˆæ¨èç”¨äº Agentï¼‰
python3 -m verl.trainer.main_ppo \
    --config-path=examples/sglang_multiturn/config \
    data.train_files="['~/data/gsm8k_tool/train.parquet']" \
    actor_rollout_ref.model.path=~/models/Qwen2.5-7B-Instruct \
    actor_rollout_ref.rollout.name=sglang \
    actor_rollout_ref.rollout.agent_loop=tool_agent_loop \
    tool_config_path=examples/sglang_multiturn/config/tool_config/gsm8k_tool_config.yaml
```

**å…³é”®é…ç½®ï¼š**
- `actor_rollout_ref.rollout.agent_loop`: æŒ‡å®š agent loop ç±»å‹
  - `single_turn_agent_loop`: å•è½®
  - `tool_agent_loop`: å·¥å…·è°ƒç”¨ï¼ˆReActï¼‰
  - è‡ªå®šä¹‰ï¼š`my_custom_agent_loop`

### 4.6 è‡ªå®šä¹‰ Agent Loop

**åœºæ™¯ï¼š** å®ç°ç‰¹æ®Šçš„å·¥å…·è°ƒç”¨é€»è¾‘æˆ–å¤šè½®äº¤äº’æ¨¡å¼

```python
# my_agent_loop.py
from verl.experimental.agent_loop import AgentLoopBase, AgentLoopOutput

class MyAgentLoop(AgentLoopBase):
    """è‡ªå®šä¹‰ Agent Loop"""

    async def run(self, sampling_params, **kwargs) -> AgentLoopOutput:
        """
        æ‰§è¡Œå¤šè½®äº¤äº’

        Returns:
            AgentLoopOutput: åŒ…å« prompt_ids, response_ids, response_mask
        """
        messages = kwargs.get("messages", [])
        max_turns = kwargs.get("max_turns", 5)

        all_response_ids = []
        all_response_mask = []

        for turn in range(max_turns):
            # 1. è°ƒç”¨ LLM ç”Ÿæˆå“åº”
            prompt_text = self._format_messages(messages)
            prompt_ids = self._tokenize(prompt_text)

            response_ids = await self.server_manager.generate(
                request_id=self.request_id,
                prompt_ids=prompt_ids,
                sampling_params=sampling_params,
            )

            # 2. è§£æå“åº”
            response_text = self._decode(response_ids)
            tool_calls = self._parse_tool_calls(response_text)

            # 3. æ‰§è¡Œå·¥å…·ï¼ˆå¦‚æœæœ‰ï¼‰
            if tool_calls:
                for tool_call in tool_calls:
                    result = await self._execute_tool(tool_call)
                    messages.append({
                        "role": "tool",
                        "content": result,
                        "tool_call_id": tool_call["id"]
                    })
            else:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç»“æŸ
                break

            all_response_ids.append(response_ids)
            all_response_mask.append([1] * len(response_ids))

        return AgentLoopOutput(
            prompt_ids=prompt_ids,
            response_ids=torch.cat(all_response_ids),
            response_mask=torch.cat(all_response_mask),
        )

    def _parse_tool_calls(self, text):
        """è§£æå·¥å…·è°ƒç”¨"""
        # ä½ çš„è§£æé€»è¾‘
        # ä¾‹å¦‚ï¼šåŒ¹é… <tool>calculator</tool><args>{"expr": "1+1"}</args>
        pass

    async def _execute_tool(self, tool_call):
        """æ‰§è¡Œå·¥å…·"""
        tool_name = tool_call["name"]

        if tool_name == "calculator":
            expr = tool_call["args"]["expression"]
            return str(eval(expr))  # æ³¨æ„ï¼šå®é™…ä½¿ç”¨éœ€è¦å®‰å…¨çš„ eval

        elif tool_name == "python":
            code = tool_call["args"]["code"]
            # ä½¿ç”¨æ²™ç®±æ‰§è¡Œ
            result = self._safe_exec_python(code)
            return result
```

**æ³¨å†Œè‡ªå®šä¹‰ Agent Loopï¼š**

```python
# åœ¨è®­ç»ƒè„šæœ¬ä¸­
from verl.experimental.agent_loop import register_agent_loop
from my_agent_loop import MyAgentLoop

register_agent_loop("my_agent_loop", MyAgentLoop)
```

### 4.7 Agent è®­ç»ƒæŠ€å·§

**1. å¢åŠ æ¢ç´¢**

```yaml
# ä½¿ç”¨æ›´é«˜çš„ temperature
actor_rollout_ref.rollout.temperature: 0.7  # é»˜è®¤ 0.6

# ä½¿ç”¨ top_p é‡‡æ ·
actor_rollout_ref.rollout.top_p: 0.9
```

**2. é™åˆ¶å·¥å…·è°ƒç”¨æ¬¡æ•°**

```yaml
# é˜²æ­¢æ— é™å¾ªç¯
tool_config:
  max_tool_calls: 5
  timeout: 30  # ç§’
```

**3. åˆ†å±‚ Reward**

```python
def compute_agent_reward(data_source, solution_str, ground_truth, extra_info):
    """Agent reward = ä»»åŠ¡å®Œæˆåº¦ + å·¥å…·ä½¿ç”¨æ•ˆç‡"""

    # ä»»åŠ¡å®Œæˆåº¦
    task_reward = 1.0 if check_answer(solution_str, ground_truth) else 0.0

    # å·¥å…·ä½¿ç”¨æ•ˆç‡ï¼ˆæƒ©ç½šè¿‡å¤šå·¥å…·è°ƒç”¨ï¼‰
    num_tool_calls = extra_info.get("num_tool_calls", 0)
    efficiency_penalty = -0.1 * max(0, num_tool_calls - 3)

    return task_reward + efficiency_penalty
```

### 4.8 å®Œæ•´ Agent è®­ç»ƒç¤ºä¾‹

æŸ¥çœ‹å®˜æ–¹ç¤ºä¾‹ï¼š

```bash
# GSM8K å·¥å…·è°ƒç”¨
examples/sglang_multiturn/run_qwen2.5-3b_gsm8k_tool_agent_mlflow.sh

# é…ç½®æ–‡ä»¶
examples/sglang_multiturn/config/tool_config/gsm8k_tool_config.yaml
```

---

## è¿›é˜¶æŠ€å·§

### 5.1 ä½¿ç”¨ LoRA åŠ é€Ÿè®­ç»ƒ

**åœºæ™¯ï¼š** æ¨¡å‹å¤ªå¤§ï¼Œå…¨é‡è®­ç»ƒæ˜¾å­˜ä¸å¤Ÿ

```yaml
actor_rollout_ref:
  actor:
    lora:
      enable: true
      r: 16                          # LoRA rank
      lora_alpha: 32
      target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
```

```bash
# LoRA è®­ç»ƒç¤ºä¾‹
examples/grpo_trainer/run_qwen2_5-3b_gsm8k_grpo_lora.sh
```

### 5.2 å¤šæ•°æ®é›†æ··åˆè®­ç»ƒ

```yaml
data:
  train_files:
    - "~/data/gsm8k/train.parquet"
    - "~/data/math/train.parquet"
    - "~/data/code/train.parquet"

  # æ•°æ®é›†é‡‡æ ·æƒé‡
  dataset_weights: [0.5, 0.3, 0.2]
```

### 5.3 Curriculum Learningï¼ˆè¯¾ç¨‹å­¦ä¹ ï¼‰

**ç­–ç•¥ï¼š** ä»ç®€å•ä»»åŠ¡åˆ°å›°éš¾ä»»åŠ¡

```python
# prepare_curriculum_data.py
import pandas as pd

# æŒ‰éš¾åº¦åˆ†çº§
easy_df = df[df['difficulty'] == 'easy']
medium_df = df[df['difficulty'] == 'medium']
hard_df = df[df['difficulty'] == 'hard']

easy_df.to_parquet("curriculum/stage1_easy.parquet")
medium_df.to_parquet("curriculum/stage2_medium.parquet")
hard_df.to_parquet("curriculum/stage3_hard.parquet")
```

```bash
# åˆ†é˜¶æ®µè®­ç»ƒ
# Stage 1: ç®€å•ä»»åŠ¡
python3 -m verl.trainer.main_ppo data.train_files="['curriculum/stage1_easy.parquet']" trainer.total_epochs=5

# Stage 2: ä¸­ç­‰ä»»åŠ¡ï¼ˆä» stage1 checkpoint ç»§ç»­ï¼‰
python3 -m verl.trainer.main_ppo data.train_files="['curriculum/stage2_medium.parquet']" actor_rollout_ref.model.path=outputs/stage1/checkpoint-xxx

# Stage 3: å›°éš¾ä»»åŠ¡
python3 -m verl.trainer.main_ppo data.train_files="['curriculum/stage3_hard.parquet']" actor_rollout_ref.model.path=outputs/stage2/checkpoint-xxx
```

### 5.4 åœ¨çº¿æ•°æ®å¢å¼º

```python
# åœ¨ reward å‡½æ•°ä¸­åŠ¨æ€ç”Ÿæˆæ–°æ ·æœ¬
def compute_score_with_augmentation(data_source, solution_str, ground_truth, extra_info):
    score = basic_score(solution_str, ground_truth)

    # å¦‚æœç­”å¯¹äº†ï¼Œç”Ÿæˆç›¸ä¼¼çš„éš¾é¢˜
    if score > 0.9:
        augmented_prompt = generate_harder_version(extra_info['original_prompt'])
        # ä¿å­˜åˆ°æ•°æ®é›†ä¾›ä¸‹ä¸€è½®è®­ç»ƒä½¿ç”¨
        save_to_buffer(augmented_prompt)

    return score
```

### 5.5 ç›‘æ§å’Œè°ƒè¯•

```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export VERL_LOGGING_LEVEL=DEBUG

# æŸ¥çœ‹ç”Ÿæˆçš„æ ·æœ¬
trainer.log_generation_samples: true

# ä½¿ç”¨ WandB
trainer.logger: '["wandb"]'
trainer.project_name: my_project
trainer.run_name: exp1
```

---

## å¸¸è§é—®é¢˜

### Q1: Reward ä¸€ç›´æ˜¯ 0ï¼Œæ€ä¹ˆåŠï¼Ÿ

**å¯èƒ½åŸå› ï¼š**
1. Reward å‡½æ•°å†™é”™äº†
2. æ•°æ®æ ¼å¼ä¸å¯¹ï¼ˆç¼ºå°‘ `reward_model.ground_truth`ï¼‰
3. æ¨¡å‹è¾“å‡ºæ ¼å¼ä¸ç¬¦åˆé¢„æœŸ

**è°ƒè¯•æ–¹æ³•ï¼š**
```python
# åœ¨ reward å‡½æ•°ä¸­æ·»åŠ è°ƒè¯•è¾“å‡º
def compute_score(data_source, solution_str, ground_truth, extra_info):
    print(f"DEBUG:")
    print(f"  Solution: {solution_str[:100]}")
    print(f"  Ground truth: {ground_truth}")

    score = ...
    print(f"  Score: {score}")
    return score
```

### Q2: è®­ç»ƒè¿‡ç¨‹ä¸­ reward çªç„¶ä¸‹é™

**å¯èƒ½åŸå› ï¼š**
- å­¦ä¹ ç‡å¤ªå¤§ï¼Œå¯¼è‡´ç­–ç•¥å´©æºƒ
- KL divergence å¤ªå¤§ï¼Œåç¦»åŸå§‹æ¨¡å‹å¤ªè¿œ

**è§£å†³æ–¹æ³•ï¼š**
```yaml
# é™ä½å­¦ä¹ ç‡
actor_rollout_ref.actor.optim.lr: 5e-7  # ä» 1e-6 é™ä½

# å¢åŠ  KL æƒ©ç½š
algorithm.kl_penalty: 0.01  # ä» 0.001 å¢åŠ 
```

### Q3: OOMï¼ˆæ˜¾å­˜ä¸è¶³ï¼‰

**è§£å†³æ–¹æ³•ï¼š**
```yaml
# 1. å‡å° batch size
data.train_batch_size: 512
actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu: 8

# 2. å‡å°æ¨ç†æ˜¾å­˜å ç”¨
actor_rollout_ref.rollout.gpu_memory_utilization: 0.4

# 3. ä½¿ç”¨ LoRA
actor_rollout_ref.actor.lora.enable: true

# 4. å¼€å¯ gradient checkpointing
actor_rollout_ref.model.enable_gradient_checkpointing: true
```

### Q4: Agent é™·å…¥æ­»å¾ªç¯è°ƒç”¨å·¥å…·

**è§£å†³æ–¹æ³•ï¼š**
```yaml
# é™åˆ¶å·¥å…·è°ƒç”¨æ¬¡æ•°
tool_config:
  max_tool_calls: 5

# åœ¨ Agent Loop ä¸­æ·»åŠ è¶…æ—¶
sampling_params:
  max_tokens: 2048
  timeout: 30
```

**æˆ–è€…åœ¨ Reward ä¸­æƒ©ç½šï¼š**
```python
def compute_score(...):
    base_score = ...

    # æƒ©ç½šè¿‡å¤šå·¥å…·è°ƒç”¨
    num_calls = extra_info.get("num_tool_calls", 0)
    if num_calls > 5:
        return base_score - 0.5

    return base_score
```

### Q5: å¦‚ä½•çŸ¥é“è®­ç»ƒæ•ˆæœå¥½ä¸å¥½ï¼Ÿ

**å…³é”®æŒ‡æ ‡ï¼š**

1. **Reward è¶‹åŠ¿**ï¼šåº”è¯¥æŒç»­ä¸Šå‡
2. **Accuracy**ï¼šåœ¨æœ‰æ ‡å‡†ç­”æ¡ˆçš„ä»»åŠ¡ä¸Šåº”è¯¥æå‡
3. **KL divergence**ï¼šä¸åº”è¯¥å¤ªå¤§ï¼ˆ<10ï¼‰
4. **Response length**ï¼šä¸åº”è¯¥è¿‡çŸ­æˆ–è¿‡é•¿

**å¯¹æ¯”åŸºçº¿ï¼š**
```python
# è¯„ä¼°è„šæœ¬
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("outputs/checkpoint-xxx")
tokenizer = AutoTokenizer.from_pretrained("outputs/checkpoint-xxx")

# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
correct = 0
for sample in test_data:
    response = generate(model, tokenizer, sample['prompt'])
    if check_answer(response, sample['ground_truth']):
        correct += 1

accuracy = correct / len(test_data)
print(f"Accuracy: {accuracy:.2%}")
```

---

## å­¦ä¹ æ£€æŸ¥æ¸…å•

### å¿«é€Ÿä¸Šæ‰‹ âœ“
- [ ] æˆåŠŸå®‰è£… verl
- [ ] è·‘é€š GRPO quickstart
- [ ] ç†è§£è®­ç»ƒæ—¥å¿—å’ŒæŒ‡æ ‡

### æ•°æ®å‡†å¤‡ âœ“
- [ ] ç†è§£ Parquet æ•°æ®æ ¼å¼
- [ ] èƒ½å¤Ÿå‡†å¤‡å•è½®å¯¹è¯æ•°æ®
- [ ] èƒ½å¤Ÿå‡†å¤‡å¤šè½®å¯¹è¯æ•°æ®ï¼ˆAgentï¼‰
- [ ] æ£€æŸ¥æ•°æ®è´¨é‡

### RL ç®—æ³• âœ“
- [ ] ç†è§£ GRPO vs PPO çš„åŒºåˆ«
- [ ] è®­ç»ƒè¿‡è‡³å°‘ 3 ä¸ªä¸åŒç®—æ³•
- [ ] å®ç°è‡ªå®šä¹‰ Reward å‡½æ•°
- [ ] èƒ½å¤Ÿè°ƒä¼˜å­¦ä¹ ç‡ç­‰å‚æ•°
- [ ] èƒ½å¤Ÿå¯¹æ¯”ä¸åŒç®—æ³•çš„æ•ˆæœ

### Agent RL âœ“
- [ ] ç†è§£ Agent Loop çš„å·¥ä½œåŸç†
- [ ] å‡†å¤‡å·¥å…·è°ƒç”¨æ•°æ®
- [ ] é…ç½®å·¥å…·å®šä¹‰
- [ ] è®­ç»ƒä¸€ä¸ªå·¥å…·è°ƒç”¨ Agent
- [ ] ï¼ˆå¯é€‰ï¼‰å®ç°è‡ªå®šä¹‰ Agent Loop

- [ ] è®¾ç½®ç›‘æ§å’Œè°ƒè¯•

---

## è®­ç»ƒæµç¨‹æ·±åº¦è§£æï¼ˆåŸç†å±‚ï¼‰â­

> **é¢å‘å¯¹è±¡**ï¼šæƒ³æ·±å…¥ç†è§£ verl è®­ç»ƒæµç¨‹çš„å¼€å‘è€…
> **æ ¸å¿ƒæ–‡ä»¶**ï¼š`verl/trainer/ppo/ray_trainer.py` (1741 è¡Œ)
> **å‰ç½®çŸ¥è¯†**ï¼šç†è§£ PPO ç®—æ³•ã€Ray åˆ†å¸ƒå¼æ¡†æ¶

### 6.1 RayPPOTrainer æ¶æ„æ¦‚è§ˆ

**RayPPOTrainer** æ˜¯ verl çš„æ ¸å¿ƒè®­ç»ƒåè°ƒå™¨ï¼ŒèŒè´£ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   RayPPOTrainer                          â”‚
â”‚                  (å•æ§åˆ¶å™¨æ¶æ„)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ åœ¨ Driver è¿›ç¨‹ä¸Šè¿è¡Œè®­ç»ƒå¾ªç¯                            â”‚
â”‚ â€¢ é€šè¿‡ Ray RPC è°ƒç”¨åˆ†å¸ƒå¼ Worker                         â”‚
â”‚ â€¢ æ‰§è¡Œè½»é‡çº§è®¡ç®—ï¼ˆAdvantageã€KL penaltyï¼‰                 â”‚
â”‚ â€¢ ç®¡ç† 4 ç±» WorkerGroupï¼š                                â”‚
â”‚   - ActorRollout: æ¨ç†ç”Ÿæˆå“åº”                           â”‚
â”‚   - Critic: è®­ç»ƒä»·å€¼ç½‘ç»œï¼ˆä»… PPOï¼‰                        â”‚
â”‚   - RefPolicy: è®¡ç®—å‚è€ƒç­–ç•¥ log_prob                     â”‚
â”‚   - RewardModel: æ¨¡å‹æ‰“åˆ†ï¼ˆå¯é€‰ï¼‰                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®è®¾è®¡åŸåˆ™**ï¼š

1. **å•æ§åˆ¶å™¨ï¼ˆSingle Controllerï¼‰**ï¼šæ‰€æœ‰åè°ƒé€»è¾‘åœ¨ driver ä¸Šï¼Œworker åªæ‰§è¡Œè®¡ç®—
2. **æ··åˆå¼•æ“ï¼ˆHybrid Engineï¼‰**ï¼šè®­ç»ƒç”¨ FSDP/Megatronï¼Œæ¨ç†ç”¨ vLLM/SGLang
3. **å¼‚æ­¥æ¨ç†ï¼ˆAsync Rolloutï¼‰**ï¼šæ¨ç†å’Œè®­ç»ƒå¯ä»¥å¹¶è¡Œ

---

### 6.2 è®­ç»ƒä¸»å¾ªç¯è¯¦è§£

#### fit() æ–¹æ³•æµç¨‹å›¾

```python
# verl/trainer/ppo/ray_trainer.py: 1349-1741
def fit(self):
    for epoch in range(epochs):
        for batch in train_dataloader:
            # ==================== ç¬¬ 1 æ­¥ï¼šç”Ÿæˆå“åº” ====================
            gen_batch = self.actor_rollout_wg.generate_sequences(batch)
            # è¿”å›ï¼šresponses, log_probs, attention_mask

            # ==================== ç¬¬ 2 æ­¥ï¼šè®¡ç®— Reward ====================
            reward_tensor, extra_info = compute_reward(gen_batch, reward_fn)
            # è¿”å›ï¼štoken_level_scores [batch_size, seq_len]

            # ==================== ç¬¬ 3 æ­¥ï¼šé‡æ–°è®¡ç®— Log Prob ====================
            old_log_prob = self._compute_old_log_prob(gen_batch)
            # ä¸ºä»€ä¹ˆé‡ç®—ï¼Ÿéœ€è¦æ¢¯åº¦ä¿¡æ¯ç”¨äº PPO æ›´æ–°

            # ==================== ç¬¬ 4 æ­¥ï¼šå‚è€ƒç­–ç•¥ ====================
            if self.use_reference_policy:
                ref_log_prob = self._compute_ref_log_prob(gen_batch)
                # KL æƒ©ç½šéœ€è¦ï¼šKL(Ï€_new || Ï€_ref)

            # ==================== ç¬¬ 5 æ­¥ï¼šä»·å€¼ä¼°è®¡ï¼ˆä»… PPOï¼‰====================
            if self.use_critic:
                values = self._compute_values(gen_batch)
                # Critic é¢„æµ‹ V(s)ï¼Œç”¨äº GAE

            # ==================== ç¬¬ 6 æ­¥ï¼šAdvantage è®¡ç®— ====================
            gen_batch = compute_advantage(
                gen_batch,
                adv_estimator="grpo",  # æˆ– "gae", "rloo" ç­‰
                gamma=1.0,
                lam=0.95,
            )
            # è¿”å›ï¼šadvantages, returns

            # ==================== ç¬¬ 7 æ­¥ï¼šæ›´æ–° Critic ====================
            if self.use_critic:
                critic_output = self._update_critic(gen_batch)

            # ==================== ç¬¬ 8 æ­¥ï¼šæ›´æ–° Actor ====================
            actor_output = self._update_actor(gen_batch)
            # PPO Loss: clip(ratio * A, ...) - Î² * KL + Î± * H
```

---

### 6.3 æ ¸å¿ƒè®¡ç®—æ–¹æ³•è¯¦è§£

#### 6.3.1 compute_advantage() - Advantage ä¼°è®¡

**ä½ç½®**ï¼š`verl/trainer/ppo/ray_trainer.py:187-276`

**æ”¯æŒçš„ç®—æ³•**ï¼š

| ç®—æ³• | Advantage å…¬å¼ | ç‰¹ç‚¹ |
|------|---------------|------|
| **GAE** | `A_t = Î´_t + (Î³Î»)Î´_{t+1} + ...` <br> `Î´_t = r_t + Î³V(s_{t+1}) - V(s_t)` | éœ€è¦ Criticï¼Œæ–¹å·®å° |
| **GRPO** | `a_i = (r_i - Î¼_g) / Ïƒ_g` | ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ï¼Œæ— éœ€ Critic |
| **RLOO** | `a_i = r_i - mean(r_{-i})` | Leave-one-out baseline |
| **REINFORCE++** | `A_t = R_t - b` | ç®€å•æŠ˜æ‰£å›æŠ¥ |

**ç¤ºä¾‹ä»£ç **ï¼š

```python
# GAE å®ç°
if adv_estimator == AdvantageEstimator.GAE:
    # core_algos.compute_gae_advantage_return()
    advantages = []
    for t in reversed(range(T)):
        delta = rewards[t] + gamma * values[t+1] - values[t]
        advantages[t] = delta + gamma * lam * advantages[t+1]

    return advantages, returns

# GRPO å®ç°
elif adv_estimator == AdvantageEstimator.GRPO:
    # 1. æŒ‰ uid åˆ†ç»„ï¼ˆåŒä¸€ä¸ª prompt çš„å¤šä¸ªå“åº”ï¼‰
    grouped_rewards = group_by_uid(token_level_rewards)

    # 2. ç»„å†…å½’ä¸€åŒ–
    for group in grouped_rewards:
        mean = group.mean()
        std = group.std()
        advantages = (group - mean) / (std + 1e-8)

    return advantages
```

**å…³é”®ç‚¹**ï¼š

- GAE éœ€è¦ `values` è¾“å…¥ï¼ˆæ¥è‡ª Criticï¼‰ï¼ŒGRPO ä¸éœ€è¦
- GRPO çš„ `uid` å­—æ®µç”¨äºåˆ†ç»„ï¼ˆ`data.non_tensor_batch["uid"]`ï¼‰
- Advantage ä¼šè¢« normalizeï¼ˆå‡å‡å€¼é™¤æ–¹å·®ï¼‰

---

#### 6.3.2 apply_kl_penalty() - KL æƒ©ç½š

**ä½ç½®**ï¼š`verl/trainer/ppo/ray_trainer.py:127-166`

**ç›®çš„**ï¼šé˜²æ­¢æ–°ç­–ç•¥åç¦»å‚è€ƒç­–ç•¥å¤ªè¿œ

```python
def apply_kl_penalty(data, kl_ctrl, kl_penalty="kl"):
    # 1. è®¡ç®— KL æ•£åº¦
    old_log_prob = data.batch["old_log_probs"]  # Ï€_new
    ref_log_prob = data.batch["ref_log_prob"]   # Ï€_ref

    if kl_penalty == "kl":  # æœ€å¸¸ç”¨
        kld = old_log_prob - ref_log_prob
    elif kl_penalty == "mse":
        kld = 0.5 * (old_log_prob - ref_log_prob).square()
    elif kl_penalty == "low_var_kl":  # K3
        ratio = torch.exp(ref_log_prob - old_log_prob)
        kld = ratio - (ref_log_prob - old_log_prob) - 1

    # 2. åº”ç”¨ KL æƒ©ç½š
    beta = kl_ctrl.value  # åŠ¨æ€è°ƒæ•´çš„ç³»æ•°
    token_level_rewards = token_level_scores - beta * kld

    # 3. æ›´æ–°è‡ªé€‚åº” KL æ§åˆ¶å™¨
    current_kl = masked_mean(kld, mask=response_mask)
    kl_ctrl.update(current_kl=current_kl, n_steps=batch_size)
    # å¦‚æœ KL è¿‡å¤§ï¼Œå¢åŠ  betaï¼›å¦‚æœè¿‡å°ï¼Œå‡å° beta

    return token_level_rewards, metrics
```

**KL æ§åˆ¶å™¨ï¼ˆAdaptiveKLControllerï¼‰**ï¼š

```python
class AdaptiveKLController:
    def __init__(self, init_kl_coef=0.01, target_kl=6.0, horizon=10000):
        self.value = init_kl_coef  # åˆå§‹ Î²
        self.target = target_kl    # ç›®æ ‡ KL
        self.horizon = horizon

    def update(self, current_kl, n_steps):
        # PID æ§åˆ¶ç®—æ³•
        proportional_error = np.clip(current_kl / target - 1, -0.2, 0.2)
        mult = 1 + proportional_error * n_steps / self.horizon
        self.value *= mult  # è‡ªé€‚åº”è°ƒæ•´ Î²
```

---

#### 6.3.3 _update_actor() - Actor æ›´æ–°

**ä½ç½®**ï¼š`verl/trainer/ppo/ray_trainer.py:1283-1317`

```python
def _update_actor(self, batch):
    # é…ç½® PPO è®­ç»ƒå‚æ•°
    ppo_mini_batch_size = 256
    ppo_epochs = 1

    # è½¬æ¢ä¸º TensorDict æ ¼å¼
    batch_td = batch.to_tensordict()
    batch_td = left_right_2_no_padding(batch_td)  # è½¬ä¸º no-padding æ ¼å¼

    # è®¾ç½®è®­ç»ƒå…ƒæ•°æ®
    tu.assign_non_tensor(
        batch_td,
        calculate_entropy=True,           # è®¡ç®—ç†µæ­£åˆ™
        global_batch_size=ppo_mini_batch_size,
        mini_batch_size=ppo_mini_batch_size,
        epochs=ppo_epochs,
        seed=42,
        dataloader_kwargs={"shuffle": True},
    )

    # RPC è°ƒç”¨ Actor Worker
    actor_output = self.actor_rollout_wg.update_actor(batch_td)
    # Worker å†…éƒ¨ä¼šæ‰§è¡Œ PPO loss è®¡ç®—å’Œåå‘ä¼ æ’­

    return actor_output
```

**Actor Worker å†…éƒ¨**ï¼ˆåœ¨ `verl/workers/fsdp_workers.py`ï¼‰ï¼š

```python
def update_actor(self, batch_td):
    # PPO Loss è®¡ç®—
    for epoch in range(ppo_epochs):
        for mini_batch in DataLoader(batch_td, batch_size=mini_batch_size):
            # å‰å‘ä¼ æ’­
            new_log_probs = model(mini_batch["input_ids"], ...)

            # è®¡ç®— ratio
            ratio = torch.exp(new_log_probs - mini_batch["old_log_probs"])

            # PPO Clipped Loss
            advantages = mini_batch["advantages"]
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * advantages
            policy_loss = -torch.min(surr1, surr2).mean()

            # ç†µæ­£åˆ™
            entropy = -(new_log_probs * torch.exp(new_log_probs)).sum(-1).mean()
            entropy_loss = -entropy_coeff * entropy

            # æ€» Loss
            loss = policy_loss + entropy_loss

            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()

    return {"policy_loss": policy_loss, "entropy": entropy}
```

---

#### 6.3.4 _update_critic() - Critic æ›´æ–°

**ä½ç½®**ï¼š`verl/trainer/ppo/ray_trainer.py:1319-1347`

```python
def _update_critic(self, batch):
    # Critic è®­ç»ƒå‚æ•°
    ppo_mini_batch_size = 256
    ppo_epochs = 1

    # è½¬æ¢æ ¼å¼å¹¶ RPC è°ƒç”¨
    batch_td = batch.to_tensordict()
    batch_td = left_right_2_no_padding(batch_td)

    tu.assign_non_tensor(
        batch_td,
        global_batch_size=ppo_mini_batch_size,
        mini_batch_size=ppo_mini_batch_size,
        epochs=ppo_epochs,
    )

    output = self.critic_wg.train_mini_batch(batch_td)
    return output
```

**Critic Worker å†…éƒ¨**ï¼š

```python
def train_mini_batch(self, batch_td):
    # Value Function Loss (MSE)
    for epoch in range(ppo_epochs):
        for mini_batch in DataLoader(batch_td, batch_size=mini_batch_size):
            # å‰å‘ä¼ æ’­
            predicted_values = critic_model(mini_batch["input_ids"])

            # MSE Loss
            target_returns = mini_batch["returns"]
            value_loss = F.mse_loss(predicted_values, target_returns)

            # åå‘ä¼ æ’­
            value_loss.backward()
            optimizer.step()

    return {"value_loss": value_loss.item()}
```

---

### 6.4 WorkerGroup åˆå§‹åŒ–æµç¨‹

**ä½ç½®**ï¼š`verl/trainer/ppo/ray_trainer.py:788-975`

#### æ­¥éª¤ 1ï¼šåˆ›å»ºèµ„æºæ± 

```python
def init_workers(self):
    # 1. åˆ›å»º Ray èµ„æºæ± 
    self.resource_pool_manager.create_resource_pool()
    # ä¾‹å¦‚ï¼š{"global_pool": [8, 8]} â†’ 2 nodes, 8 GPUs each
```

**ResourcePoolManager**ï¼š

```python
@dataclass
class ResourcePoolManager:
    resource_pool_spec: dict[str, list[int]]
    # ä¾‹å¦‚ï¼š{"global_pool": [8, 8], "rollout_pool": [4]}

    def create_resource_pool(self):
        for pool_name, gpus_per_node in self.resource_pool_spec.items():
            pool = RayResourcePool(
                process_on_nodes=gpus_per_node,
                use_gpu=True,
                max_colocate_count=3,  # æœ€å¤š 3 ä¸ª WorkerGroup å…±äº«èŠ‚ç‚¹
            )
            self.resource_pool_dict[pool_name] = pool
```

#### æ­¥éª¤ 2ï¼šåˆ›å»º WorkerGroup

```python
# 2.1 ActorRollout WorkerGroup
actor_rollout_cls = RayClassWithInitArgs(
    cls=FSDPActorRolloutRefWorker,  # æˆ– MegatronWorker
    config=self.config.actor_rollout_ref,
    role="ActorRolloutRef",
)

self.actor_rollout_wg = RayWorkerGroup(
    resource_pool=actor_rollout_resource_pool,
    ray_cls_with_init=actor_rollout_cls,
    num_workers=8,  # 8 ä¸ª workerï¼Œæ¯ä¸ªç®¡ç† 1 å¼  GPU
)

# 2.2 Critic WorkerGroupï¼ˆä»… PPOï¼‰
if self.use_critic:
    critic_cls = RayClassWithInitArgs(
        cls=FSDPCriticWorker,
        config=self.config.critic,
    )

    self.critic_wg = RayWorkerGroup(
        resource_pool=critic_resource_pool,
        ray_cls_with_init=critic_cls,
        num_workers=8,
    )

# 2.3 RefPolicy WorkerGroup
if self.use_reference_policy:
    ref_policy_cls = RayClassWithInitArgs(
        cls=FSDPRefPolicyWorker,
        config=self.config.actor_rollout_ref,
    )

    self.ref_policy_wg = RayWorkerGroup(...)

# 2.4 RewardModel WorkerGroupï¼ˆå¯é€‰ï¼‰
if self.use_rm:
    rm_cls = RayClassWithInitArgs(
        cls=RewardModelWorker,
        config=self.config.reward_model,
    )

    self.rm_wg = RayWorkerGroup(...)
```

#### æ­¥éª¤ 3ï¼šåˆå§‹åŒ– Workers

```python
# 3. è°ƒç”¨å„ WorkerGroup çš„åˆå§‹åŒ–æ–¹æ³•
self.actor_rollout_wg.init_model()
# åœ¨æ¯ä¸ª worker ä¸ŠåŠ è½½æ¨¡å‹æƒé‡

if self.use_critic:
    self.critic_wg.init_model()

if self.use_reference_policy:
    self.ref_policy_wg.init_model()

# 4. åˆå§‹åŒ– Rollout Manager
self.async_rollout_manager = AgentLoopManager(
    config=self.config,
    worker_group=self.actor_rollout_wg,
    rollout_resource_pool=actor_rollout_resource_pool,
    rm_resource_pool=rm_resource_pool,
)
```

---

### 6.5 æ•°æ®æµå’Œ DataProto

**DataProto** æ˜¯è®­ç»ƒæ•°æ®åœ¨ pipeline ä¸­çš„æ ‡å‡†æ ¼å¼ï¼š

```python
@dataclass
class DataProto:
    batch: Dict[str, torch.Tensor]      # Tensor æ•°æ®
    non_tensor_batch: Dict[str, Any]    # é Tensor æ•°æ®ï¼ˆå…ƒä¿¡æ¯ï¼‰
```

**å®Œæ•´æ•°æ®æµ**ï¼š

```
åˆå§‹ Batchï¼ˆä» DataLoaderï¼‰
â”œâ”€â”€ batch:
â”‚   â”œâ”€â”€ prompts: [bs, prompt_len]
â”‚   â””â”€â”€ attention_mask: [bs, total_len]
â””â”€â”€ non_tensor_batch:
    â”œâ”€â”€ uid: [bs]              # åˆ†ç»„ ID
    â”œâ”€â”€ data_source: [bs]      # "gsm8k"
    â””â”€â”€ reward_model:
        â””â”€â”€ ground_truth: [bs]

â†“ ç”Ÿæˆå“åº”å

â”œâ”€â”€ batch:
â”‚   â”œâ”€â”€ prompts: [bs, prompt_len]
â”‚   â”œâ”€â”€ responses: [bs, response_len]
â”‚   â”œâ”€â”€ log_probs: [bs, response_len]  # ç”Ÿæˆæ—¶çš„ log prob
â”‚   â””â”€â”€ attention_mask: [bs, total_len]
â””â”€â”€ non_tensor_batch: ...

â†“ è®¡ç®— Reward å

â”œâ”€â”€ batch:
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ token_level_scores: [bs, response_len]  # Reward åœ¨æœ€åä¸€ä¸ª token
â””â”€â”€ non_tensor_batch: ...

â†“ é‡æ–°è®¡ç®— Log Prob å

â”œâ”€â”€ batch:
â”‚   â”œâ”€â”€ ...
â”‚   â”œâ”€â”€ old_log_probs: [bs, response_len]  # ç”¨äº PPO ratio
â”‚   â””â”€â”€ entropys: [bs, response_len]
â””â”€â”€ ...

â†“ å‚è€ƒç­–ç•¥å

â”œâ”€â”€ batch:
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ ref_log_prob: [bs, response_len]  # ç”¨äº KL æƒ©ç½š
â””â”€â”€ ...

â†“ ä»·å€¼ä¼°è®¡å

â”œâ”€â”€ batch:
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ values: [bs, response_len]  # ç”¨äº GAE
â””â”€â”€ ...

â†“ Advantage è®¡ç®—å

â”œâ”€â”€ batch:
â”‚   â”œâ”€â”€ ...
â”‚   â”œâ”€â”€ advantages: [bs, response_len]
â”‚   â””â”€â”€ returns: [bs, response_len]
â””â”€â”€ ...

â†“ ç”¨äº Actor/Critic è®­ç»ƒ
```

---

### 6.6 Checkpoint ç®¡ç†

#### ä¿å­˜ Checkpoint

```python
def _save_checkpoint(self):
    checkpoint_dir = f"outputs/global_step_{self.global_steps}"

    # 1. ä¿å­˜ Actor
    self.actor_rollout_wg.save_checkpoint(
        local_path=f"{checkpoint_dir}/actor",
        global_steps=self.global_steps,
    )

    # 2. ä¿å­˜ Critic
    if self.use_critic:
        self.critic_wg.save_checkpoint(
            local_path=f"{checkpoint_dir}/critic",
            global_steps=self.global_steps,
        )

    # 3. ä¿å­˜ DataLoader çŠ¶æ€ï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ï¼‰
    dataloader_state = self.train_dataloader.state_dict()
    torch.save(dataloader_state, f"{checkpoint_dir}/dataloader.pt")
```

#### åŠ è½½ Checkpoint

```python
def _load_checkpoint(self):
    # 1. æ‰¾åˆ°æœ€æ–°çš„ checkpoint
    latest_ckpt = find_latest_ckpt_path("outputs/")
    global_step = int(latest_ckpt.split("global_step_")[-1])

    # 2. åŠ è½½ Actor
    self.actor_rollout_wg.load_checkpoint(f"{latest_ckpt}/actor")

    # 3. åŠ è½½ Critic
    if self.use_critic:
        self.critic_wg.load_checkpoint(f"{latest_ckpt}/critic")

    # 4. æ¢å¤ DataLoader çŠ¶æ€
    dataloader_state = torch.load(f"{latest_ckpt}/dataloader.pt")
    self.train_dataloader.load_state_dict(dataloader_state)

    return global_step
```

---

### 6.7 å®æˆ˜æ¡ˆä¾‹ï¼šè¿½è¸ªä¸€ä¸ª Batch

å‡è®¾æˆ‘ä»¬è®­ç»ƒ GSM8Kï¼Œbatch_size=4ï¼Œæ¯ä¸ª prompt ç”Ÿæˆ 2 ä¸ªå“åº”ï¼ˆgroup_size=2ï¼‰ã€‚

```python
# åˆå§‹æ•°æ®
batch = {
    "prompts": [[101, 2023, 2003, ...], ...],  # 4 ä¸ª prompts
    "uid": [0, 0, 1, 1],  # å‰ 2 ä¸ªæ˜¯ prompt 0 çš„ï¼Œå 2 ä¸ªæ˜¯ prompt 1 çš„
    "data_source": ["gsm8k", "gsm8k", "gsm8k", "gsm8k"],
    "reward_model": {
        "ground_truth": ["42", "42", "100", "100"],
    }
}

# Step 1: ç”Ÿæˆå“åº”
gen_batch = actor_rollout_wg.generate_sequences(batch)
# gen_batch["responses"] = [[5, 42, 102], [5, 40, 102], [5, 100, 102], [5, 99, 102]]
#                            â†‘ æ­£ç¡®      â†‘ é”™è¯¯         â†‘ æ­£ç¡®          â†‘ é”™è¯¯

# Step 2: è®¡ç®— Reward
reward_tensor = compute_reward(gen_batch)
# reward_tensor = [[0, 0, 1.0], [0, 0, 0.0], [0, 0, 1.0], [0, 0, 0.0]]
#                  â†‘ æœ€åä¸€ä¸ª token æœ‰ reward

# Step 3: é‡æ–°è®¡ç®— Log Prob
old_log_prob = actor_rollout_wg.compute_log_prob(gen_batch)
# old_log_prob = [[-2.3, -1.5, -0.8], ...]  # æ¯ä¸ª token çš„ log prob

# Step 4: å‚è€ƒç­–ç•¥
ref_log_prob = ref_policy_wg.compute_ref_log_prob(gen_batch)

# Step 5: KL æƒ©ç½š
token_level_rewards = reward_tensor - beta * (old_log_prob - ref_log_prob)

# Step 6: Advantageï¼ˆGRPOï¼‰
# æŒ‰ uid åˆ†ç»„
group_0 = [token_level_rewards[0], token_level_rewards[1]]  # uid=0
group_1 = [token_level_rewards[2], token_level_rewards[3]]  # uid=1

# ç»„å†…å½’ä¸€åŒ–
advantages[0] = (group_0[0] - mean(group_0)) / std(group_0)  # æ­£å€¼
advantages[1] = (group_0[1] - mean(group_0)) / std(group_0)  # è´Ÿå€¼
advantages[2] = (group_1[0] - mean(group_1)) / std(group_1)  # æ­£å€¼
advantages[3] = (group_1[1] - mean(group_1)) / std(group_1)  # è´Ÿå€¼

# Step 7: æ›´æ–° Actor
# PPO ä¼šé¼“åŠ± sample 0 å’Œ 2ï¼ˆæ­£ advantageï¼‰ï¼ŒæŠ‘åˆ¶ sample 1 å’Œ 3ï¼ˆè´Ÿ advantageï¼‰
```

---

### 6.8 å°ç»“

**RayPPOTrainer æ ¸å¿ƒæµç¨‹**ï¼š

1. **ç”Ÿæˆ** â†’ 2. **Reward** â†’ 3. **Old Log Prob** â†’ 4. **Ref Log Prob** â†’ 5. **Values**ï¼ˆPPOï¼‰â†’ 6. **Advantage** â†’ 7. **Critic æ›´æ–°** â†’ 8. **Actor æ›´æ–°**

**å…³é”®æ–‡ä»¶**ï¼š
- `verl/trainer/ppo/ray_trainer.py` - ä¸»è®­ç»ƒå¾ªç¯
- `verl/trainer/ppo/core_algos.py` - Advantage ç®—æ³•å®ç°
- `verl/workers/fsdp_workers.py` - FSDP Worker å®ç°
- `verl/single_controller/base.py` - RayWorkerGroup åŸºç±»

**è¿›ä¸€æ­¥å­¦ä¹ **ï¼š
- é˜…è¯» `core_algos.py` äº†è§£å„ç§ Advantage ç®—æ³•çš„æ•°å­¦ç»†èŠ‚
- æŸ¥çœ‹ `fsdp_workers.py` äº†è§£ Worker å†…éƒ¨çš„æ¨¡å‹ç®¡ç†
- ç ”ç©¶ `ray_resource_pool.py` äº†è§£ Ray èµ„æºåˆ†é…ç­–ç•¥

---

## Reward ç³»ç»Ÿæ¶æ„æ·±åº¦è§£æï¼ˆåŸç†å±‚ï¼‰â­

> **é¢å‘å¯¹è±¡**ï¼šæƒ³è‡ªå®šä¹‰ Reward å‡½æ•°æˆ–ç†è§£ Reward è®¡ç®—æµç¨‹çš„å¼€å‘è€…
> **æ ¸å¿ƒæ–‡ä»¶**ï¼š`verl/workers/reward_manager/`, `verl/utils/reward_score/`
> **å‰ç½®çŸ¥è¯†**ï¼šç†è§£ RL ä¸­ Reward çš„ä½œç”¨

### 7.1 Reward ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

verl çš„ Reward ç³»ç»Ÿé‡‡ç”¨**æ’ä»¶åŒ–æ¶æ„**ï¼Œæ”¯æŒä¸¤å¤§ç±»ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Reward è®¡ç®—æ¶æ„                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Rule-based      â”‚         â”‚  Model-based     â”‚    â”‚
â”‚  â”‚  (å‡½æ•°æ‰“åˆ†)       â”‚         â”‚  (æ¨¡å‹æ‰“åˆ†)       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚            â”‚                           â”‚              â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                        â”‚                              â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚              â”‚  RewardManager     â”‚                   â”‚
â”‚              â”‚  (æŠ½è±¡åŸºç±»)         â”‚                   â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                        â”‚                              â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚        â”‚               â”‚               â”‚             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  Naive    â”‚  â”‚   Batch    â”‚  â”‚   Prime   â”‚       â”‚
â”‚  â”‚ (é€ä¸ªæ‰“åˆ†) â”‚  â”‚ (æ‰¹é‡æ‰“åˆ†)  â”‚  â”‚ (å¹¶è¡Œæ‰“åˆ†) â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**4 ç§ RewardManager**ï¼š

| RewardManager | å¤„ç†æ–¹å¼ | é€‚ç”¨åœºæ™¯ | å¹¶å‘åº¦ |
|---------------|---------|---------|-------|
| **Naive** | é€ä¸ªæ ·æœ¬ | è°ƒè¯•ã€ç®€å•è§„åˆ™ | 1 |
| **Batch** | æ‰¹é‡å‘é‡åŒ– | å¤§ batchã€ç®€å•è§„åˆ™ | 1 |
| **Prime** | å¹¶è¡Œå¼‚æ­¥ | ä»£ç æ‰§è¡Œã€æ²™ç®± | 64 è¿›ç¨‹ |
| **DAPO** | é€ä¸ª + é•¿åº¦æƒ©ç½š | DAPO è®­ç»ƒ | 1 |

---

### 7.2 æŠ½è±¡åŸºç±»ï¼šAbstractRewardManager

**ä½ç½®**ï¼š`verl/workers/reward_manager/abstract.py`

```python
class AbstractRewardManager(ABC):
    def __init__(
        self,
        tokenizer: Any,
        num_examine: int,           # æ‰“å°å‰ N ä¸ªæ ·æœ¬ç”¨äºè°ƒè¯•
        compute_score: RawRewardFn | None,  # è‡ªå®šä¹‰ reward å‡½æ•°
        reward_fn_key: str = "data_source",  # ç”¨å“ªä¸ªå­—æ®µè·¯ç”± reward å‡½æ•°
        **kwargs,
    ):
        self.tokenizer = tokenizer
        self.num_examine = num_examine
        self.compute_score = compute_score or default_compute_score
        self.reward_fn_key = reward_fn_key

    @abstractmethod
    def __call__(
        self,
        data: DataProto,
        return_dict: bool = False,
    ) -> torch.Tensor | dict[str, Any]:
        """
        è®¡ç®— Reward

        è¾“å…¥ï¼š
            data: DataProto åŒ…å« responses, prompts, å…ƒä¿¡æ¯

        è¾“å‡ºï¼š
            reward_tensor: [batch_size, seq_len]
            reward æ”¾åœ¨æœ€åä¸€ä¸ªæœ‰æ•ˆ tokenï¼šreward_tensor[i, valid_len-1]
        """
        pass
```

**å…³é”®è®¾è®¡**ï¼š

1. **Reward ä½ç½®**ï¼šåªåœ¨å“åº”çš„æœ€åä¸€ä¸ª token è®¾ç½® rewardï¼Œå…¶ä»–ä½ç½®ä¸º 0
2. **è§£ç å“åº”**ï¼šéœ€è¦ `tokenizer.decode()` å°† token IDs è½¬ä¸ºæ–‡æœ¬
3. **å…ƒä¿¡æ¯æå–**ï¼šä» `data.non_tensor_batch` è·å– `ground_truth`, `data_source`

---

### 7.3 NaiveRewardManager - é€ä¸ªæ‰“åˆ†

**ä½ç½®**ï¼š`verl/workers/reward_manager/naive.py`

```python
@register("naive")
class NaiveRewardManager(AbstractRewardManager):
    def __call__(self, data: DataProto, return_dict: bool = False):
        reward_tensor = torch.zeros_like(
            data.batch["responses"],
            dtype=torch.float32
        )
        reward_extra_info = {}

        # é€ä¸ªæ ·æœ¬å¤„ç†
        for i in range(len(data)):
            data_item = data[i]  # å•ä¸ªæ ·æœ¬çš„ DataProto

            # 1. æå– prompt å’Œ response
            prompt_ids = data_item.batch["prompts"]
            response_ids = data_item.batch["responses"]

            prompt_str = self.tokenizer.decode(
                prompt_ids,
                skip_special_tokens=True
            )
            response_str = self.tokenizer.decode(
                response_ids,
                skip_special_tokens=True
            )

            # 2. æå–å…ƒä¿¡æ¯
            ground_truth = data_item.non_tensor_batch["reward_model"]["ground_truth"]
            data_source = data_item.non_tensor_batch[self.reward_fn_key]
            extra_info = data_item.non_tensor_batch.get("extra_info", {})

            # 3. è°ƒç”¨ compute_score
            score = self.compute_score(
                data_source=data_source,
                solution_str=response_str,
                ground_truth=ground_truth,
                extra_info=extra_info,
            )

            # 4. å°† reward æ”¾åœ¨æœ€åä¸€ä¸ªæœ‰æ•ˆ token
            prompt_length = data_item.batch["attention_mask"][:len(prompt_ids)].sum()
            valid_response_length = data_item.batch["attention_mask"][prompt_length:].sum()
            reward_tensor[i, valid_response_length - 1] = score

            # 5. æ‰“å°å‰å‡ ä¸ªæ ·æœ¬ï¼ˆè°ƒè¯•ç”¨ï¼‰
            if i < self.num_examine:
                print(f"[Reward Debug {i}]")
                print(f"  Prompt: {prompt_str[:100]}...")
                print(f"  Response: {response_str}")
                print(f"  Ground Truth: {ground_truth}")
                print(f"  Score: {score}")

        if return_dict:
            return {
                "reward_tensor": reward_tensor,
                "reward_extra_info": reward_extra_info,
            }
        return reward_tensor
```

**ç‰¹ç‚¹**ï¼š
- é€ä¸ªå¤„ç†ï¼Œé€‚åˆè°ƒè¯•
- æ‰“å°å‰ N ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
- ç®€å•ç›´è§‚

---

### 7.4 BatchRewardManager - æ‰¹é‡æ‰“åˆ†

**ä½ç½®**ï¼š`verl/workers/reward_manager/batch.py`

```python
@register("batch")
class BatchRewardManager(AbstractRewardManager):
    def __call__(self, data: DataProto, return_dict: bool = False):
        reward_tensor = torch.zeros_like(
            data.batch["responses"],
            dtype=torch.float32
        )

        # 1. æ‰¹é‡è§£ç æ‰€æœ‰å“åº”
        responses_str = []
        ground_truths = []
        data_sources = []
        extra_infos = []
        valid_response_lengths = []

        for i in range(len(data)):
            data_item = data[i]

            # è§£ç å“åº”
            response_ids = data_item.batch["responses"]
            valid_len = data_item.batch["attention_mask"][prompt_len:].sum()
            response_str = self.tokenizer.decode(
                response_ids[:valid_len],
                skip_special_tokens=True
            )
            responses_str.append(response_str)

            # æå–å…ƒä¿¡æ¯
            ground_truths.append(
                data_item.non_tensor_batch["reward_model"]["ground_truth"]
            )
            data_sources.append(
                data_item.non_tensor_batch[self.reward_fn_key]
            )
            extra_infos.append(
                data_item.non_tensor_batch.get("extra_info", {})
            )
            valid_response_lengths.append(valid_len)

        # 2. æ‰¹é‡è°ƒç”¨ compute_scoreï¼ˆå‘é‡åŒ–ï¼‰
        scores = self.compute_score(
            data_sources=data_sources,        # åˆ—è¡¨
            solution_strs=responses_str,      # åˆ—è¡¨
            ground_truths=ground_truths,      # åˆ—è¡¨
            extra_infos=extra_infos,          # åˆ—è¡¨
            **self.reward_kwargs,
        )
        # è¿”å›ï¼š[score1, score2, ...]

        # 3. å°† scores æ”¾å…¥ reward_tensor
        for i in range(len(data)):
            reward_tensor[i, valid_response_lengths[i] - 1] = scores[i]

        return reward_tensor
```

**ç‰¹ç‚¹**ï¼š
- æ‰¹é‡å¤„ç†ï¼Œæ•ˆç‡æ›´é«˜
- Reward å‡½æ•°éœ€è¦æ”¯æŒåˆ—è¡¨è¾“å…¥
- é€‚åˆå¤§ batch

**å¯¹æ¯” Naive**ï¼š

| ç»´åº¦ | Naive | Batch |
|------|-------|-------|
| compute_score ç­¾å | `(data_source, solution_str, ground_truth, ...)` | `(data_sources, solution_strs, ground_truths, ...)` |
| å¤„ç†æ–¹å¼ | å¾ªç¯è°ƒç”¨ | ä¸€æ¬¡è°ƒç”¨ |
| è°ƒè¯•ä¿¡æ¯ | æ‰“å°è¯¦ç»† | æ—  |
| æ€§èƒ½ | æ…¢ | å¿« |

---

### 7.5 PrimeRewardManager - å¹¶è¡Œå¼‚æ­¥æ‰“åˆ†

**ä½ç½®**ï¼š`verl/workers/reward_manager/prime.py`

```python
async def parallel_compute_score_async(
    evaluation_func,
    completions: list[str],
    references: list[str],
    tasks: list[str],
    extra_info: list[dict],
    num_processes=64,
):
    """ä½¿ç”¨ ProcessPoolExecutor å¹¶è¡Œæ‰§è¡Œ"""
    with ProcessPoolExecutor(max_workers=num_processes) as executor:
        # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        tasks_async = [
            single_compute_score(
                evaluation_func, c, r, t, ei, executor, timeout=300.0
            )
            for c, r, t, ei in zip(completions, references, tasks, extra_info)
        ]

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks_async, return_exceptions=False)

    # æå– scores
    scores = [r["score"] if isinstance(r, dict) else r for r in results]
    return scores

async def single_compute_score(
    evaluation_func, completion, reference, task, extra_info, executor, timeout
):
    """å•ä¸ªæ ·æœ¬çš„è¯„åˆ†ï¼ˆå¸¦è¶…æ—¶ï¼‰"""
    try:
        # åœ¨è¿›ç¨‹æ± ä¸­æ‰§è¡Œ
        loop = asyncio.get_event_loop()
        result = await asyncio.wait_for(
            loop.run_in_executor(
                executor,
                evaluation_func,
                completion, reference, task, extra_info
            ),
            timeout=timeout
        )
        return result
    except asyncio.TimeoutError:
        print(f"[Timeout] Sample took > {timeout}s")
        return 0.0  # è¶…æ—¶è¿”å› 0
    except Exception as e:
        print(f"[Error] {e}")
        return 0.0  # å¼‚å¸¸è¿”å› 0

@register("prime")
class PrimeRewardManager(AbstractRewardManager):
    def verify(self, data):
        """åŒæ­¥å°è£…ï¼Œå†…éƒ¨è°ƒç”¨å¼‚æ­¥å‡½æ•°"""
        scores = run_reward_scoring(
            self.compute_score,
            completions=sequences_str,
            references=ground_truth,
            tasks=data_sources,
            extra_info=extra_info,
            num_processes=64,
        )
        return scores
```

**ç‰¹ç‚¹**ï¼š
- 64 ä¸ªè¿›ç¨‹å¹¶è¡Œæ‰§è¡Œï¼ˆå¯é…ç½®ï¼‰
- æ¯ä¸ªæ ·æœ¬è¶…æ—¶ 300 ç§’
- è¶…æ—¶æˆ–å¼‚å¸¸è¿”å› 0.0
- é€‚åˆä»£ç æ‰§è¡Œã€æ²™ç®±ç¯å¢ƒ

**ä½¿ç”¨åœºæ™¯**ï¼š

```python
# ä»£ç æ‰§è¡Œ Reward
def compute_score(solution_str, ground_truth, extra_info):
    # 1. æå–ç”Ÿæˆçš„ä»£ç 
    code = extract_code(solution_str)

    # 2. åœ¨æ²™ç®±ä¸­æ‰§è¡Œ
    test_cases = extra_info["test_cases"]
    results = []
    for test_input, expected_output in test_cases:
        try:
            actual_output = execute_code(code, test_input, timeout=5)
            results.append(actual_output == expected_output)
        except Exception:
            results.append(False)

    # 3. è®¡ç®—é€šè¿‡ç‡
    pass_rate = sum(results) / len(results)
    return pass_rate

# Prime ä¼šå¹¶è¡Œæ‰§è¡Œ 64 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æœ€å¤š 300 ç§’
```

---

### 7.6 å†…ç½® Reward Score å‡½æ•°

**ç›®å½•**ï¼š`verl/utils/reward_score/`

#### 7.6.1 GSM8K Reward

**æ–‡ä»¶**ï¼š`verl/utils/reward_score/gsm8k.py`

```python
def extract_solution(solution_str, method="strict"):
    """æå– #### åçš„ç­”æ¡ˆ"""
    if method == "strict":
        # GSM8K æ ¼å¼ï¼š#### 42
        solutions = re.findall(r"#### (\-?[0-9\.\,]+)", solution_str)
        if len(solutions) == 0:
            return None
        return solutions[-1].replace(",", "").replace("$", "")
    elif method == "flexible":
        # æ›´å®½æ¾çš„åŒ¹é…
        ...

def compute_score(
    solution_str,
    ground_truth,
    method="strict",
    format_score=0.0,
    score=1.0
):
    """
    GSM8K è¯„åˆ†é€»è¾‘

    è¿”å›ï¼š
        - ç­”æ¡ˆæ­£ç¡® â†’ 1.0
        - æ ¼å¼æ­£ç¡®ä½†ç­”æ¡ˆé”™è¯¯ â†’ 0.0ï¼ˆæˆ– format_scoreï¼‰
        - æ ¼å¼é”™è¯¯ â†’ 0.0
    """
    answer = extract_solution(solution_str, method=method)

    if answer is None:
        return 0.0  # æ ¼å¼é”™è¯¯
    else:
        return score if answer == ground_truth else format_score
```

**ç¤ºä¾‹**ï¼š

```python
solution_1 = "Let's think step by step.\n1 + 1 = 2\n#### 2"
solution_2 = "The answer is 2."
ground_truth = "2"

compute_score(solution_1, ground_truth)  # 1.0 (æ­£ç¡®)
compute_score(solution_2, ground_truth)  # 0.0 (æ ¼å¼é”™è¯¯ï¼Œæ²¡æœ‰ ####)
```

---

#### 7.6.2 MATH Rewardï¼ˆLaTeXï¼‰

**æ–‡ä»¶**ï¼š`verl/utils/reward_score/math_reward.py`

```python
def last_boxed_only_string(string):
    """æå– \boxed{} ä¸­çš„å†…å®¹"""
    idx = string.rfind("\\boxed")
    if idx < 0:
        return None

    i = idx
    right_brace_idx = None
    num_left_braces = 0
    while i < len(string):
        if string[i] == "{":
            num_left_braces += 1
        elif string[i] == "}":
            num_left_braces -= 1
            if num_left_braces == 0:
                right_brace_idx = i
                break
        i += 1

    if right_brace_idx is None:
        return None

    return string[idx:right_brace_idx + 1]

def is_equiv(str1, str2):
    """åˆ¤æ–­ä¸¤ä¸ªæ•°å­¦è¡¨è¾¾å¼æ˜¯å¦ç­‰ä»·"""
    # 1. è§„èŒƒåŒ–ï¼ˆå»ç©ºæ ¼ã€LaTeX å‘½ä»¤ï¼‰
    str1 = strip_string(str1)
    str2 = strip_string(str2)

    # 2. ç›´æ¥å­—ç¬¦ä¸²æ¯”è¾ƒ
    if str1 == str2:
        return True

    # 3. å°è¯• sympy ç¬¦å·è®¡ç®—
    try:
        parsed1 = parse_latex(str1)
        parsed2 = parse_latex(str2)
        return simplify(parsed1 - parsed2) == 0
    except:
        return False

def compute_score(solution_str, ground_truth) -> float:
    """MATH æ•°æ®é›†è¯„åˆ†"""
    try:
        string_in_last_boxed = last_boxed_only_string(solution_str)
        if string_in_last_boxed is None:
            return 0.0

        answer = remove_boxed(string_in_last_boxed)
        if is_equiv(answer, ground_truth):
            return 1.0
    except Exception:
        pass

    return 0.0
```

**ç¤ºä¾‹**ï¼š

```python
solution_1 = "The solution is \\boxed{\\frac{1}{2}}"
solution_2 = "The answer is \\boxed{0.5}"
ground_truth = "\\frac{1}{2}"

compute_score(solution_1, ground_truth)  # 1.0
compute_score(solution_2, ground_truth)  # 1.0 (0.5 = 1/2)
```

---

#### 7.6.3 ä»£ç æ‰§è¡Œ Rewardï¼ˆPrimeï¼‰

**æ–‡ä»¶**ï¼š`verl/utils/reward_score/prime_code.py`

```python
def compute_score(
    solution_str,
    ground_truth,
    extra_info,
    sandbox_fusion_url=None,
    concurrent_semaphore=None,
    continuous=False,
):
    """
    ä»£ç æ‰§è¡Œè¯„åˆ†

    å‚æ•°ï¼š
        sandbox_fusion_url: äº‘å‡½æ•° URL
        continuous: True â†’ è¿”å›é€šè¿‡ç‡ [0, 1]ï¼ŒFalse â†’ è¿”å› 0 æˆ– 1
    """
    # 1. æå–ä»£ç 
    code = extract_code_from_solution(solution_str)

    # 2. å‡†å¤‡æµ‹è¯•ç”¨ä¾‹
    test_cases = extra_info.get("test_cases", [])

    # 3. å‘é€åˆ°æ²™ç®±æ‰§è¡Œ
    async with concurrent_semaphore:  # æ§åˆ¶å¹¶å‘æ•°
        response = await send_to_sandbox(
            url=sandbox_fusion_url,
            code=code,
            test_cases=test_cases,
            memory_limit_mb=1024,
            timeout_seconds=10,
        )

    # 4. è§£æç»“æœ
    passed = response["num_passed"]
    total = response["num_total"]

    if continuous:
        return passed / total  # é€šè¿‡ç‡
    else:
        return 1.0 if passed == total else 0.0  # å…¨å¯¹æˆ–å…¨é”™
```

---

#### 7.6.4 Reward Dispatcherï¼ˆè·¯ç”±ï¼‰

**æ–‡ä»¶**ï¼š`verl/utils/reward_score/__init__.py`

```python
def default_compute_score(
    data_source,
    solution_str,
    ground_truth,
    extra_info=None,
    **kwargs
):
    """
    æ ¹æ® data_source è·¯ç”±åˆ°å¯¹åº”çš„ reward å‡½æ•°
    """
    if data_source == "openai/gsm8k":
        from . import gsm8k
        return gsm8k.compute_score(solution_str, ground_truth)

    elif data_source in ["lighteval/MATH", "DigitalLearningGmbH/MATH-lighteval"]:
        from . import math_reward
        return math_reward.compute_score(solution_str, ground_truth)

    elif data_source in ["codecontests", "apps", "codeforces", "taco"]:
        from . import prime_code
        return prime_code.compute_score(
            solution_str, ground_truth, extra_info,
            continuous=True, **kwargs
        )

    elif data_source in ["searchR1_nq", "searchR1_triviaqa", "searchR1_hotpotqa"]:
        from . import search_r1_like_qa_em
        return search_r1_like_qa_em.compute_score(solution_str, ground_truth)

    else:
        raise NotImplementedError(f"Reward function for {data_source=} not found")
```

**å·¥ä½œæµç¨‹**ï¼š

1. ä» `data.non_tensor_batch["data_source"]` è·å–æ•°æ®æ¥æº
2. æ ¹æ®æ•°æ®æ¥æºé€‰æ‹©å¯¹åº”çš„ reward å‡½æ•°
3. è°ƒç”¨è¯¥å‡½æ•°è®¡ç®— score

---

### 7.7 é…ç½®å’ŒåŠ è½½

#### 7.7.1 Reward Manager é…ç½®

**æ–‡ä»¶**ï¼š`verl/trainer/config/reward_manager.yaml`

```yaml
# Reward Manager é…ç½®
_target_: verl.trainer.config.config.RewardManagerConfig

source: register  # æˆ– "importlib"ï¼ˆåŠ è½½å¤–éƒ¨æ¨¡å—ï¼‰
name: ${oc.select:reward_model.reward_manager,naive}  # é»˜è®¤ naive

# å¤–éƒ¨æ¨¡å—ï¼ˆå½“ source=importlibï¼‰
module:
  _target_: verl.trainer.config.config.ModuleConfig
  path: /path/to/my_reward_manager.py
  name: MyRewardManager
```

#### 7.7.2 åŠ è½½ Reward Manager

**æ–‡ä»¶**ï¼š`verl/trainer/ppo/reward.py`

```python
def load_reward_manager(
    config: DictConfig,
    tokenizer: Any,
    num_examine: int,
    **reward_kwargs,
) -> AbstractRewardManager:
    """åŠ è½½ RewardManager"""

    # 1. åŠ è½½è‡ªå®šä¹‰ reward å‡½æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
    compute_score = get_custom_reward_fn(config)

    # 2. è·å– RewardManager ç±»
    reward_manager_cfg = config.reward_manager
    if reward_manager_cfg.source == "register":
        # ä»æ³¨å†Œè¡¨åŠ è½½
        from verl.workers.reward_manager import get_reward_manager_cls
        reward_manager_cls = get_reward_manager_cls(reward_manager_cfg.name)
    elif reward_manager_cfg.source == "importlib":
        # ä»å¤–éƒ¨æ¨¡å—åŠ è½½
        reward_manager_cls = load_extern_object(
            module_path=reward_manager_cfg.module.path,
            class_name=reward_manager_cfg.module.name,
        )

    # 3. å¤„ç† sandbox fusionï¼ˆä»£ç æ‰§è¡Œï¼‰
    if compute_score is None and reward_manager_cfg.name == "prime":
        sandbox_config = config.reward_model.get("sandbox_fusion", {})
        if sandbox_config.get("url"):
            from functools import partial
            compute_score = partial(
                default_compute_score,
                sandbox_fusion_url=sandbox_config["url"],
                concurrent_semaphore=create_semaphore(
                    sandbox_config.get("max_concurrent", 64)
                ),
            )

    # 4. å®ä¾‹åŒ– RewardManager
    return reward_manager_cls(
        tokenizer=tokenizer,
        num_examine=num_examine,
        compute_score=compute_score,
        reward_fn_key=config.data.reward_fn_key,
        **reward_kwargs,
    )

def compute_reward(data: DataProto, reward_fn: AbstractRewardManager):
    """è®¡ç®— batch çš„ reward"""
    reward_result = reward_fn(data, return_dict=True)
    reward_tensor = reward_result["reward_tensor"]
    reward_extra_infos = reward_result.get("reward_extra_info", {})
    return reward_tensor, reward_extra_infos
```

---

### 7.8 å®æˆ˜æ¡ˆä¾‹

#### æ¡ˆä¾‹ 1ï¼šGSM8K è§„åˆ™ Reward

```bash
# è®­ç»ƒå‘½ä»¤
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    data.train_files="['~/data/gsm8k/train.parquet']" \
    data.reward_fn_key='data_source' \
    actor_rollout_ref.model.path=~/models/Qwen2.5-7B-Instruct \
    reward_model.reward_manager=naive \
    # ä¸è®¾ç½® reward_model.enableï¼Œä½¿ç”¨è§„åˆ™ reward
```

**æ•°æ®æ ¼å¼**ï¼š

```python
{
    "prompts": "Janet's ducks lay 16 eggs per day...",
    "data_source": "openai/gsm8k",  # è·¯ç”±åˆ° gsm8k.compute_score
    "reward_model": {
        "ground_truth": "18"
    }
}
```

**Reward æµç¨‹**ï¼š

1. `NaiveRewardManager` é€ä¸ªå¤„ç†æ ·æœ¬
2. è§£ç å“åº”ï¼š`"Let's think step by step... #### 18"`
3. `gsm8k.extract_solution()` æå– `"18"`
4. ä¸ `ground_truth="18"` æ¯”è¾ƒ â†’ è¿”å› 1.0
5. æ”¾å…¥ `reward_tensor[i, last_token_idx] = 1.0`

---

#### æ¡ˆä¾‹ 2ï¼šä»£ç æ‰§è¡Œ Rewardï¼ˆPrimeï¼‰

```bash
# è®­ç»ƒå‘½ä»¤
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    data.train_files="['~/data/code/train.parquet']" \
    reward_model.reward_manager=prime \
    reward_model.sandbox_fusion.url='https://api.sandbox.com/run' \
    reward_model.sandbox_fusion.max_concurrent=128 \
    actor_rollout_ref.model.path=~/models/CodeLlama-7B
```

**æ•°æ®æ ¼å¼**ï¼š

```python
{
    "prompts": "Write a function to check if a number is prime.",
    "data_source": "codecontests",
    "reward_model": {
        "ground_truth": null  # ä»£ç æ‰§è¡Œä¸éœ€è¦ ground_truth
    },
    "extra_info": {
        "test_cases": [
            {"input": "2", "output": "True"},
            {"input": "4", "output": "False"},
            {"input": "17", "output": "True"},
        ]
    }
}
```

**Reward æµç¨‹**ï¼š

1. `PrimeRewardManager` å¹¶è¡Œå¤„ç† 64 ä¸ªæ ·æœ¬
2. å¯¹æ¯ä¸ªæ ·æœ¬ï¼š
   - æå–ä»£ç ï¼š`extract_code(response)`
   - å‘é€åˆ°æ²™ç®±ï¼š`sandbox_fusion_url` with test cases
   - æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹ï¼ˆtimeout 10sï¼‰
   - è¿”å›é€šè¿‡ç‡ï¼š`passed / total`
3. è¶…æ—¶æ ·æœ¬è¿”å› 0.0
4. å…¨éƒ¨å®Œæˆåè¿”å› reward_tensor

---

#### æ¡ˆä¾‹ 3ï¼šè‡ªå®šä¹‰ Reward å‡½æ•°

```python
# my_reward.py
def compute_score(data_source, solution_str, ground_truth, extra_info=None):
    """
    è‡ªå®šä¹‰ Rewardï¼šç»“åˆæ­£ç¡®æ€§å’Œç®€æ´æ€§
    """
    from difflib import SequenceMatcher

    # 1. æ­£ç¡®æ€§ï¼ˆåŸºäºç›¸ä¼¼åº¦ï¼‰
    similarity = SequenceMatcher(
        None,
        solution_str.lower(),
        ground_truth.lower()
    ).ratio()
    correctness_score = similarity

    # 2. ç®€æ´æ€§ï¼ˆå­—ç¬¦æ•°æƒ©ç½šï¼‰
    max_length = extra_info.get("max_length", 500)
    length_penalty = min(len(solution_str) / max_length, 1.0)
    conciseness_score = 1 - length_penalty

    # 3. å…³é”®è¯å¥–åŠ±
    keywords = extra_info.get("keywords", [])
    keyword_bonus = sum(kw in solution_str for kw in keywords) * 0.1

    # 4. ç»¼åˆå¾—åˆ†
    total_score = (
        correctness_score * 0.7 +
        conciseness_score * 0.2 +
        keyword_bonus * 0.1
    )

    return total_score
```

**ä½¿ç”¨é…ç½®**ï¼š

```bash
python3 -m verl.trainer.main_ppo \
    custom_reward_function.path='my_reward.py' \
    custom_reward_function.name='compute_score' \
    reward_model.reward_manager=naive \
    ...
```

---

### 7.9 å°ç»“

**Reward ç³»ç»Ÿæ ¸å¿ƒæµç¨‹**ï¼š

```
Data (responses)
    â†“
RewardManager.__call__()
    â†“
decode responses â†’ extract metadata
    â†“
compute_score_fn()
    â†“
    â”œâ”€ default_compute_score (è·¯ç”±åˆ°å…·ä½“å‡½æ•°)
    â”‚   â”œâ”€ gsm8k.compute_score
    â”‚   â”œâ”€ math_reward.compute_score
    â”‚   â”œâ”€ prime_code.compute_score
    â”‚   â””â”€ custom compute_score
    â†“
reward_tensor [batch_size, seq_len]
(reward åœ¨ last_token_idx)
```

**å…³é”®æ–‡ä»¶**ï¼š
- `verl/workers/reward_manager/abstract.py` - æŠ½è±¡åŸºç±»
- `verl/workers/reward_manager/{naive,batch,prime,dapo}.py` - å…·ä½“å®ç°
- `verl/utils/reward_score/__init__.py` - Dispatcher
- `verl/utils/reward_score/{gsm8k,math_reward,prime_code}.py` - å†…ç½®å‡½æ•°
- `verl/trainer/ppo/reward.py` - åŠ è½½å’Œé…ç½®

**è¿›ä¸€æ­¥å­¦ä¹ **ï¼š
- æŸ¥çœ‹ `verl/utils/reward_score/` äº†è§£æ›´å¤šå†…ç½® reward å‡½æ•°
- é˜…è¯» `prime_code.py` äº†è§£æ²™ç®±ä»£ç æ‰§è¡Œç»†èŠ‚
- ç ”ç©¶ `DAPORewardManager` äº†è§£é•¿åº¦æ„ŸçŸ¥çš„ reward è®¾è®¡

---

### è¿›é˜¶æŠ€å·§ âœ“
- [ ] ä½¿ç”¨ LoRA è®­ç»ƒå¤§æ¨¡å‹
- [ ] æ··åˆå¤šä¸ªæ•°æ®é›†

---

## ä¸‹ä¸€æ­¥

å®Œæˆä»¥ä¸Šå†…å®¹åï¼Œä½ å¯ä»¥ï¼š

1. **æ·±å…¥ç®—æ³•ç»†èŠ‚**
   - é˜…è¯» HybridFlow è®ºæ–‡ï¼šhttps://arxiv.org/abs/2409.19256
   - ç ”ç©¶ DAPOã€PRIME ç­‰é«˜çº§ç®—æ³•ï¼ˆåœ¨ recipe å­æ¨¡å—ä¸­ï¼‰

2. **å¤§è§„æ¨¡è®­ç»ƒ**
   - å­¦ä¹ å¤šæœºå¤šå¡è®­ç»ƒ
   - äº†è§£ Megatron-LM åç«¯ï¼ˆè¶…å¤§æ¨¡å‹ï¼‰

3. **ç”Ÿäº§éƒ¨ç½²**
   - æ¨¡å‹å¯¼å‡ºå’ŒæœåŠ¡åŒ–
   - æ¨ç†åŠ é€Ÿä¼˜åŒ–

4. **è´¡çŒ®ç¤¾åŒº**
   - åœ¨ GitHub ä¸Šæäº¤ Issue/PR
   - åˆ†äº«ä½ çš„è®­ç»ƒç»éªŒ

---

**å®˜æ–¹èµ„æºï¼š**
- æ–‡æ¡£ï¼šhttps://verl.readthedocs.io/en/latest/
- GitHubï¼šhttps://github.com/volcengine/verl
- Slackï¼šhttps://join.slack.com/t/verl-project

*æœ€åæ›´æ–°: 2026-01-25*
