# PPO è®­ç»ƒå™¨ (Proximal Policy Optimization)

> åŸºäº Actor-Critic æ¶æ„çš„ç¨³å®šå¯é çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒ

---

## ğŸ“‹ æ¦‚è¿°

**PPO (Proximal Policy Optimization)** æ˜¯æœ€å¹¿æ³›ä½¿ç”¨çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¹‹ä¸€ï¼Œç”± OpenAI äº 2017 å¹´æå‡ºã€‚å®ƒåœ¨ LLM è®­ç»ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«é€‚åˆéœ€è¦ç²¾ç»†æ§åˆ¶å’Œç¨³å®šè®­ç»ƒçš„åœºæ™¯ã€‚

### æ ¸å¿ƒç‰¹ç‚¹

- âœ… **Actor-Critic æ¶æ„**ï¼šåŒæ—¶è®­ç»ƒç­–ç•¥æ¨¡å‹ï¼ˆActorï¼‰å’Œä»·å€¼æ¨¡å‹ï¼ˆCriticï¼‰
- âœ… **GAE ä¼˜åŠ¿ä¼°è®¡**ï¼šå¹³è¡¡åå·®å’Œæ–¹å·®ï¼Œè·å¾—æ›´ç¨³å®šçš„æ¢¯åº¦
- âœ… **Clipped Objective**ï¼šé˜²æ­¢ç­–ç•¥æ›´æ–°è¿‡å¤§ï¼Œé¿å…è®­ç»ƒå´©æºƒ
- âœ… **è®­ç»ƒç¨³å®šæ€§é«˜**ï¼šé€‚åˆé•¿åºåˆ—å’Œè¿‡ç¨‹å¯¼å‘ä»»åŠ¡
- âœ… **KL æ•£åº¦æ§åˆ¶**ï¼šé˜²æ­¢ç­–ç•¥åç¦»å‚è€ƒæ¨¡å‹å¤ªè¿œ

### é€‚ç”¨åœºæ™¯

| åœºæ™¯ | è¯´æ˜ | æ¨èåº¦ |
|------|------|--------|
| **å¯¹è¯è´¨é‡ä¼˜åŒ–** | éœ€è¦ç²¾ç»†æ§åˆ¶ç”Ÿæˆè´¨é‡ | â­â­â­â­â­ |
| **é•¿æ–‡æœ¬ç”Ÿæˆ** | éœ€è¦è¿‡ç¨‹çº§åˆ«çš„ä¼˜åŒ– | â­â­â­â­â­ |
| **RLHF è®­ç»ƒ** | å¯¹é½äººç±»åå¥½ | â­â­â­â­â­ |
| **æ•°å­¦æ¨ç†** | éœ€è¦ step-by-step ä¼˜åŒ– | â­â­â­â­ |
| **ä»£ç ç”Ÿæˆ** | å¯ä»¥ï¼Œä½† GRPO ä¹Ÿä¸é”™ | â­â­â­ |
| **å¿«é€ŸåŸå‹** | éœ€è¦è®­ç»ƒ Criticï¼Œè¾ƒæ…¢ | â­â­ |

### PPO vs GRPO å¯¹æ¯”

| ç‰¹æ€§ | PPO | GRPO |
|------|-----|------|
| **Critic æ¨¡å‹** | âœ… éœ€è¦ | âŒ ä¸éœ€è¦ |
| **GPU æ˜¾å­˜** | æ›´å¤šï¼ˆActor + Criticï¼‰ | æ›´å°‘ï¼ˆåªæœ‰ Actorï¼‰ |
| **è®­ç»ƒç¨³å®šæ€§** | æ›´é«˜ï¼ˆæœ‰ä»·å€¼å‡½æ•°ï¼‰ | ä¾èµ–é‡‡æ ·æ•°é‡ |
| **ä¼˜åŠ¿ä¼°è®¡** | GAEï¼ˆæ—¶åºå·®åˆ†ï¼‰ | ç»„å†…å‡å€¼å½’ä¸€åŒ– |
| **é€‚ç”¨åœºæ™¯** | è¿‡ç¨‹å¯¼å‘ã€é•¿åºåˆ— | ç»“æœå¯¼å‘ã€æ•°å­¦æ¨ç† |
| **è®­ç»ƒé€Ÿåº¦** | è¾ƒæ…¢ | è¾ƒå¿« |

---

## ğŸ”§ å‰ç½®æ¡ä»¶

### ç¡¬ä»¶è¦æ±‚

```
æœ€ä½é…ç½®ï¼š
- GPU: 2 å¼  24GB GPUï¼ˆå¦‚ RTX 3090ï¼‰
- å†…å­˜: 64GB
- å­˜å‚¨: 100GB

æ¨èé…ç½®ï¼š
- GPU: 4-8 å¼  40GB GPUï¼ˆå¦‚ A100/H100ï¼‰
- å†…å­˜: 128GB+
- å­˜å‚¨: 500GB+
```

### è½¯ä»¶ä¾èµ–

```bash
# å®‰è£… verlï¼ˆåŒ…å« vLLMï¼‰
pip install -e .[test,vllm]

# æˆ–ä½¿ç”¨ SGLang
pip install -e .[test,sglang]

# éªŒè¯å®‰è£…
python -c "import verl; print(verl.__version__)"
```

### æ•°æ®å‡†å¤‡

PPO è®­ç»ƒéœ€è¦å‡†å¤‡ Parquet æ ¼å¼çš„æ•°æ®é›†ï¼š

```bash
# 1. å¤„ç† GSM8K æ•°æ®é›†
python examples/data_preprocess/gsm8k.py \
    --local_save_dir ~/data/gsm8k

# 2. éªŒè¯æ•°æ®æ ¼å¼
python learning_notes/01_å¿«é€Ÿä¸Šæ‰‹/check_data.py ~/data/gsm8k/train.parquet

# 3. ç¡®è®¤æ–‡ä»¶å­˜åœ¨
ls ~/data/gsm8k/
# è¾“å‡º: train.parquet  test.parquet
```

### æ¨¡å‹å‡†å¤‡

ç¡®ä¿èƒ½è®¿é—® HuggingFace æ¨¡å‹ï¼š

```bash
# è®¾ç½® HuggingFace tokenï¼ˆå¦‚æœéœ€è¦ï¼‰
export HF_TOKEN=your_token_here

# æˆ–ä½¿ç”¨é•œåƒç«™ç‚¹
export HF_ENDPOINT=https://hf-mirror.com

# é¢„ä¸‹è½½æ¨¡å‹ï¼ˆå¯é€‰ï¼Œæ¨èï¼‰
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æœ€ç®€å•çš„ä¾‹å­ï¼ˆå•æœº 2 GPUï¼‰

```bash
# ä½¿ç”¨ Gemma 2B æ¨¡å‹åœ¨ GSM8K ä¸Šè®­ç»ƒ
bash examples/ppo_trainer/run_gemma.sh

# è‡ªå®šä¹‰å‚æ•°
bash examples/ppo_trainer/run_gemma.sh \
    trainer.n_gpus_per_node=2 \
    trainer.total_epochs=20 \
    actor_rollout_ref.actor.optim.lr=5e-7
```

**é¢„æœŸè¾“å‡ºï¼š**
```
[2026-01-28 10:00:00] Initializing Ray...
[2026-01-28 10:00:05] Creating Actor worker pool...
[2026-01-28 10:00:10] Creating Critic worker pool...
[2026-01-28 10:00:15] Creating Rollout worker pool...
[2026-01-28 10:00:20] Starting training...

Epoch 0:
  rollout: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 512/512 [00:30<00:00]
  compute_values: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 512/512 [00:10<00:00]
  train_actor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:20<00:00]
  train_critic: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:20<00:00]
  metrics: reward_mean=0.35, kl=0.002, actor_loss=0.234

âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åˆ°: ./checkpoints/gemma2b_function_rm/
```

### ä½¿ç”¨ Qwen æ¨¡å‹ï¼ˆæ¨èï¼‰

```bash
# Qwen2.5-0.5Bï¼ˆæœ€å°æ¨¡å‹ï¼Œå¿«é€Ÿæµ‹è¯•ï¼‰
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=gae \
    data.train_files=$HOME/data/gsm8k/train.parquet \
    data.val_files=$HOME/data/gsm8k/test.parquet \
    data.train_batch_size=256 \
    actor_rollout_ref.model.path=Qwen/Qwen2.5-0.5B-Instruct \
    critic.model.path=Qwen/Qwen2.5-0.5B-Instruct \
    actor_rollout_ref.actor.ppo_mini_batch_size=64 \
    actor_rollout_ref.actor.optim.lr=1e-6 \
    critic.optim.lr=1e-5 \
    trainer.n_gpus_per_node=2 \
    trainer.total_epochs=15

# Qwen2-7Bï¼ˆæ›´å¼ºæ€§èƒ½ï¼‰
bash examples/ppo_trainer/run_qwen2-7b_seq_balance.sh
```

### ä½¿ç”¨ Reward Model

```bash
# ä½¿ç”¨ç‹¬ç«‹çš„ Reward Modelï¼ˆè€Œé rule-based rewardï¼‰
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=gae \
    data.train_files=$HOME/data/hh_rlhf/train.parquet \
    actor_rollout_ref.model.path=Qwen/Qwen2.5-7B-Instruct \
    critic.model.path=Qwen/Qwen2.5-7B-Instruct \
    reward_model.enable=True \
    reward_model.path=OpenAssistant/reward-model-deberta-v3-large \
    trainer.n_gpus_per_node=8

# æˆ–å‚è€ƒç°æœ‰è„šæœ¬
bash examples/ppo_trainer/run_qwen2-7b_rm.sh
```

---

## ğŸ“– è¯¦ç»†é…ç½®

### æ ¸å¿ƒé…ç½®å‚æ•°

PPO è®­ç»ƒçš„é…ç½®é€šè¿‡ Hydra ç®¡ç†ï¼Œå¯ä»¥åœ¨å‘½ä»¤è¡Œè¦†ç›– YAML é…ç½®ã€‚

#### 1. ç®—æ³•é…ç½® (`algorithm.*`)

```yaml
algorithm:
  adv_estimator: gae              # å¿…é¡»è®¾ç½®ä¸º gaeï¼ˆPPO ç®—æ³•ï¼‰
  gamma: 0.99                     # æŠ˜æ‰£å› å­ï¼ˆæœªæ¥å¥–åŠ±æƒé‡ï¼‰
  lam: 0.95                       # GAE lambdaï¼ˆbias-variance tradeoffï¼‰
  use_kl_in_reward: False         # æ˜¯å¦åœ¨ reward ä¸­åŠ  KL penalty
  kl_penalty: kl                  # KL penalty ç±»å‹: kl, abs, mse
  kl_ctrl:
    type: fixed                   # KL æ§åˆ¶å™¨: fixed æˆ– adaptive
    kl_coef: 0.001                # KL ç³»æ•°ï¼ˆåˆå§‹å€¼ï¼‰
```

**å‚æ•°è¯¦è§£ï¼š**

- **gammaï¼ˆæŠ˜æ‰£å› å­ï¼‰**ï¼šæ§åˆ¶æœªæ¥å¥–åŠ±çš„æƒé‡
  - `0.99`ï¼ˆé»˜è®¤ï¼‰ï¼šé‡è§†é•¿æœŸå¥–åŠ±
  - `0.95`ï¼šæ›´æ³¨é‡è¿‘æœŸå¥–åŠ±
  - `1.0`ï¼šMonte Carloï¼Œæ— æŠ˜æ‰£

- **lamï¼ˆGAE lambdaï¼‰**ï¼šå¹³è¡¡åå·®å’Œæ–¹å·®
  - `0.95`ï¼ˆé»˜è®¤ï¼‰ï¼šæ ‡å‡†é€‰æ‹©
  - `1.0`ï¼šä½åå·®ï¼Œé«˜æ–¹å·®
  - `0.0`ï¼šé«˜åå·®ï¼Œä½æ–¹å·®ï¼ˆ1-step TDï¼‰

#### 2. æ•°æ®é…ç½® (`data.*`)

```yaml
data:
  train_files: ~/data/gsm8k/train.parquet    # è®­ç»ƒæ•°æ®è·¯å¾„
  val_files: ~/data/gsm8k/test.parquet       # éªŒè¯æ•°æ®è·¯å¾„
  train_batch_size: 512                      # å…¨å±€ batch sizeï¼ˆprompt æ•°é‡ï¼‰
  max_prompt_length: 1024                    # æœ€å¤§ prompt é•¿åº¦
  max_response_length: 512                   # æœ€å¤§ response é•¿åº¦
  filter_overlong_prompts: True              # è¿‡æ»¤è¿‡é•¿çš„ prompt
  truncation: error                          # æˆªæ–­ç­–ç•¥: error, left, right
```

**é‡è¦è¯´æ˜ï¼š**

- `train_batch_size`ï¼šå†³å®šæ¯è½®ç”Ÿæˆå¤šå°‘æ¡æ•°æ®
  - å°æ¨¡å‹ï¼ˆ<7Bï¼‰ï¼š256-512
  - å¤§æ¨¡å‹ï¼ˆ7B-70Bï¼‰ï¼š128-256
  - è¶…å¤§æ¨¡å‹ï¼ˆ>70Bï¼‰ï¼š64-128

- **å“åº”æ•°é‡** = `train_batch_size Ã— rollout.n`
  - PPO é€šå¸¸ `rollout.n=1`ï¼ˆæ¯ä¸ª prompt ä¸€æ¡å“åº”ï¼‰
  - GRPO éœ€è¦ `rollout.n>1`ï¼ˆæ¯ä¸ª prompt å¤šæ¡å“åº”ï¼‰

#### 3. Actor é…ç½® (`actor_rollout_ref.actor.*`)

```yaml
actor_rollout_ref:
  actor:
    # ä¼˜åŒ–å™¨é…ç½®
    optim:
      lr: 1e-6                          # å­¦ä¹ ç‡
      weight_decay: 0.01                # æƒé‡è¡°å‡
      warmup_steps: 0                   # warmup æ­¥æ•°

    # PPO å‚æ•°
    ppo_mini_batch_size: 128            # PPO mini-batch å¤§å°
    ppo_micro_batch_size_per_gpu: 4     # æ¯å¼  GPU çš„ micro-batch
    ppo_epochs: 1                       # PPO æ›´æ–°è½®æ•°
    clip_ratio: 0.2                     # PPO clipping èŒƒå›´

    # KL æ•£åº¦æ§åˆ¶
    use_kl_loss: False                  # æ˜¯å¦ä½¿ç”¨ KL loss
    kl_loss_coef: 0.001                 # KL loss ç³»æ•°
    kl_loss_type: kl                    # KL è®¡ç®—ç±»å‹

    # FSDP é…ç½®
    fsdp_config:
      param_offload: False              # å‚æ•°å¸è½½åˆ° CPU
      optimizer_offload: False          # ä¼˜åŒ–å™¨å¸è½½åˆ° CPU
      gradient_checkpointing: True      # æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆçœæ˜¾å­˜ï¼‰
```

**å…³é”®å‚æ•°ï¼š**

- **ppo_mini_batch_size**ï¼šå†³å®š PPO æ›´æ–°çš„ batch å¤§å°
  - å¿…é¡»èƒ½è¢« `train_batch_size Ã— rollout.n` æ•´é™¤
  - ç¤ºä¾‹ï¼š`train_batch_size=512, rollout.n=1 â†’ ppo_mini_batch_size=128`
  - æ›´å¤§çš„å€¼ â†’ æ›´ç¨³å®šï¼Œä½†æ›´æ…¢

- **ppo_epochs**ï¼šå¯¹åŒä¸€æ‰¹æ•°æ®æ›´æ–°å‡ æ¬¡
  - `1`ï¼ˆé»˜è®¤ï¼‰ï¼šæ¯æ‰¹æ•°æ®åªç”¨ä¸€æ¬¡ï¼ˆon-policyï¼‰
  - `2-4`ï¼šå¤ç”¨æ•°æ®ï¼Œæé«˜æ ·æœ¬æ•ˆç‡
  - è¿‡å¤§å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆ

- **clip_ratio**ï¼šPPO clipping çš„èŒƒå›´
  - `0.2`ï¼ˆé»˜è®¤ï¼‰ï¼šæ ‡å‡†é€‰æ‹©
  - æ›´å°ï¼ˆ0.1ï¼‰ï¼šæ›´ä¿å®ˆçš„æ›´æ–°
  - æ›´å¤§ï¼ˆ0.3ï¼‰ï¼šæ›´æ¿€è¿›çš„æ›´æ–°

#### 4. Critic é…ç½® (`critic.*`)

```yaml
critic:
  model:
    path: Qwen/Qwen2.5-7B-Instruct      # Critic æ¨¡å‹è·¯å¾„ï¼ˆé€šå¸¸ä¸ Actor ç›¸åŒï¼‰
    enable_gradient_checkpointing: True # æ¢¯åº¦æ£€æŸ¥ç‚¹

  optim:
    lr: 1e-5                            # Critic å­¦ä¹ ç‡ï¼ˆé€šå¸¸æ¯” Actor å¤§ 10xï¼‰

  ppo_mini_batch_size: 128              # Critic mini-batchï¼ˆé€šå¸¸ä¸ Actor ç›¸åŒï¼‰
  ppo_micro_batch_size_per_gpu: 4       # æ¯å¼  GPU çš„ micro-batch
  ppo_epochs: 1                         # Critic æ›´æ–°è½®æ•°ï¼ˆé»˜è®¤åŒ Actorï¼‰
```

**é‡è¦ï¼š**

- Critic å­¦ä¹ ç‡é€šå¸¸æ˜¯ Actor çš„ **10 å€**
  - Actor: `1e-6`, Critic: `1e-5`
  - Actor: `5e-7`, Critic: `5e-6`

- Critic æ¨¡å‹é€šå¸¸ä¸ Actor ä½¿ç”¨ **ç›¸åŒæ¶æ„**
  - åˆå§‹åŒ–ï¼šä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½
  - è®­ç»ƒï¼švalue head ä¼šè¢«æ·»åŠ åˆ°æ¨¡å‹ä¸Š

#### 5. Rollout é…ç½® (`actor_rollout_ref.rollout.*`)

```yaml
actor_rollout_ref:
  rollout:
    name: vllm                          # æ¨ç†å¼•æ“: vllm, sglang, hf
    tensor_model_parallel_size: 2       # TPï¼ˆå¼ é‡å¹¶è¡Œï¼‰å¤§å°
    gpu_memory_utilization: 0.4         # GPU æ˜¾å­˜åˆ©ç”¨ç‡

    # ç”Ÿæˆå‚æ•°
    temperature: 1.0                    # é‡‡æ ·æ¸©åº¦
    top_p: 1.0                          # nucleus sampling
    top_k: -1                           # top-k sampling
    n: 1                                # æ¯ä¸ª prompt ç”Ÿæˆå‡ æ¡ï¼ˆPPO=1ï¼‰

    log_prob_micro_batch_size_per_gpu: 4  # è®¡ç®— log_prob çš„ batch size
```

**Rollout Engine é€‰æ‹©ï¼š**

| å¼•æ“ | é€Ÿåº¦ | æ˜¾å­˜ | åŠŸèƒ½ | æ¨èåœºæ™¯ |
|------|------|------|------|----------|
| **vLLM** | â­â­â­â­â­ | ä¸­ç­‰ | å…¨é¢ | é¦–é€‰ |
| **SGLang** | â­â­â­â­â­ | è¾ƒä½ | å¤šè½®å¯¹è¯ | Agentã€å¤šè½® |
| **TRT-LLM** | â­â­â­â­â­ | æœ€ä½ | é«˜æ€§èƒ½ | ç”Ÿäº§ç¯å¢ƒ |
| **HF** | â­â­ | è¾ƒé«˜ | åŸºç¡€ | è°ƒè¯• |

#### 6. Trainer é…ç½® (`trainer.*`)

```yaml
trainer:
  n_gpus_per_node: 2                    # æ¯ä¸ªèŠ‚ç‚¹çš„ GPU æ•°é‡
  nnodes: 1                             # èŠ‚ç‚¹æ•°é‡
  total_epochs: 15                      # æ€»è®­ç»ƒè½®æ•°
  save_freq: 5                          # æ¯ N è½®ä¿å­˜ä¸€æ¬¡
  test_freq: 5                          # æ¯ N è½®æµ‹è¯•ä¸€æ¬¡

  # æ—¥å¿—é…ç½®
  logger: '["console","wandb"]'         # æ—¥å¿—å·¥å…·
  project_name: verl_ppo                # é¡¹ç›®åç§°
  experiment_name: qwen2.5_gsm8k        # å®éªŒåç§°

  # Critic warmup
  critic_warmup: 0                      # Critic é¢„çƒ­æ­¥æ•°
```

---

## ğŸ’¡ è¿è¡Œç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šQwen2.5-0.5B å¿«é€Ÿæµ‹è¯•

```bash
# æœ€å°é…ç½®ï¼Œé€‚åˆå¿«é€ŸéªŒè¯
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=gae \
    data.train_files=$HOME/data/gsm8k/train.parquet \
    data.val_files=$HOME/data/gsm8k/test.parquet \
    data.train_batch_size=256 \
    actor_rollout_ref.model.path=Qwen/Qwen2.5-0.5B-Instruct \
    critic.model.path=Qwen/Qwen2.5-0.5B-Instruct \
    actor_rollout_ref.actor.ppo_mini_batch_size=64 \
    actor_rollout_ref.actor.ppo_micro_batch_size=2 \
    critic.ppo_micro_batch_size=2 \
    trainer.n_gpus_per_node=2 \
    trainer.total_epochs=15 \
    trainer.logger=console

# é¢„æœŸç»“æœï¼ˆ15 epochs åï¼‰ï¼š
# - è®­ç»ƒæ—¶é—´: ~30 åˆ†é’Ÿ
# - GSM8K å‡†ç¡®ç‡: ~56.7%ï¼ˆä»é¢„è®­ç»ƒçš„ 36.4% æå‡ï¼‰
# - æ¨¡å‹ä¿å­˜: ./checkpoints/
```

### ç¤ºä¾‹ 2ï¼šQwen2-7B å®Œæ•´è®­ç»ƒ

```bash
# æ¨èé…ç½®ï¼Œè·å¾—æ›´å¥½æ•ˆæœ
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=gae \
    data.train_files=$HOME/data/gsm8k/train.parquet \
    data.val_files=$HOME/data/gsm8k/test.parquet \
    data.train_batch_size=512 \
    actor_rollout_ref.model.path=Qwen/Qwen2-7B-Instruct \
    critic.model.path=Qwen/Qwen2-7B-Instruct \
    actor_rollout_ref.actor.optim.lr=1e-6 \
    critic.optim.lr=1e-5 \
    actor_rollout_ref.actor.ppo_mini_batch_size=128 \
    actor_rollout_ref.rollout.tensor_model_parallel_size=2 \
    trainer.n_gpus_per_node=8 \
    trainer.total_epochs=20

# æˆ–ç›´æ¥ä½¿ç”¨é¢„è®¾è„šæœ¬
bash examples/ppo_trainer/run_qwen2-7b_seq_balance.sh
```

### ç¤ºä¾‹ 3ï¼šä½¿ç”¨ Reward Modelï¼ˆRLHFï¼‰

```bash
# äººç±»åå¥½å¯¹é½è®­ç»ƒ
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=gae \
    data.train_files=$HOME/data/hh_rlhf/train.parquet \
    data.val_files=$HOME/data/hh_rlhf/test.parquet \
    actor_rollout_ref.model.path=Qwen/Qwen2.5-7B-Instruct \
    critic.model.path=Qwen/Qwen2.5-7B-Instruct \
    reward_model.enable=True \
    reward_model.path=OpenAssistant/reward-model-deberta-v3-large \
    algorithm.use_kl_in_reward=True \
    algorithm.kl_ctrl.kl_coef=0.01 \
    trainer.n_gpus_per_node=8

# å‚è€ƒè„šæœ¬
bash examples/ppo_trainer/run_qwen2-7b_rm.sh
```

### ç¤ºä¾‹ 4ï¼šå¯ç”¨ KL Lossï¼ˆæ¨èï¼‰

```bash
# ä½¿ç”¨ KL Loss è€Œé KL Reward
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=gae \
    data.train_files=$HOME/data/gsm8k/train.parquet \
    actor_rollout_ref.model.path=Qwen/Qwen2.5-7B-Instruct \
    critic.model.path=Qwen/Qwen2.5-7B-Instruct \
    actor_rollout_ref.actor.use_kl_loss=True \
    actor_rollout_ref.actor.kl_loss_coef=0.001 \
    actor_rollout_ref.actor.kl_loss_type=kl \
    algorithm.use_kl_in_reward=False \
    trainer.n_gpus_per_node=8

# KL Loss ä¼˜åŠ¿ï¼š
# âœ… æ¢¯åº¦æ›´ç›´æ¥ï¼Œè®­ç»ƒæ›´ç¨³å®š
# âœ… ä¸å½±å“ reward è®¾è®¡
# âœ… æ›´å®¹æ˜“è°ƒå‚
```

### ç¤ºä¾‹ 5ï¼šå¤šèŠ‚ç‚¹åˆ†å¸ƒå¼è®­ç»ƒ

```bash
# ä½¿ç”¨ 4 ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹ 8 GPUï¼ˆæ€»å…± 32 GPUï¼‰
# èŠ‚ç‚¹ 0 (ä¸»èŠ‚ç‚¹)
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=gae \
    data.train_files=$HOME/data/gsm8k/train.parquet \
    data.train_batch_size=1024 \
    actor_rollout_ref.model.path=Qwen/Qwen2-72B-Instruct \
    critic.model.path=Qwen/Qwen2-72B-Instruct \
    actor_rollout_ref.rollout.tensor_model_parallel_size=4 \
    trainer.n_gpus_per_node=8 \
    trainer.nnodes=4 \
    trainer.node_rank=0 \
    trainer.master_addr=192.168.1.100 \
    trainer.master_port=29500

# èŠ‚ç‚¹ 1-3ï¼ˆå·¥ä½œèŠ‚ç‚¹ï¼‰
# å°† trainer.node_rank è®¾ç½®ä¸º 1, 2, 3
# master_addr ä¿æŒä¸€è‡´
```

### ç¤ºä¾‹ 6ï¼šä½¿ç”¨ Megatron-LM åç«¯

```bash
# è¶…å¤§æ¨¡å‹è®­ç»ƒï¼ˆä½¿ç”¨å¼ é‡å¹¶è¡Œ + æµæ°´çº¿å¹¶è¡Œï¼‰
bash examples/ppo_trainer/run_qwen2-7b_math_megatron.sh

# è‡ªå®šä¹‰ Megatron é…ç½®
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=gae \
    actor_rollout_ref.actor.strategy=megatron \
    actor_rollout_ref.actor.megatron_config.tensor_model_parallel_size=4 \
    actor_rollout_ref.actor.megatron_config.pipeline_model_parallel_size=2 \
    critic.strategy=megatron \
    critic.megatron_config.tensor_model_parallel_size=4 \
    trainer.n_gpus_per_node=8
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: OOMï¼ˆæ˜¾å­˜ä¸è¶³ï¼‰æ€ä¹ˆåŠï¼Ÿ

**ç—‡çŠ¶ï¼š**
```
CUDA out of memory. Tried to allocate XXX GiB
```

**è§£å†³æ–¹æ¡ˆï¼š**

```bash
# æ–¹æ³• 1: å‡å° batch size
data.train_batch_size=256  # ä» 512 å‡å°åˆ° 256
actor_rollout_ref.actor.ppo_mini_batch_size=64  # ç›¸åº”å‡å°

# æ–¹æ³• 2: å‡å° micro_batch_size
actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=2  # ä» 4 å‡å°åˆ° 2
critic.ppo_micro_batch_size_per_gpu=2

# æ–¹æ³• 3: å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
actor_rollout_ref.actor.fsdp_config.gradient_checkpointing=True
critic.model.enable_gradient_checkpointing=True

# æ–¹æ³• 4: å¯ç”¨å‚æ•°å¸è½½ï¼ˆä¼šå˜æ…¢ï¼‰
actor_rollout_ref.actor.fsdp_config.param_offload=True
actor_rollout_ref.actor.fsdp_config.optimizer_offload=True

# æ–¹æ³• 5: å‡å° rollout GPU æ˜¾å­˜å ç”¨
actor_rollout_ref.rollout.gpu_memory_utilization=0.3  # ä» 0.4 å‡å°

# æ–¹æ³• 6: å¢å¤§å¼ é‡å¹¶è¡Œ
actor_rollout_ref.rollout.tensor_model_parallel_size=4  # ä» 2 å¢å¤§åˆ° 4
```

### Q2: è®­ç»ƒä¸ç¨³å®šï¼Œloss çˆ†ç‚¸æ€ä¹ˆåŠï¼Ÿ

**ç—‡çŠ¶ï¼š**
```
actor_loss: nan
æˆ–
actor_loss: 1e10
```

**è§£å†³æ–¹æ¡ˆï¼š**

```bash
# æ–¹æ³• 1: é™ä½å­¦ä¹ ç‡
actor_rollout_ref.actor.optim.lr=5e-7  # ä» 1e-6 é™ä½
critic.optim.lr=5e-6

# æ–¹æ³• 2: å‡å° clip_ratio
actor_rollout_ref.actor.clip_ratio=0.1  # ä» 0.2 å‡å°

# æ–¹æ³• 3: å¯ç”¨æ¢¯åº¦è£å‰ª
actor_rollout_ref.actor.grad_clip=1.0
critic.grad_clip=1.0

# æ–¹æ³• 4: å¢åŠ  KL çº¦æŸ
actor_rollout_ref.actor.use_kl_loss=True
actor_rollout_ref.actor.kl_loss_coef=0.01  # å¢å¤§ç³»æ•°

# æ–¹æ³• 5: å‡å° ppo_epochs
actor_rollout_ref.actor.ppo_epochs=1  # é¿å…è¿‡åº¦ä¼˜åŒ–
```

### Q3: Reward å§‹ç»ˆä¸º 0 æ€ä¹ˆåŠï¼Ÿ

**æ£€æŸ¥æ­¥éª¤ï¼š**

```bash
# 1. æ£€æŸ¥æ•°æ®æ ¼å¼
python learning_notes/02_æ•°æ®å‡†å¤‡/data_quality_check.py ~/data/gsm8k/train.parquet

# 2. æ£€æŸ¥ reward_model å­—æ®µ
python -c "
import pandas as pd
df = pd.read_parquet('~/data/gsm8k/train.parquet')
print(df.iloc[0]['reward_model'])
# åº”è¯¥è¾“å‡º: {'style': 'rule', 'ground_truth': '...'}
"

# 3. æ£€æŸ¥ data_source æ˜¯å¦æ­£ç¡®
python -c "
import pandas as pd
df = pd.read_parquet('~/data/gsm8k/train.parquet')
print(df.iloc[0]['data_source'])
# åº”è¯¥è¾“å‡º: openai/gsm8k
"

# 4. ç¡®è®¤ Reward å‡½æ•°å·²æ³¨å†Œ
# æŸ¥çœ‹ verl/trainer/ppo/reward_score/gsm8k.py
# ç¡®ä¿ data_source åŒ¹é…

# 5. æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ä¸­çš„ reward è®¡ç®—
# åº”è¯¥çœ‹åˆ°ç±»ä¼¼ï¼š
# [RewardManager] Computing rewards for data_source=openai/gsm8k
# [GSM8K Reward] Correct: 123/512, Accuracy: 0.24
```

### Q4: Critic loss ä¸ä¸‹é™æ€ä¹ˆåŠï¼Ÿ

**å¯èƒ½åŸå› ï¼š**

```bash
# åŸå›  1: Critic å­¦ä¹ ç‡å¤ªå°
critic.optim.lr=1e-5  # åº”è¯¥æ˜¯ Actor çš„ 10 å€

# åŸå›  2: Critic warmup ä¸è¶³
trainer.critic_warmup=10  # å…ˆå•ç‹¬è®­ç»ƒ Critic

# åŸå›  3: Reward signal å¤ªå¼±
# æ£€æŸ¥ reward çš„åˆ†å¸ƒ
# åº”è¯¥çœ‹åˆ° reward_mean åœ¨å˜åŒ–ï¼Œä¸æ˜¯å§‹ç»ˆä¸º 0

# åŸå›  4: batch size å¤ªå°
data.train_batch_size=512  # å¢å¤§ batch size
```

### Q5: å¦‚ä½•é€‰æ‹© GAE çš„ gamma å’Œ lam å‚æ•°ï¼Ÿ

**æ¨èé…ç½®ï¼š**

```bash
# æ ‡å‡†é…ç½®ï¼ˆé€‚åˆå¤§å¤šæ•°ä»»åŠ¡ï¼‰
algorithm.gamma=0.99
algorithm.lam=0.95

# çŸ­åºåˆ—ä»»åŠ¡ï¼ˆå¦‚åˆ†ç±»ï¼‰
algorithm.gamma=0.95
algorithm.lam=0.9

# é•¿åºåˆ—ä»»åŠ¡ï¼ˆå¦‚é•¿æ–‡æœ¬ç”Ÿæˆï¼‰
algorithm.gamma=0.99
algorithm.lam=0.97

# è°ƒè¯•æŠ€å·§ï¼š
# - å¦‚æœè®­ç»ƒä¸ç¨³å®šï¼ˆé«˜æ–¹å·®ï¼‰ï¼šå‡å° lamï¼ˆå¦‚ 0.9ï¼‰
# - å¦‚æœæ”¶æ•›æ…¢ï¼ˆé«˜åå·®ï¼‰ï¼šå¢å¤§ lamï¼ˆå¦‚ 0.98ï¼‰
```

### Q6: vLLM å’Œ SGLang å¦‚ä½•é€‰æ‹©ï¼Ÿ

**é€‰æ‹©æŒ‡å—ï¼š**

```bash
# ä½¿ç”¨ vLLMï¼ˆæ¨èï¼‰
actor_rollout_ref.rollout.name=vllm
# ä¼˜åŠ¿ï¼šæˆç†Ÿç¨³å®šï¼Œæ€§èƒ½ä¼˜ç§€ï¼Œç¤¾åŒºæ”¯æŒå¥½
# åŠ£åŠ¿ï¼šå¤šè½®å¯¹è¯æ”¯æŒä¸€èˆ¬

# ä½¿ç”¨ SGLang
actor_rollout_ref.rollout.name=sglang
# ä¼˜åŠ¿ï¼šå¤šè½®å¯¹è¯æ”¯æŒæ›´å¥½ï¼ŒAgent RL æ¨è
# åŠ£åŠ¿ï¼šè¾ƒæ–°ï¼Œæ–‡æ¡£ç›¸å¯¹å°‘

# å¦‚æœä»»åŠ¡æ˜¯å•è½® â†’ vLLM
# å¦‚æœä»»åŠ¡æ˜¯å¤šè½®/Agent â†’ SGLang
```

### Q7: å¦‚ä½•ç›‘æ§è®­ç»ƒè¿›åº¦ï¼Ÿ

**ä½¿ç”¨ TensorBoardï¼š**

```bash
# è®­ç»ƒæ—¶å¯ç”¨ tensorboard
trainer.logger='["console","tensorboard"]'

# åœ¨å¦ä¸€ä¸ªç»ˆç«¯å¯åŠ¨ TensorBoard
tensorboard --logdir ./runs/

# è®¿é—® http://localhost:6006
```

**ä½¿ç”¨ W&Bï¼ˆæ¨èï¼‰ï¼š**

```bash
# 1. å®‰è£… wandb
pip install wandb

# 2. ç™»å½•
wandb login

# 3. å¯ç”¨ wandb
trainer.logger='["console","wandb"]' \
trainer.project_name='my_ppo_project' \
trainer.experiment_name='qwen2.5_gsm8k_v1'

# 4. æŸ¥çœ‹è®­ç»ƒæ›²çº¿
# è®¿é—® https://wandb.ai/<your-username>/my_ppo_project
```

**å…³é”®æŒ‡æ ‡ï¼š**

```yaml
# éœ€è¦å…³æ³¨çš„æŒ‡æ ‡ï¼š
- reward_mean: å¹³å‡å¥–åŠ±ï¼ˆåº”è¯¥ä¸Šå‡ï¼‰
- actor_loss: Actor æŸå¤±ï¼ˆåº”è¯¥ä¸‹é™ï¼‰
- critic_loss: Critic æŸå¤±ï¼ˆåº”è¯¥ä¸‹é™ï¼‰
- kl_divergence: KL æ•£åº¦ï¼ˆåº”è¯¥ä¿æŒåœ¨åˆç†èŒƒå›´ï¼Œå¦‚ < 0.1ï¼‰
- ppo_ratio: PPO ratioï¼ˆåº”è¯¥åœ¨ [0.8, 1.2] ä¹‹é—´ï¼‰
- grad_norm: æ¢¯åº¦èŒƒæ•°ï¼ˆä¸åº”è¯¥çˆ†ç‚¸ï¼‰

# å¥åº·çš„è®­ç»ƒæ›²çº¿ï¼š
# - reward_mean: ç¨³æ­¥ä¸Šå‡
# - kl_divergence: ç¼“æ…¢å¢é•¿ï¼Œä½†ä¸è¶…è¿‡ 0.1-0.2
# - actor_loss/critic_loss: ç¨³æ­¥ä¸‹é™
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†

### Qwen2.5-0.5B on GSM8K

```
é¢„è®­ç»ƒæ¨¡å‹å‡†ç¡®ç‡: 36.4%
PPO è®­ç»ƒåå‡†ç¡®ç‡: 56.7%
è®­ç»ƒæ—¶é—´: ~30 åˆ†é’Ÿï¼ˆ2x RTX 3090ï¼‰
é…ç½®: batch_size=256, epochs=15

å‘½ä»¤:
bash examples/ppo_trainer/run_gemma.sh \
    actor_rollout_ref.model.path=Qwen/Qwen2.5-0.5B-Instruct \
    critic.model.path=Qwen/Qwen2.5-0.5B-Instruct
```

### Qwen2-7B on GSM8K

```
é¢„è®­ç»ƒæ¨¡å‹å‡†ç¡®ç‡: ~65%
PPO è®­ç»ƒåå‡†ç¡®ç‡: ~75%
è®­ç»ƒæ—¶é—´: ~2 å°æ—¶ï¼ˆ8x A100ï¼‰
é…ç½®: batch_size=512, epochs=20

å‘½ä»¤:
bash examples/ppo_trainer/run_qwen2-7b_seq_balance.sh
```

---

## ğŸ”— å‚è€ƒèµ„æ–™

### å®˜æ–¹æ–‡æ¡£

- [PPO ç®—æ³•åŸç†](https://spinningup.openai.com/en/latest/algorithms/ppo.html)
- [PPO è®ºæ–‡](https://arxiv.org/abs/1707.06347)
- [verl æ–‡æ¡£](https://verl.readthedocs.io/)

### å­¦ä¹ ç¬”è®°

- [03_RLç®—æ³•/PPO_è¯¦è§£.md](../../learning_notes/03_RLç®—æ³•/PPO_è¯¦è§£.md) - PPO ç®—æ³•æºç çº§è¯¦è§£
- [03_RLç®—æ³•/03_RLç®—æ³•æ¦‚è§ˆ.md](../../learning_notes/03_RLç®—æ³•/03_RLç®—æ³•æ¦‚è§ˆ.md) - ç®—æ³•å¯¹æ¯”ä¸é€‰æ‹©
- [01_å¿«é€Ÿä¸Šæ‰‹/ray_trainer_è¯¦è§£.md](../../learning_notes/01_å¿«é€Ÿä¸Šæ‰‹/ray_trainer_è¯¦è§£.md) - è®­ç»ƒæµç¨‹è¯¦è§£

### ç›¸å…³ç¤ºä¾‹

- `examples/grpo_trainer/` - GRPO è®­ç»ƒç¤ºä¾‹ï¼ˆæ—  Criticï¼‰
- `examples/sft/` - SFT è®­ç»ƒï¼ˆPPO çš„å‰ç½®æ­¥éª¤ï¼‰
- `examples/data_preprocess/` - æ•°æ®é¢„å¤„ç†

### è®ºæ–‡

- [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) - InstructGPT (PPO + RLHF)
- [Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593) - PPO åœ¨ LLM çš„æ—©æœŸåº”ç”¨

---

**åˆ›å»ºæ—¶é—´**: 2026-01-28
**é€‚ç”¨ç‰ˆæœ¬**: verl v0.2+
**ç»´æŠ¤è€…**: verl team
