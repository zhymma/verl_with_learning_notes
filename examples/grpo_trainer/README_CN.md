# GRPO è®­ç»ƒå™¨ (Group Relative Policy Optimization)

> æ— éœ€ Critic æ¨¡å‹çš„é«˜æ•ˆå¼ºåŒ–å­¦ä¹ è®­ç»ƒ

---

## ğŸ“‹ æ¦‚è¿°

**GRPO (Group Relative Policy Optimization)** æ˜¯ DeepSeek åœ¨ 2024 å¹´æå‡ºçš„åˆ›æ–°å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚å®ƒé€šè¿‡**ç»„å†…ç›¸å¯¹æ¯”è¾ƒ**æ¶ˆé™¤äº†å¯¹ Critic æ¨¡å‹çš„éœ€æ±‚ï¼Œå¤§å¹…é™ä½äº†è®­ç»ƒæˆæœ¬å’Œæ˜¾å­˜å ç”¨ã€‚

### æ ¸å¿ƒç‰¹ç‚¹

- âœ… **æ— éœ€ Critic**ï¼šä¸éœ€è¦è®­ç»ƒä»·å€¼å‡½æ•°æ¨¡å‹ï¼ŒèŠ‚çœ 50% æ˜¾å­˜
- âœ… **ç»„é‡‡æ ·**ï¼šä¸ºæ¯ä¸ª prompt ç”Ÿæˆå¤šä¸ªå“åº”ï¼Œç»„å†…æ¯”è¾ƒ
- âœ… **ç›¸å¯¹ä¼˜åŠ¿**ï¼šä½¿ç”¨ç»„å†…å‡å€¼å½’ä¸€åŒ–ï¼Œè‡ªåŠ¨å½¢æˆ baseline
- âœ… **è®­ç»ƒé«˜æ•ˆ**ï¼šæ›´å¿«çš„è®­ç»ƒé€Ÿåº¦ï¼Œæ›´å°‘çš„ GPU éœ€æ±‚
- âœ… **é€‚åˆæ•°å­¦æ¨ç†**ï¼šåœ¨ GSM8Kã€MATH ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²

### é€‚ç”¨åœºæ™¯

| åœºæ™¯ | è¯´æ˜ | æ¨èåº¦ |
|------|------|--------|
| **æ•°å­¦æ¨ç†** | GSM8Kã€MATH ç­‰ç»“æœå¯¼å‘ä»»åŠ¡ | â­â­â­â­â­ |
| **ä»£ç ç”Ÿæˆ** | é€šè¿‡æµ‹è¯•å³å¯åˆ¤æ–­æ­£ç¡®æ€§ | â­â­â­â­â­ |
| **å¿«é€ŸåŸå‹** | ä¸éœ€è¦è®­ç»ƒ Criticï¼Œå¿«é€ŸéªŒè¯æƒ³æ³• | â­â­â­â­â­ |
| **æ˜¾å­˜å—é™** | GPU æ˜¾å­˜ä¸è¶³ï¼Œæ— æ³•åŒæ—¶è®­ç»ƒ Actor + Critic | â­â­â­â­â­ |
| **é—®ç­”ä»»åŠ¡** | æ˜ç¡®çš„å¯¹é”™æ ‡å‡† | â­â­â­â­ |
| **é•¿æ–‡æœ¬ç”Ÿæˆ** | éœ€è¦è¿‡ç¨‹çº§ä¼˜åŒ–ï¼ŒGRPO æ•ˆæœä¸€èˆ¬ | â­â­ |
| **å¯¹è¯è´¨é‡** | éœ€è¦ç²¾ç»†æ§åˆ¶ï¼Œå»ºè®®ç”¨ PPO | â­â­ |

### GRPO vs PPO å¯¹æ¯”

| ç‰¹æ€§ | GRPO | PPO |
|------|------|-----|
| **Critic æ¨¡å‹** | âŒ ä¸éœ€è¦ | âœ… éœ€è¦ |
| **GPU æ˜¾å­˜** | æ›´å°‘ï¼ˆåªæœ‰ Actorï¼‰ | æ›´å¤šï¼ˆActor + Criticï¼‰ |
| **è®­ç»ƒé€Ÿåº¦** | æ›´å¿« | è¾ƒæ…¢ |
| **Baseline** | ç»„å†…æ ·æœ¬å‡å€¼ | Critic çš„ V(s) |
| **ä¼˜åŠ¿ä¼°è®¡** | å½’ä¸€åŒ–çš„ç›¸å¯¹ reward | GAEï¼ˆæ—¶åºå·®åˆ†ï¼‰ |
| **æ¯ä¸ª prompt é‡‡æ ·æ•°** | n > 1ï¼ˆé€šå¸¸ 4-8ï¼‰ | n = 1 |
| **é€‚ç”¨åœºæ™¯** | ç»“æœå¯¼å‘ï¼ˆæ•°å­¦ã€ä»£ç ï¼‰ | è¿‡ç¨‹å¯¼å‘ï¼ˆå¯¹è¯ã€é•¿æ–‡æœ¬ï¼‰ |
| **è®­ç»ƒç¨³å®šæ€§** | ä¾èµ–é‡‡æ ·æ•° n | æ›´ç¨³å®š |

---

## ğŸ”§ å‰ç½®æ¡ä»¶

### ç¡¬ä»¶è¦æ±‚

GRPO æ¯” PPO çš„ç¡¬ä»¶éœ€æ±‚æ›´ä½ï¼ˆæ— éœ€ Criticï¼‰ï¼š

```
æœ€ä½é…ç½®ï¼š
- GPU: 1 å¼  24GB GPUï¼ˆå¦‚ RTX 3090ï¼‰
- å†…å­˜: 32GB
- å­˜å‚¨: 50GB

æ¨èé…ç½®ï¼š
- GPU: 2-4 å¼  40GB GPUï¼ˆå¦‚ A100ï¼‰
- å†…å­˜: 64GB+
- å­˜å‚¨: 200GB+
```

### è½¯ä»¶ä¾èµ–

```bash
# å®‰è£… verlï¼ˆåŒ…å« vLLMï¼‰
pip install -e .[test,vllm]

# æˆ–ä½¿ç”¨ SGLangï¼ˆå¤šè½®å¯¹è¯æ¨èï¼‰
pip install -e .[test,sglang]

# éªŒè¯å®‰è£…
python -c "import verl; print(verl.__version__)"
```

### æ•°æ®å‡†å¤‡

```bash
# 1. å¤„ç† GSM8K æ•°æ®é›†
python examples/data_preprocess/gsm8k.py \
    --local_save_dir ~/data/gsm8k

# 2. éªŒè¯æ•°æ®æ ¼å¼
python learning_notes/01_å¿«é€Ÿä¸Šæ‰‹/check_data.py ~/data/gsm8k/train.parquet

# 3. ç¡®è®¤æ–‡ä»¶å­˜åœ¨
ls ~/data/gsm8k/
# è¾“å‡º: train.parquet  test.parquet
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æœ€ç®€å•çš„ä¾‹å­ï¼ˆå•æœº 2 GPUï¼‰

```bash
# ä½¿ç”¨ Qwen2.5-3B åœ¨ GSM8K ä¸Šå¿«é€Ÿè®­ç»ƒ
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    data.train_files=$HOME/data/gsm8k/train.parquet \
    data.val_files=$HOME/data/gsm8k/test.parquet \
    data.train_batch_size=256 \
    actor_rollout_ref.model.path=Qwen/Qwen2.5-3B-Instruct \
    actor_rollout_ref.rollout.n=4 \
    actor_rollout_ref.actor.ppo_mini_batch_size=256 \
    actor_rollout_ref.actor.use_kl_loss=True \
    actor_rollout_ref.actor.kl_loss_coef=0.001 \
    trainer.n_gpus_per_node=2 \
    trainer.total_epochs=15

# æ³¨æ„ GRPO çš„å…³é”®é…ç½®ï¼š
# - algorithm.adv_estimator=grpo  ï¼ˆè®¾ç½®ä¸º GRPO ç®—æ³•ï¼‰
# - actor_rollout_ref.rollout.n=4  ï¼ˆæ¯ä¸ª prompt ç”Ÿæˆ 4 æ¡å“åº”ï¼‰
# - actor_rollout_ref.actor.use_kl_loss=True  ï¼ˆä½¿ç”¨ KL loss è€Œé KL rewardï¼‰
# - æ²¡æœ‰ critic é…ç½®ï¼
```

**é¢„æœŸè¾“å‡ºï¼š**
```
[2026-01-28 10:00:00] Initializing Ray...
[2026-01-28 10:00:05] Creating Actor worker pool...
[2026-01-28 10:00:10] Creating Rollout worker pool...
[2026-01-28 10:00:15] Starting GRPO training...

Epoch 0:
  rollout: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 256/256 [00:40<00:00]  # 256 prompts Ã— 4 responses = 1024 samples
  train_actor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:15<00:00]
  metrics: reward_mean=0.28, kl=0.003, actor_loss=0.187, advantage_std=1.0

âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åˆ°: ./checkpoints/qwen2.5-3b_grpo/
```

### ä½¿ç”¨æ¨èé…ç½®ï¼ˆQwen3-8Bï¼‰

```bash
# ç›´æ¥è¿è¡Œé¢„è®¾è„šæœ¬
bash examples/grpo_trainer/run_qwen3-8b.sh

# æˆ–è‡ªå®šä¹‰å‚æ•°
bash examples/grpo_trainer/run_qwen3-8b.sh \
    data.train_batch_size=512 \
    actor_rollout_ref.rollout.n=8 \
    trainer.total_epochs=20
```

### ä½¿ç”¨ LoRA è®­ç»ƒï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰

```bash
# LoRA è®­ç»ƒï¼ˆæ˜¾å­˜å ç”¨æ›´å°‘ï¼‰
bash examples/grpo_trainer/run_qwen2_5-3b_gsm8k_grpo_lora.sh

# æˆ–è‡ªå®šä¹‰ LoRA é…ç½®
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    data.train_files=$HOME/data/gsm8k/train.parquet \
    actor_rollout_ref.model.path=Qwen/Qwen2.5-7B-Instruct \
    actor_rollout_ref.actor.lora.enable=True \
    actor_rollout_ref.actor.lora.r=16 \
    actor_rollout_ref.actor.lora.alpha=32 \
    actor_rollout_ref.actor.lora.target_modules='["q_proj","v_proj"]' \
    actor_rollout_ref.rollout.n=4 \
    trainer.n_gpus_per_node=2
```

---

## ğŸ“– è¯¦ç»†é…ç½®

### GRPO æ ¸å¿ƒé…ç½®

#### 1. ç®—æ³•é…ç½® (`algorithm.*`)

```yaml
algorithm:
  adv_estimator: grpo             # å¿…é¡»è®¾ç½®ä¸º grpo
  norm_adv_by_std_in_grpo: True   # ä½¿ç”¨æ ‡å‡†å·®å½’ä¸€åŒ–ä¼˜åŠ¿å€¼ï¼ˆæ¨èï¼‰
  use_kl_in_reward: False         # GRPO ä¸ä½¿ç”¨ KL rewardï¼ˆç”¨ KL loss ä»£æ›¿ï¼‰
```

**é‡è¦ï¼š**
- GRPO **ä¸ä½¿ç”¨** `algorithm.gamma` å’Œ `algorithm.lam`ï¼ˆè¿™æ˜¯ GAE çš„å‚æ•°ï¼‰
- GRPO **ä¸ä½¿ç”¨** `algorithm.use_kl_in_reward`ï¼ˆä½¿ç”¨ KL loss ä»£æ›¿ï¼‰

#### 2. ç»„é‡‡æ ·é…ç½® (`actor_rollout_ref.rollout.n`)

è¿™æ˜¯ GRPO æœ€å…³é”®çš„å‚æ•°ï¼š

```yaml
actor_rollout_ref:
  rollout:
    n: 4                          # æ¯ä¸ª prompt ç”Ÿæˆå‡ æ¡å“åº”ï¼ˆç»„å¤§å°ï¼‰
```

**å¦‚ä½•é€‰æ‹© nï¼š**

| n å€¼ | é€‚ç”¨åœºæ™¯ | ä¼˜åŠ¿ | åŠ£åŠ¿ |
|------|---------|------|------|
| **n=2** | å¿«é€Ÿå®éªŒ | é€Ÿåº¦å¿« | ç»Ÿè®¡ä¸ç¨³å®š |
| **n=4** | æ ‡å‡†é…ç½®ï¼ˆæ¨èï¼‰ | å¹³è¡¡é€Ÿåº¦å’Œç¨³å®šæ€§ | - |
| **n=8** | é«˜è´¨é‡è®­ç»ƒ | æ›´ç¨³å®šçš„æ¢¯åº¦ | æ›´æ…¢ï¼Œæ˜¾å­˜å ç”¨æ›´å¤š |
| **n=16** | ç ”ç©¶çº§è®­ç»ƒ | æœ€ç¨³å®š | éå¸¸æ…¢ |

**è®¡ç®—å…¬å¼ï¼š**
```
æ€»å“åº”æ•° = train_batch_size Ã— n

ç¤ºä¾‹ï¼š
train_batch_size=256, n=4 â†’ 1024 æ¡å“åº”
train_batch_size=128, n=8 â†’ 1024 æ¡å“åº”
```

#### 3. Actor é…ç½®ï¼ˆæ—  Criticï¼ï¼‰

```yaml
actor_rollout_ref:
  actor:
    # ä¼˜åŒ–å™¨é…ç½®
    optim:
      lr: 1e-6                    # å­¦ä¹ ç‡ï¼ˆGRPO å¯ä»¥ç¨é«˜äº PPOï¼‰

    # GRPO ç‰¹æœ‰å‚æ•°
    ppo_mini_batch_size: 256      # mini-batch å¤§å°
    ppo_epochs: 1                 # è®­ç»ƒè½®æ•°
    clip_ratio: 0.2               # Clipping èŒƒå›´
    loss_agg_mode: token-mean     # æŸå¤±èšåˆæ–¹å¼ï¼ˆæ¨èï¼‰

    # KL æ•£åº¦æ§åˆ¶ï¼ˆå¿…é¡»å¯ç”¨ï¼‰
    use_kl_loss: True             # å¿…é¡»è®¾ç½®ä¸º True
    kl_loss_coef: 0.001           # KL ç³»æ•°
    kl_loss_type: kl              # KL ç±»å‹
```

**loss_agg_mode è¯¦è§£ï¼š**

| æ¨¡å¼ | è®¡ç®—æ–¹å¼ | é€‚ç”¨åœºæ™¯ |
|------|---------|---------|
| `token-mean` | æ‰€æœ‰ token å¹³å‡ï¼ˆæ¨èï¼‰ | æ ‡å‡†é€‰æ‹©ï¼Œç¨³å®š |
| `seq-mean-token-sum` | å…ˆæŒ‰åºåˆ—æ±‚å’Œï¼Œå†å¹³å‡ | é•¿åºåˆ—ä»»åŠ¡ |
| `seq-mean-token-mean` | åºåˆ—çº§åˆ«å¹³å‡ï¼ˆåŸè®ºæ–‡ï¼‰ | çŸ­åºåˆ—ä»»åŠ¡ |

**åŸè®ºæ–‡ä½¿ç”¨ `seq-mean-token-mean`ï¼Œä½† verl æ¨è `token-mean` ä»¥è·å¾—æ›´å¥½çš„ç¨³å®šæ€§ã€‚**

#### 4. æ•°æ®é…ç½®

```yaml
data:
  train_files: ~/data/gsm8k/train.parquet
  val_files: ~/data/gsm8k/test.parquet
  train_batch_size: 256           # Prompt æ•°é‡
  max_prompt_length: 1024
  max_response_length: 512
```

**å…³é”®ï¼š**
- `train_batch_size` æ˜¯ **prompt æ•°é‡**
- å®é™…è®­ç»ƒçš„å“åº”æ•° = `train_batch_size Ã— rollout.n`
- ç¤ºä¾‹ï¼š`batch_size=256, n=4 â†’ 1024 æ¡å“åº”`

#### 5. å®Œæ•´é…ç½®ç¤ºä¾‹

```bash
python3 -m verl.trainer.main_ppo \
    # ========== ç®—æ³•é…ç½® ==========
    algorithm.adv_estimator=grpo \
    algorithm.norm_adv_by_std_in_grpo=True \

    # ========== æ•°æ®é…ç½® ==========
    data.train_files=$HOME/data/gsm8k/train.parquet \
    data.val_files=$HOME/data/gsm8k/test.parquet \
    data.train_batch_size=256 \
    data.max_prompt_length=1024 \
    data.max_response_length=512 \

    # ========== æ¨¡å‹é…ç½® ==========
    actor_rollout_ref.model.path=Qwen/Qwen2.5-7B-Instruct \

    # ========== Rollout é…ç½®ï¼ˆç»„é‡‡æ ·ï¼‰==========
    actor_rollout_ref.rollout.n=4 \
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.tensor_model_parallel_size=2 \

    # ========== Actor é…ç½® ==========
    actor_rollout_ref.actor.optim.lr=1e-6 \
    actor_rollout_ref.actor.ppo_mini_batch_size=256 \
    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 \
    actor_rollout_ref.actor.clip_ratio=0.2 \
    actor_rollout_ref.actor.loss_agg_mode=token-mean \

    # ========== KL æ§åˆ¶ï¼ˆGRPO å¿…é¡»ï¼‰==========
    actor_rollout_ref.actor.use_kl_loss=True \
    actor_rollout_ref.actor.kl_loss_coef=0.001 \
    actor_rollout_ref.actor.kl_loss_type=kl \

    # ========== Trainer é…ç½® ==========
    trainer.n_gpus_per_node=4 \
    trainer.total_epochs=20 \
    trainer.save_freq=5 \
    trainer.logger='["console","wandb"]'
```

---

## ğŸ’¡ è¿è¡Œç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šQwen2.5-3B å¿«é€Ÿæµ‹è¯•

```bash
# æœ€å°é…ç½®ï¼Œ2 GPUï¼Œ30 åˆ†é’Ÿå®Œæˆ
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    data.train_files=$HOME/data/gsm8k/train.parquet \
    data.val_files=$HOME/data/gsm8k/test.parquet \
    data.train_batch_size=128 \
    actor_rollout_ref.model.path=Qwen/Qwen2.5-3B-Instruct \
    actor_rollout_ref.rollout.n=4 \
    actor_rollout_ref.actor.ppo_mini_batch_size=128 \
    actor_rollout_ref.actor.use_kl_loss=True \
    actor_rollout_ref.actor.kl_loss_coef=0.001 \
    trainer.n_gpus_per_node=2 \
    trainer.total_epochs=10 \
    trainer.logger=console

# é¢„æœŸç»“æœï¼š
# - è®­ç»ƒæ—¶é—´: ~30 åˆ†é’Ÿ
# - GSM8K å‡†ç¡®ç‡: 60-65%
# - æ˜¾å­˜å ç”¨: ~18GB/GPU
```

### ç¤ºä¾‹ 2ï¼šQwen2-7B æ ‡å‡†è®­ç»ƒ

```bash
# æ ‡å‡†é…ç½®ï¼Œ4 GPU
bash examples/grpo_trainer/run_qwen2-7b_math.sh

# æˆ–è‡ªå®šä¹‰
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    data.train_files=$HOME/data/gsm8k/train.parquet \
    data.train_batch_size=512 \
    actor_rollout_ref.model.path=Qwen/Qwen2-7B-Instruct \
    actor_rollout_ref.rollout.n=4 \
    actor_rollout_ref.rollout.tensor_model_parallel_size=2 \
    actor_rollout_ref.actor.ppo_mini_batch_size=512 \
    actor_rollout_ref.actor.use_kl_loss=True \
    trainer.n_gpus_per_node=4 \
    trainer.total_epochs=20

# é¢„æœŸç»“æœï¼š
# - è®­ç»ƒæ—¶é—´: ~1.5 å°æ—¶
# - GSM8K å‡†ç¡®ç‡: 70-75%
```

### ç¤ºä¾‹ 3ï¼šQwen3-8B é«˜è´¨é‡è®­ç»ƒ

```bash
# æ¨èé…ç½®ï¼Œ8 GPU
bash examples/grpo_trainer/run_qwen3-8b.sh

# å…³é”®é…ç½®ï¼š
# - train_batch_size=512
# - rollout.n=8ï¼ˆæ›´å¤šé‡‡æ ·ï¼‰
# - total_epochs=30

# é¢„æœŸç»“æœï¼š
# - è®­ç»ƒæ—¶é—´: ~3 å°æ—¶
# - GSM8K å‡†ç¡®ç‡: 75-80%
```

### ç¤ºä¾‹ 4ï¼šä½¿ç”¨ Megatron-LM è®­ç»ƒè¶…å¤§æ¨¡å‹

```bash
# DeepSeek-Math-671Bï¼ˆéœ€è¦å¤šèŠ‚ç‚¹ï¼‰
bash examples/grpo_trainer/run_deepseek671b_math_megatron_96gb.sh

# æˆ– Qwen3-235B
bash examples/grpo_trainer/run_qwen3-235b_megatron_96gb.sh

# é…ç½®è¦ç‚¹ï¼š
# - Megatron-LM backend
# - tensor_model_parallel_size=8
# - pipeline_model_parallel_size=8
# - éœ€è¦ 64+ GPU
```

### ç¤ºä¾‹ 5ï¼šå¤šæ¨¡æ€ VLM è®­ç»ƒ

```bash
# Qwen2.5-VL-7Bï¼ˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼‰
bash examples/grpo_trainer/run_qwen2_5_vl-7b.sh

# æˆ– Qwen3-VL-8B
bash examples/grpo_trainer/run_qwen3_vl-8b-megatron.sh

# éœ€è¦å‡†å¤‡åŒ…å«å›¾åƒçš„æ•°æ®
# å‚è€ƒ examples/data_preprocess/geo3k.py
```

### ç¤ºä¾‹ 6ï¼šDrGRPOï¼ˆå‡å°‘é•¿åº¦åå·®ï¼‰

```bash
# DrGRPO é…ç½®ï¼ˆæ¨èç”¨äºé•¿ CoT ä»»åŠ¡ï¼‰
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    algorithm.norm_adv_by_std_in_grpo=False \
    actor_rollout_ref.actor.loss_agg_mode=seq-mean-token-sum-norm \
    actor_rollout_ref.actor.loss_scale_factor=512 \
    actor_rollout_ref.actor.use_kl_loss=False \
    data.train_files=$HOME/data/gsm8k/train.parquet \
    actor_rollout_ref.model.path=Qwen/Qwen2.5-7B-Instruct \
    actor_rollout_ref.rollout.n=4 \
    trainer.n_gpus_per_node=4

# DrGRPO å…³é”®åŒºåˆ«ï¼š
# - loss_agg_mode=seq-mean-token-sum-normï¼ˆå–æ¶ˆåºåˆ—çº§å¹³å‡ï¼‰
# - norm_adv_by_std_in_grpo=Falseï¼ˆå–æ¶ˆæ ‡å‡†å·®å½’ä¸€åŒ–ï¼‰
# - use_kl_loss=Falseï¼ˆä¸ä½¿ç”¨ KL lossï¼‰
# - loss_scale_factor=512ï¼ˆå›ºå®šå½’ä¸€åŒ–å¸¸æ•°ï¼‰
```

---

## ğŸ¯ GRPO æ ¸å¿ƒåŸç†

### ä¼˜åŠ¿è®¡ç®—å…¬å¼

**ä¼ ç»Ÿ GRPOï¼ˆç»„å†…å½’ä¸€åŒ–ï¼‰ï¼š**

```python
# å¯¹æ¯ä¸ª promptï¼Œç”Ÿæˆ n ä¸ªå“åº”
responses = [resp_1, resp_2, ..., resp_n]
rewards = [r_1, r_2, ..., r_n]

# è®¡ç®—ç»„å†…ç»Ÿè®¡é‡
mean = sum(rewards) / n
std = sqrt(sum((r_i - mean)^2) / n)

# å½’ä¸€åŒ–ä¼˜åŠ¿å€¼
advantage_i = (r_i - mean) / std

# ä½¿ç”¨ advantage_i æ›´æ–°ç­–ç•¥
```

**DrGRPOï¼ˆå…¨å±€å½’ä¸€åŒ–ï¼‰ï¼š**

```python
# ä¸ä½¿ç”¨ç»„å†…æ ‡å‡†å·®
advantage_i = r_i - mean

# ä½¿ç”¨å›ºå®šçš„å…¨å±€å½’ä¸€åŒ–å› å­
loss_i = -advantage_i * log_ratio_i / scale_factor
```

### ä¸ PPO çš„å¯¹æ¯”

**PPOï¼ˆGAEï¼‰ï¼š**
```python
# éœ€è¦ Critic æ¨¡å‹
values = critic_model(states)

# è®¡ç®— TD-error
delta_t = reward_t + gamma * value_{t+1} - value_t

# è®¡ç®— GAE ä¼˜åŠ¿
advantage_t = delta_t + gamma * lambda * advantage_{t+1}
```

**GRPOï¼š**
```python
# ä¸éœ€è¦ Critic
# åªéœ€è¦åŒä¸€ç»„å†…çš„å…¶ä»–å“åº”

# ç»„å†…å½’ä¸€åŒ–
advantage_i = (reward_i - group_mean) / group_std
```

**å…³é”®åŒºåˆ«ï¼š**
- PPOï¼šä¾èµ– Critic æä¾› baseline
- GRPOï¼šä¾èµ–ç»„å†…å…¶ä»–æ ·æœ¬æä¾› baseline

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: GRPO vs PPO å¦‚ä½•é€‰æ‹©ï¼Ÿ

**é€‰æ‹© GRPO çš„åœºæ™¯ï¼š**
```
âœ… æ•°å­¦æ¨ç†ä»»åŠ¡ï¼ˆGSM8Kã€MATHï¼‰
âœ… ä»£ç ç”Ÿæˆä»»åŠ¡ï¼ˆHumanEvalã€MBPPï¼‰
âœ… æ˜ç¡®çš„å¯¹é”™æ ‡å‡†ï¼ˆé—®ç­”ã€é€‰æ‹©é¢˜ï¼‰
âœ… GPU æ˜¾å­˜å—é™
âœ… å¿«é€Ÿå®éªŒåŸå‹
âœ… ç»“æœå¯¼å‘çš„ä»»åŠ¡
```

**é€‰æ‹© PPO çš„åœºæ™¯ï¼š**
```
âœ… å¯¹è¯è´¨é‡ä¼˜åŒ–
âœ… é•¿æ–‡æœ¬ç”Ÿæˆ
âœ… éœ€è¦è¿‡ç¨‹çº§ä¼˜åŒ–
âœ… å¤æ‚çš„ reward shaping
âœ… RLHF äººç±»åå¥½å¯¹é½
âœ… éœ€è¦æ›´ç¨³å®šçš„è®­ç»ƒ
```

**å†³ç­–æ ‘ï¼š**
```
ä»»åŠ¡æ˜¯å¦æœ‰æ˜ç¡®çš„å¯¹é”™æ ‡å‡†ï¼Ÿ
â”œâ”€ Yes â†’ æ˜¯å¦æ˜¯æ•°å­¦/ä»£ç ä»»åŠ¡ï¼Ÿ
â”‚   â”œâ”€ Yes â†’ ä½¿ç”¨ GRPOï¼ˆæ¨èï¼‰
â”‚   â””â”€ No â†’ GRPO æˆ– PPO éƒ½å¯ä»¥
â””â”€ No â†’ æ˜¯å¦éœ€è¦è¿‡ç¨‹çº§ä¼˜åŒ–ï¼Ÿ
    â”œâ”€ Yes â†’ ä½¿ç”¨ PPO
    â””â”€ No â†’ ä½¿ç”¨ GRPOï¼ˆæ›´å¿«ï¼‰
```

### Q2: rollout.n åº”è¯¥è®¾ç½®ä¸ºå¤šå°‘ï¼Ÿ

**æ¨èé…ç½®ï¼š**

```bash
# å¿«é€Ÿå®éªŒï¼ˆä¸æ¨èç”¨äºæœ€ç»ˆè®­ç»ƒï¼‰
actor_rollout_ref.rollout.n=2

# æ ‡å‡†é…ç½®ï¼ˆæ¨èï¼‰
actor_rollout_ref.rollout.n=4

# é«˜è´¨é‡è®­ç»ƒ
actor_rollout_ref.rollout.n=8

# ç ”ç©¶çº§åˆ«
actor_rollout_ref.rollout.n=16
```

**æƒè¡¡è€ƒè™‘ï¼š**

| n | ä¼˜åŠ¿ | åŠ£åŠ¿ | é€‚ç”¨åœºæ™¯ |
|---|------|------|---------|
| 2 | å¿«é€Ÿ | ä¸ç¨³å®š | è°ƒè¯• |
| 4 | å¹³è¡¡ | - | ç”Ÿäº§ç¯å¢ƒï¼ˆæ¨èï¼‰ |
| 8 | ç¨³å®š | æ…¢ 2x | é«˜è´¨é‡è®­ç»ƒ |
| 16 | éå¸¸ç¨³å®š | æ…¢ 4x | ç ”ç©¶ |

**å®éªŒå»ºè®®ï¼š**
```bash
# ç¬¬ä¸€æ¬¡å°è¯•ï¼šn=4
# å¦‚æœè®­ç»ƒä¸ç¨³å®šï¼šå¢å¤§åˆ° n=8
# å¦‚æœé€Ÿåº¦å¤ªæ…¢ï¼šå‡å°åˆ° n=2ï¼ˆä½†å¯èƒ½éœ€è¦è°ƒæ•´å…¶ä»–å‚æ•°ï¼‰
```

### Q3: Advantage æ ‡å‡†å·®ä¸º 0 æ€ä¹ˆåŠï¼Ÿ

**ç—‡çŠ¶ï¼š**
```
Warning: advantage std is 0, setting to 1
æˆ–
All rewards in the group are the same
```

**åŸå› ï¼š**
- ç»„å†…æ‰€æœ‰å“åº”çš„ reward å®Œå…¨ç›¸åŒ
- å¯èƒ½æ˜¯ reward å‡½æ•°è®¾è®¡é—®é¢˜

**è§£å†³æ–¹æ¡ˆï¼š**

```bash
# æ–¹æ³• 1: å¢å¤§ rollout.nï¼ˆå¢åŠ å¤šæ ·æ€§ï¼‰
actor_rollout_ref.rollout.n=8  # ä» 4 å¢å¤§åˆ° 8

# æ–¹æ³• 2: è°ƒæ•´é‡‡æ ·å‚æ•°ï¼ˆå¢åŠ éšæœºæ€§ï¼‰
actor_rollout_ref.rollout.temperature=1.0  # å¢å¤§æ¸©åº¦
actor_rollout_ref.rollout.top_p=0.95  # ä¸è¦è®¾ç½®ä¸º 1.0

# æ–¹æ³• 3: æ£€æŸ¥ reward å‡½æ•°
# ç¡®ä¿ reward ä¸æ˜¯äºŒå…ƒçš„ï¼ˆ0 æˆ– 1ï¼‰ï¼Œåº”è¯¥æœ‰ç»†ç²’åº¦çš„è¯„åˆ†

# æ–¹æ³• 4: ä½¿ç”¨ reward shaping
# æ·»åŠ ä¸­é—´æ­¥éª¤çš„å¥–åŠ±ï¼Œè¯¦è§ learning_notes/04_Rewardè®¾è®¡/
```

### Q4: GRPO è®­ç»ƒä¸ç¨³å®šæ€ä¹ˆåŠï¼Ÿ

**ç—‡çŠ¶ï¼š**
```
- Loss éœ‡è¡ä¸¥é‡
- Reward ä¸ä¸Šå‡
- KL æ•£åº¦çˆ†ç‚¸
```

**è§£å†³æ–¹æ¡ˆï¼š**

```bash
# æ–¹æ³• 1: å¢å¤§ rollout.nï¼ˆæ›´ç¨³å®šçš„æ¢¯åº¦ï¼‰
actor_rollout_ref.rollout.n=8  # ä» 4 å¢å¤§

# æ–¹æ³• 2: é™ä½å­¦ä¹ ç‡
actor_rollout_ref.actor.optim.lr=5e-7  # ä» 1e-6 é™ä½

# æ–¹æ³• 3: å¢å¤§ KL çº¦æŸ
actor_rollout_ref.actor.kl_loss_coef=0.01  # ä» 0.001 å¢å¤§

# æ–¹æ³• 4: å‡å° clip_ratio
actor_rollout_ref.actor.clip_ratio=0.1  # ä» 0.2 å‡å°

# æ–¹æ³• 5: å‡å° batch size
data.train_batch_size=128  # ä» 256 å‡å°

# æ–¹æ³• 6: ä½¿ç”¨ DrGRPO
algorithm.norm_adv_by_std_in_grpo=False
actor_rollout_ref.actor.loss_agg_mode=seq-mean-token-sum-norm
```

### Q5: GRPO çš„æ˜¾å­˜å ç”¨å¦‚ä½•ï¼Ÿ

**æ˜¾å­˜å ç”¨å¯¹æ¯”ï¼ˆQwen2-7Bï¼Œå• GPUï¼‰ï¼š**

```
PPO:
- Actor è®­ç»ƒ: ~20GB
- Critic è®­ç»ƒ: ~20GB
- Rollout: ~15GB
- æ€»è®¡ï¼ˆå³°å€¼ï¼‰: ~40GB
- éœ€è¦: 2-4 å¼  40GB GPU

GRPO:
- Actor è®­ç»ƒ: ~20GB
- Rollout: ~15GB
- æ€»è®¡ï¼ˆå³°å€¼ï¼‰: ~25GB
- éœ€è¦: 1-2 å¼  40GB GPU

èŠ‚çœ: ~40%
```

**ä¼˜åŒ–æŠ€å·§ï¼š**

```bash
# 1. å‡å° rollout GPU æ˜¾å­˜å ç”¨
actor_rollout_ref.rollout.gpu_memory_utilization=0.3

# 2. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
actor_rollout_ref.actor.fsdp_config.gradient_checkpointing=True

# 3. ä½¿ç”¨ LoRA
actor_rollout_ref.actor.lora.enable=True
actor_rollout_ref.actor.lora.r=16

# 4. å¢å¤§å¼ é‡å¹¶è¡Œ
actor_rollout_ref.rollout.tensor_model_parallel_size=4
```

### Q6: loss_agg_mode å¦‚ä½•é€‰æ‹©ï¼Ÿ

**ä¸‰ç§æ¨¡å¼å¯¹æ¯”ï¼š**

```python
# å‡è®¾ä¸€ä¸ª batch æœ‰ 2 æ¡åºåˆ—
responses = [
    [token1, token2, token3],  # é•¿åº¦ 3
    [token1, token2, token3, token4, token5]  # é•¿åº¦ 5
]
advantages = [
    [adv1, adv2, adv3],
    [adv4, adv5, adv6, adv7, adv8]
]
ratios = [
    [ratio1, ratio2, ratio3],
    [ratio4, ratio5, ratio6, ratio7, ratio8]
]

# æ¨¡å¼ 1: token-meanï¼ˆæ¨èï¼‰
loss = mean([-adv1*ratio1, -adv2*ratio2, ..., -adv8*ratio8])
# æ‰€æœ‰ token å¹³ç­‰å¯¹å¾…

# æ¨¡å¼ 2: seq-mean-token-sum
seq_loss_1 = sum([-adv1*ratio1, -adv2*ratio2, -adv3*ratio3])
seq_loss_2 = sum([-adv4*ratio4, ..., -adv8*ratio8])
loss = mean([seq_loss_1, seq_loss_2])
# å…ˆæŒ‰åºåˆ—æ±‚å’Œï¼Œå†å¹³å‡ï¼ˆåå‘é•¿åºåˆ—ï¼‰

# æ¨¡å¼ 3: seq-mean-token-meanï¼ˆåŸè®ºæ–‡ï¼‰
seq_loss_1 = mean([-adv1*ratio1, -adv2*ratio2, -adv3*ratio3])
seq_loss_2 = mean([-adv4*ratio4, ..., -adv8*ratio8])
loss = mean([seq_loss_1, seq_loss_2])
# æ¯ä¸ªåºåˆ—å¹³ç­‰å¯¹å¾…ï¼ˆå¯èƒ½ä¸ç¨³å®šï¼‰
```

**æ¨èï¼š**
- **ä¸€èˆ¬ä»»åŠ¡**ï¼š`token-mean`ï¼ˆé»˜è®¤ï¼Œæœ€ç¨³å®šï¼‰
- **é•¿åºåˆ—ä»»åŠ¡**ï¼š`seq-mean-token-sum`
- **çŸ­åºåˆ—ä»»åŠ¡**ï¼š`seq-mean-token-mean`ï¼ˆåŸè®ºæ–‡ï¼‰
- **å‡å°‘é•¿åº¦åå·®**ï¼š`seq-mean-token-sum-norm`ï¼ˆDrGRPOï¼‰

### Q7: GRPO éœ€è¦ Critic warmup å—ï¼Ÿ

**ä¸éœ€è¦ï¼**

```bash
# GRPO æ²¡æœ‰ Criticï¼Œæ‰€ä»¥ä¸éœ€è¦ warmup
trainer.critic_warmup=0  # ä¿æŒä¸º 0

# ä½†å¦‚æœæƒ³è®© Actor å…ˆé€‚åº”æ•°æ®ï¼Œå¯ä»¥ï¼š
# 1. å…ˆåš SFT
bash examples/sft/run_qwen2_5_7b.sh

# 2. ç„¶åä» SFT checkpoint å¼€å§‹ GRPO
actor_rollout_ref.model.path=./checkpoints/sft_qwen2.5_7b/
```

### Q8: å¦‚ä½•ç›‘æ§ GRPO è®­ç»ƒï¼Ÿ

**å…³é”®æŒ‡æ ‡ï¼š**

```yaml
# 1. Reward ç›¸å…³
reward_mean: å¹³å‡å¥–åŠ±ï¼ˆåº”è¯¥ä¸Šå‡ï¼‰
reward_std: å¥–åŠ±æ ‡å‡†å·®ï¼ˆç»„å†…å·®å¼‚ï¼‰

# 2. Advantage ç›¸å…³
advantage_mean: åº”è¯¥æ¥è¿‘ 0ï¼ˆå½’ä¸€åŒ–åï¼‰
advantage_std: åº”è¯¥æ¥è¿‘ 1ï¼ˆå½’ä¸€åŒ–åï¼‰

# 3. æŸå¤±ç›¸å…³
actor_loss: Actor æŸå¤±ï¼ˆåº”è¯¥ä¸‹é™ï¼‰
kl_divergence: KL æ•£åº¦ï¼ˆåº”è¯¥ < 0.1ï¼‰

# 4. PPO ç›¸å…³
ppo_ratio_mean: åº”è¯¥æ¥è¿‘ 1.0
ppo_ratio_clipped: è¢« clip çš„æ¯”ä¾‹ï¼ˆåº”è¯¥ < 30%ï¼‰

# 5. æ¢¯åº¦ç›¸å…³
grad_norm: æ¢¯åº¦èŒƒæ•°ï¼ˆä¸åº”è¯¥çˆ†ç‚¸ï¼‰
```

**å¥åº·çš„è®­ç»ƒæ›²çº¿ï¼š**
```
Epoch 0:  reward_mean=0.25, kl=0.003
Epoch 5:  reward_mean=0.45, kl=0.008
Epoch 10: reward_mean=0.60, kl=0.015
Epoch 15: reward_mean=0.68, kl=0.020

âœ… reward ç¨³æ­¥ä¸Šå‡
âœ… kl ç¼“æ…¢å¢é•¿ä½†ä¸çˆ†ç‚¸
```

**å¼‚å¸¸æƒ…å†µï¼š**
```
Epoch 0:  reward_mean=0.25, kl=0.003
Epoch 5:  reward_mean=0.28, kl=0.150  âŒ KL å¤ªå¤§
Epoch 10: reward_mean=0.20, kl=0.250  âŒ Reward ä¸‹é™

â†’ å­¦ä¹ ç‡å¤ªé«˜ï¼Œæˆ– KL çº¦æŸå¤ªå¼±
â†’ å‡å° lr æˆ–å¢å¤§ kl_loss_coef
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†

### Qwen2.5-3B on GSM8K

```
é¢„è®­ç»ƒæ¨¡å‹å‡†ç¡®ç‡: ~50%
GRPO è®­ç»ƒåå‡†ç¡®ç‡: ~65%
è®­ç»ƒæ—¶é—´: ~30 åˆ†é’Ÿï¼ˆ2x A100ï¼‰
é…ç½®: batch_size=128, n=4, epochs=10

å‘½ä»¤:
bash examples/grpo_trainer/run_qwen2_5-3b_gsm8k_grpo_lora.sh
```

### Qwen2-7B on GSM8K

```
é¢„è®­ç»ƒæ¨¡å‹å‡†ç¡®ç‡: ~65%
GRPO è®­ç»ƒåå‡†ç¡®ç‡: ~78%
è®­ç»ƒæ—¶é—´: ~1.5 å°æ—¶ï¼ˆ4x A100ï¼‰
é…ç½®: batch_size=512, n=4, epochs=20

å‘½ä»¤:
bash examples/grpo_trainer/run_qwen2-7b_math.sh
```

### Qwen3-8B on GSM8K

```
é¢„è®­ç»ƒæ¨¡å‹å‡†ç¡®ç‡: ~70%
GRPO è®­ç»ƒåå‡†ç¡®ç‡: ~82%
è®­ç»ƒæ—¶é—´: ~3 å°æ—¶ï¼ˆ8x A100ï¼‰
é…ç½®: batch_size=512, n=8, epochs=30

å‘½ä»¤:
bash examples/grpo_trainer/run_qwen3-8b.sh
```

---

## ğŸ”— å‚è€ƒèµ„æ–™

### å®˜æ–¹æ–‡æ¡£

- [GRPO è®ºæ–‡ï¼ˆDeepSeekMathï¼‰](https://arxiv.org/pdf/2402.03300)
- [DrGRPO è®ºæ–‡](https://arxiv.org/pdf/2503.20783)
- [verl æ–‡æ¡£](https://verl.readthedocs.io/)

### å­¦ä¹ ç¬”è®°

- [03_RLç®—æ³•/GRPO_è¯¦è§£.md](../../learning_notes/03_RLç®—æ³•/GRPO_è¯¦è§£.md) - GRPO ç®—æ³•æºç çº§è¯¦è§£
- [03_RLç®—æ³•/03_RLç®—æ³•æ¦‚è§ˆ.md](../../learning_notes/03_RLç®—æ³•/03_RLç®—æ³•æ¦‚è§ˆ.md) - ç®—æ³•å¯¹æ¯”ä¸é€‰æ‹©
- [01_å¿«é€Ÿä¸Šæ‰‹/ray_trainer_è¯¦è§£.md](../../learning_notes/01_å¿«é€Ÿä¸Šæ‰‹/ray_trainer_è¯¦è§£.md) - è®­ç»ƒæµç¨‹è¯¦è§£

### ç›¸å…³ç¤ºä¾‹

- `examples/ppo_trainer/` - PPO è®­ç»ƒç¤ºä¾‹ï¼ˆæœ‰ Criticï¼‰
- `examples/rloo_trainer/` - RLOO è®­ç»ƒç¤ºä¾‹ï¼ˆå¦ä¸€ç§æ—  Critic ç®—æ³•ï¼‰
- `examples/data_preprocess/` - æ•°æ®é¢„å¤„ç†

---

**åˆ›å»ºæ—¶é—´**: 2026-01-28
**é€‚ç”¨ç‰ˆæœ¬**: verl v0.2+
**ç»´æŠ¤è€…**: verl team
