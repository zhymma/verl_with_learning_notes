# æ•™ç¨‹ (Tutorial)

> verl å…¥é—¨å’Œè¿›é˜¶æ•™ç¨‹

---

## ğŸ“‹ æ¦‚è¿°

æœ¬ç›®å½•åŒ…å« verl çš„å„ç§æ•™ç¨‹ï¼Œä»åŸºç¡€å…¥é—¨åˆ°é«˜çº§åº”ç”¨ï¼Œå¸®åŠ©ä½ å¿«é€ŸæŒæ¡ verl çš„ä½¿ç”¨ã€‚

### æ•™ç¨‹åˆ—è¡¨

| æ•™ç¨‹ | éš¾åº¦ | æ—¶é—´ | è¯´æ˜ |
|------|------|------|------|
| **agent_loop_get_started/** | â­ å…¥é—¨ | 30 åˆ†é’Ÿ | Agent Loop å¿«é€Ÿå…¥é—¨ |

---

## ğŸ“ agent_loop_get_started - Agent Loop å¿«é€Ÿå…¥é—¨

### æ•™ç¨‹ç›®æ ‡

é€šè¿‡è¿™ä¸ªæ•™ç¨‹ï¼Œä½ å°†å­¦ä¼šï¼š

- âœ… ç†è§£ Agent Loop çš„åŸºæœ¬æ¦‚å¿µ
- âœ… å®ç°ä¸€ä¸ªç®€å•çš„ Agent Loop
- âœ… ä½¿ç”¨å·¥å…·è°ƒç”¨
- âœ… è®­ç»ƒä¸€ä¸ª Tool-using Agent

### å‰ç½®æ¡ä»¶

```bash
# 1. å®‰è£… verl
pip install -e .[test,sglang]

# 2. å‡†å¤‡ GSM8K æ•°æ®
python examples/data_preprocess/gsm8k_multiturn_w_tool.py \
    --local_save_dir ~/data/gsm8k_tool

# 3. ä¸‹è½½æ¨¡å‹
huggingface-cli download Qwen/Qwen2.5-3B-Instruct
```

### å¿«é€Ÿå¼€å§‹

```bash
# è¿›å…¥æ•™ç¨‹ç›®å½•
cd examples/tutorial/agent_loop_get_started

# è¿è¡Œç¤ºä¾‹
python simple_agent_loop.py

# é¢„æœŸè¾“å‡ºï¼š
# Agent Loop Example
# ==================
# Prompt: What is 123 + 456?
#
# Turn 1:
#   Assistant: Let me calculate: calculator(123 + 456)
#   Tool: 579
#
# Turn 2:
#   Assistant: The answer is 579.
#
# âœ… Correct!
```

### æ•™ç¨‹ç»“æ„

```
agent_loop_get_started/
â”œâ”€â”€ README.md                   # æ•™ç¨‹è¯´æ˜
â”œâ”€â”€ simple_agent_loop.py        # ç®€å•çš„ Agent Loop ç¤ºä¾‹
â”œâ”€â”€ custom_agent_loop.py        # è‡ªå®šä¹‰ Agent Loop
â”œâ”€â”€ tool_calling_demo.py        # å·¥å…·è°ƒç”¨æ¼”ç¤º
â””â”€â”€ train_agent.sh              # è®­ç»ƒè„šæœ¬
```

### æ ¸å¿ƒä»£ç è®²è§£

#### 1. æœ€ç®€å•çš„ Agent Loop

```python
# simple_agent_loop.py
from verl.workers.rollout.sglang_rollout.agent_loop.base import AgentLoopBase

class SimpleAgentLoop(AgentLoopBase):
    """æœ€ç®€å•çš„ Agent Loop ç¤ºä¾‹"""

    async def generate(self, llm_server, data, **kwargs):
        prompts = data.batch['prompt']
        trajectories = []

        for prompt in prompts:
            history = prompt.copy()

            # ç¬¬ä¸€æ¬¡ç”Ÿæˆ
            response = await llm_server.generate([history])
            history.append({
                'role': 'assistant',
                'content': response['text'][0]
            })

            trajectories.append(history)

        data.batch['response'] = trajectories
        return data
```

#### 2. å¸¦å·¥å…·è°ƒç”¨çš„ Agent Loop

```python
# tool_calling_demo.py
class ToolCallingAgentLoop(AgentLoopBase):
    """æ”¯æŒå·¥å…·è°ƒç”¨çš„ Agent Loop"""

    def __init__(self, max_turns=5):
        self.max_turns = max_turns
        self.tools = {
            'calculator': self._calculator
        }

    async def generate(self, llm_server, data, **kwargs):
        prompts = data.batch['prompt']
        trajectories = []

        for prompt in prompts:
            history = prompt.copy()

            for turn in range(self.max_turns):
                # LLM ç”Ÿæˆ
                response = await llm_server.generate([history])
                assistant_msg = response['text'][0]

                history.append({
                    'role': 'assistant',
                    'content': assistant_msg
                })

                # æ£€æŸ¥å·¥å…·è°ƒç”¨
                tool_call = self._parse_tool_call(assistant_msg)

                if tool_call is None:
                    # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç»“æŸ
                    break

                # æ‰§è¡Œå·¥å…·
                tool_result = self.tools[tool_call['name']](
                    tool_call['arguments']
                )

                history.append({
                    'role': 'tool',
                    'content': tool_result,
                    'name': tool_call['name']
                })

            trajectories.append(history)

        data.batch['response'] = trajectories
        return data

    def _calculator(self, args):
        """è®¡ç®—å™¨å·¥å…·"""
        expr = args['expression']
        try:
            return str(eval(expr))
        except:
            return "Error"

    def _parse_tool_call(self, text):
        """è§£æå·¥å…·è°ƒç”¨"""
        import re
        match = re.search(r'calculator\((.*?)\)', text)
        if match:
            return {
                'name': 'calculator',
                'arguments': {'expression': match.group(1)}
            }
        return None
```

#### 3. è®­ç»ƒ Agent

```bash
# train_agent.sh
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    data.train_files=$HOME/data/gsm8k_tool/train.parquet \
    actor_rollout_ref.model.path=Qwen/Qwen2.5-3B-Instruct \
    actor_rollout_ref.rollout.name=sglang \
    actor_rollout_ref.rollout.use_agent_loop=True \
    actor_rollout_ref.rollout.agent_loop_class="examples.tutorial.agent_loop_get_started.tool_calling_demo.ToolCallingAgentLoop" \
    actor_rollout_ref.rollout.n=4 \
    trainer.n_gpus_per_node=4 \
    trainer.total_epochs=10
```

### è¿›é˜¶ç»ƒä¹ 

#### ç»ƒä¹  1ï¼šæ·»åŠ æ–°å·¥å…·

```python
# åœ¨ ToolCallingAgentLoop ä¸­æ·»åŠ 
self.tools['search'] = self._search

def _search(self, args):
    """æœç´¢å·¥å…·ï¼ˆæ¨¡æ‹Ÿï¼‰"""
    query = args['query']
    # å®é™…åº”è¯¥è°ƒç”¨æœç´¢ API
    return f"Search results for: {query}"
```

#### ç»ƒä¹  2ï¼šå®ç° Reward Shaping

```python
# reward_shaper.py
@RewardManager.register('tutorial_task')
def compute_reward(data):
    rewards = []

    for trajectory, ground_truth in zip(data['trajectories'], data['ground_truths']):
        reward = 0

        # ç»“æœå¥–åŠ±
        if is_correct(trajectory, ground_truth):
            reward += 1.0

        # è¿‡ç¨‹å¥–åŠ±
        for msg in trajectory:
            if msg['role'] == 'tool':
                reward += 0.1  # æˆåŠŸè°ƒç”¨å·¥å…·
            if msg['role'] == 'assistant' and 'let me' in msg['content'].lower():
                reward += 0.05  # è¡¨æ˜æ¨ç†è¿‡ç¨‹

        rewards.append(reward)

    return rewards
```

#### ç»ƒä¹  3ï¼šå®ç°å¤šæ­¥æ¨ç†

```python
# multi_step_agent.py
class MultiStepAgentLoop(ToolCallingAgentLoop):
    """æ”¯æŒå¤šæ­¥æ¨ç†çš„ Agent Loop"""

    def __init__(self, max_turns=10, require_reasoning=True):
        super().__init__(max_turns)
        self.require_reasoning = require_reasoning

    async def generate(self, llm_server, data, **kwargs):
        # ... åŒä¸Šï¼Œä½†æ·»åŠ æ¨ç†æ­¥éª¤éªŒè¯ ...

        for turn in range(self.max_turns):
            # ç”Ÿæˆæ¨ç†æ­¥éª¤
            reasoning_prompt = history + [{
                'role': 'user',
                'content': 'Explain your reasoning:'
            }]

            reasoning = await llm_server.generate([reasoning_prompt])

            history.append({
                'role': 'assistant',
                'content': f"Reasoning: {reasoning['text'][0]}"
            })

            # ç„¶åç”Ÿæˆç­”æ¡ˆ
            # ...
```

### å¸¸è§é—®é¢˜

#### Q1: å¦‚ä½•è°ƒè¯• Agent Loopï¼Ÿ

```python
# æ·»åŠ æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

class DebugAgentLoop(AgentLoopBase):
    async def generate(self, llm_server, data, **kwargs):
        logging.info(f"Input prompts: {len(data.batch['prompt'])}")

        for idx, prompt in enumerate(data.batch['prompt']):
            logging.info(f"Processing prompt {idx}")
            logging.info(f"Prompt content: {prompt}")

            # ... ç”Ÿæˆé€»è¾‘ ...

            logging.info(f"Generated {len(history)} turns")
            for turn_idx, msg in enumerate(history):
                logging.info(f"Turn {turn_idx}: {msg['role']} - {msg['content'][:50]}")
```

#### Q2: Agent Loop å¡ä½æ€ä¹ˆåŠï¼Ÿ

```python
# æ·»åŠ è¶…æ—¶æœºåˆ¶
import asyncio

class TimeoutAgentLoop(AgentLoopBase):
    async def generate(self, llm_server, data, timeout=60, **kwargs):
        try:
            return await asyncio.wait_for(
                self._generate_impl(llm_server, data, **kwargs),
                timeout=timeout
            )
        except asyncio.TimeoutError:
            logging.warning("Agent Loop timeout!")
            # è¿”å›éƒ¨åˆ†ç»“æœ
            return self._create_fallback_response(data)
```

### ä¸‹ä¸€æ­¥

å®Œæˆè¿™ä¸ªæ•™ç¨‹åï¼Œä½ å¯ä»¥ï¼š

1. **å­¦ä¹ æ›´å¤šç®—æ³•**ï¼šæŸ¥çœ‹ [03_RLç®—æ³•](../../learning_notes/03_RLç®—æ³•/)
2. **æ·±å…¥ Agent Loop**ï¼šé˜…è¯» [05_Agent_RL](../../learning_notes/05_Agent_RL/)
3. **å®æˆ˜é¡¹ç›®**ï¼šå°è¯• `examples/sglang_multiturn/` ä¸­çš„å®Œæ•´ç¤ºä¾‹
4. **è‡ªå®šä¹‰ Reward**ï¼šå­¦ä¹  [04_Rewardè®¾è®¡](../../learning_notes/04_Rewardè®¾è®¡/)

---

## ğŸ“š å…¶ä»–æ•™ç¨‹ï¼ˆè§„åˆ’ä¸­ï¼‰

### å³å°†æ¨å‡º

- [ ] **å•è½® RL è®­ç»ƒ** - ä»é›¶å¼€å§‹çš„ PPO/GRPO æ•™ç¨‹
- [ ] **è‡ªå®šä¹‰ Reward å‡½æ•°** - å®ç°å¤æ‚çš„ reward shaping
- [ ] **å¤šæ¨¡æ€è®­ç»ƒ** - VLM çš„ RL è®­ç»ƒ
- [ ] **åˆ†å¸ƒå¼è®­ç»ƒ** - å¤šèŠ‚ç‚¹è®­ç»ƒé…ç½®
- [ ] **æ¨¡å‹éƒ¨ç½²** - è®­ç»ƒåæ¨¡å‹çš„éƒ¨ç½²

### è´¡çŒ®æ•™ç¨‹

æ¬¢è¿è´¡çŒ®æ–°çš„æ•™ç¨‹ï¼

```bash
# 1. Fork é¡¹ç›®
git clone https://github.com/your-username/verl.git

# 2. åˆ›å»ºæ•™ç¨‹ç›®å½•
mkdir -p examples/tutorial/your_tutorial_name

# 3. æ·»åŠ æ•™ç¨‹æ–‡ä»¶
# - README.mdï¼ˆæ•™ç¨‹è¯´æ˜ï¼‰
# - ç¤ºä¾‹ä»£ç 
# - è®­ç»ƒè„šæœ¬

# 4. æäº¤ PR
git add .
git commit -m "Add tutorial: your_tutorial_name"
git push origin your-branch
```

---

## ğŸ”— å‚è€ƒèµ„æ–™

### å­¦ä¹ ç¬”è®°

- [01_å¿«é€Ÿä¸Šæ‰‹](../../learning_notes/01_å¿«é€Ÿä¸Šæ‰‹/) - ç¯å¢ƒå®‰è£…å’Œç¬¬ä¸€æ¬¡è®­ç»ƒ
- [02_æ•°æ®å‡†å¤‡](../../learning_notes/02_æ•°æ®å‡†å¤‡/) - æ•°æ®æ ¼å¼è¯¦è§£
- [05_Agent_RL](../../learning_notes/05_Agent_RL/) - Agent Loop æ·±åº¦è§£æ

### å®˜æ–¹æ–‡æ¡£

- [verl æ–‡æ¡£](https://verl.readthedocs.io/)
- [GitHub ä»“åº“](https://github.com/volcengine/verl)

### ç›¸å…³ç¤ºä¾‹

- `examples/sglang_multiturn/` - å®Œæ•´çš„å¤šè½®è®­ç»ƒç¤ºä¾‹
- `examples/data_preprocess/` - æ•°æ®é¢„å¤„ç†
- `examples/ppo_trainer/` - PPO è®­ç»ƒ
- `examples/grpo_trainer/` - GRPO è®­ç»ƒ

---

**åˆ›å»ºæ—¶é—´**: 2026-01-28
**é€‚ç”¨ç‰ˆæœ¬**: verl v0.2+
**ç»´æŠ¤è€…**: verl team
