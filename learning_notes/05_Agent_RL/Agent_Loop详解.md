# Agent Loop è¯¦è§£

> æ·±å…¥ç†è§£å¤šè½®å¯¹è¯å’Œå·¥å…·è°ƒç”¨çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒ

---

## ğŸ“– ç›®å½•

1. [Agent Loop æ ¸å¿ƒæ¦‚å¿µ](#1-agent-loop-æ ¸å¿ƒæ¦‚å¿µ)
2. [ç³»ç»Ÿæ¶æ„æ·±åº¦è§£æ](#2-ç³»ç»Ÿæ¶æ„æ·±åº¦è§£æ)
3. [AgentLoopBase æ¥å£è¯¦è§£](#3-agentloopbase-æ¥å£è¯¦è§£)
4. [å·¥å…·è°ƒç”¨å®ç°](#4-å·¥å…·è°ƒç”¨å®ç°)
5. [å¤šè½®å¯¹è¯è®­ç»ƒ](#5-å¤šè½®å¯¹è¯è®­ç»ƒ)
6. [å®Œæ•´è®­ç»ƒæµç¨‹è¿½è¸ª](#6-å®Œæ•´è®­ç»ƒæµç¨‹è¿½è¸ª)
7. [è°ƒè¯•æŠ€å·§](#7-è°ƒè¯•æŠ€å·§)
8. [æœ€ä½³å®è·µ](#8-æœ€ä½³å®è·µ)

---

## 1. Agent Loop æ ¸å¿ƒæ¦‚å¿µ

### 1.1 ä»€ä¹ˆæ˜¯ Agent Loopï¼Ÿ

**Agent Loop** æ˜¯ verl ä¸ºå¤šè½®å¯¹è¯å’Œå·¥å…·è°ƒç”¨è®¾è®¡çš„é€šç”¨æ¥å£ã€‚

**æ ¸å¿ƒç‰¹ç‚¹ï¼š**
- âœ… æ”¯æŒå¤šè½® LLM äº¤äº’
- âœ… æ”¯æŒå·¥å…·è°ƒç”¨ï¼ˆTool Callsï¼‰
- âœ… å¼‚æ­¥æ‰§è¡Œï¼Œæé«˜ GPU åˆ©ç”¨ç‡
- âœ… å¯æ’æ‹”çš„ç”¨æˆ·è‡ªå®šä¹‰ Agent
- âœ… ç»Ÿä¸€çš„ LLM generate API

**è®¾è®¡ç›®æ ‡ï¼š**
1. **å¯æ’æ‹”**ï¼šç”¨æˆ·å¯ä»¥è‡ªå®šä¹‰ Agent Loop é€»è¾‘
2. **ç»Ÿä¸€ API**ï¼šå±è”½ä¸åŒæ¨ç†å¼•æ“ï¼ˆvLLM/SGLangï¼‰çš„å·®å¼‚
3. **è´Ÿè½½å‡è¡¡**ï¼šå¤šä¸ª LLM Server ä¹‹é—´è‡ªåŠ¨è´Ÿè½½å‡è¡¡

**éç›®æ ‡ï¼ˆNot Goalsï¼‰ï¼š**
- Tool å¦‚ä½•å®šä¹‰ï¼ˆç”±ç”¨æˆ·å†³å®šï¼‰
- Tool å¦‚ä½•è°ƒç”¨ï¼ˆç”±ç”¨æˆ·å®ç°ï¼‰

### 1.2 Agent Loop vs å•è½® Rollout

**å•è½® Rolloutï¼š**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prompt â”‚  â†’   â”‚ LLM Generateâ”‚  â†’   â”‚ Response â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Agent Loopï¼ˆå¤šè½® + å·¥å…·ï¼‰ï¼š**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prompt â”‚  â†’   â”‚ LLM Generate â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ Tool Call?    â”‚
               â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
                  Yes     No
                   â†“       â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚Call Tool â”‚  â”‚  Done    â”‚
            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Tool Responseâ”‚
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ LLM Generate â”‚ (with context)
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
                  ...
```

**å…³é”®åŒºåˆ«ï¼š**

| ç‰¹æ€§ | å•è½® Rollout | Agent Loop |
|------|-------------|-----------|
| **LLM è°ƒç”¨æ¬¡æ•°** | 1 æ¬¡ | å¤šæ¬¡ |
| **Tool è°ƒç”¨** | æ—  | æœ‰ |
| **å¤–éƒ¨ I/O** | æ—  | æœ‰ï¼ˆTool è°ƒç”¨ï¼‰|
| **GPU åˆ©ç”¨ç‡** | 100%ï¼ˆç”Ÿæˆæ—¶ï¼‰ | éœ€è¦å¼‚æ­¥ä¼˜åŒ– |
| **Response ç»“æ„** | çº¯ LLM ç”Ÿæˆ | LLM + Tool æ··åˆ |

### 1.3 ä¸ºä»€ä¹ˆéœ€è¦å¼‚æ­¥ Rolloutï¼Ÿ

**é—®é¢˜ï¼š** Tool è°ƒç”¨æ¶‰åŠå¤–éƒ¨ I/Oï¼Œå¯èƒ½å¾ˆæ…¢

**åŒæ­¥æ‰§è¡Œçš„é—®é¢˜ï¼š**
```python
# ä¼ªä»£ç ï¼šåŒæ­¥ Agent Loop
def sync_agent_loop(prompt):
    response1 = llm.generate(prompt)        # GPU å·¥ä½œ âœ“
    tool_result = call_api(response1)       # GPU ç©ºé—² âœ— (ç­‰å¾…ç½‘ç»œè¯·æ±‚)
    response2 = llm.generate(context)       # GPU å·¥ä½œ âœ“
    return response2

# æ‰¹é‡æ‰§è¡Œ
for prompt in batch:
    result = sync_agent_loop(prompt)  # ä¸²è¡Œæ‰§è¡Œï¼ŒGPU å¤§é‡ç©ºé—²
```

**æ—¶é—´åˆ†æï¼š**
- LLM Generate: 100ms Ã— 2 = 200ms
- Tool Call (API): 500ms
- **æ€»æ—¶é—´**: 700ms
- **GPU åˆ©ç”¨ç‡**: 200ms / 700ms = 28.6%

**å¼‚æ­¥æ‰§è¡Œçš„ä¼˜åŠ¿ï¼š**
```python
# ä¼ªä»£ç ï¼šå¼‚æ­¥ Agent Loop
async def async_agent_loop(prompt):
    response1 = await llm.generate(prompt)        # GPU å·¥ä½œ âœ“
    tool_result = await call_api(response1)       # GPU å¤„ç†å…¶ä»–è¯·æ±‚ âœ“
    response2 = await llm.generate(context)       # GPU å·¥ä½œ âœ“
    return response2

# å¹¶å‘æ‰§è¡Œ
results = await asyncio.gather(*[
    async_agent_loop(p) for p in batch
])
```

**æ—¶é—´åˆ†æï¼ˆbatch_size=8ï¼‰ï¼š**
- æ€»æ—¶é—´ï¼šçº¦ 1000msï¼ˆå¹¶å‘æ‰§è¡Œï¼‰
- GPU åˆ©ç”¨ç‡ï¼šçº¦ 80%ï¼ˆå¤§éƒ¨åˆ†æ—¶é—´éƒ½åœ¨å¤„ç†æŸä¸ªè¯·æ±‚ï¼‰

**æ€§èƒ½æå‡ï¼š**
- åŒæ­¥ï¼š700ms Ã— 8 = 5600ms
- å¼‚æ­¥ï¼šçº¦ 1000ms
- **æå‡ï¼š5.6 å€**

---

## 2. ç³»ç»Ÿæ¶æ„æ·±åº¦è§£æ

### 2.1 æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PPOTrainer                              â”‚
â”‚  - è®­ç»ƒä¸»å¾ªç¯                                                â”‚
â”‚  - è°ƒç”¨ AgentLoopManager.generate_sequences()              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  AgentLoopManager                           â”‚
â”‚  - wake_up() æ‰€æœ‰ LLM Servers (åŒæ­¥æƒé‡)                    â”‚
â”‚  - åˆ†å‘ prompts åˆ°å¤šä¸ª AgentLoopWorkers                     â”‚
â”‚  - æ”¶é›†æ‰€æœ‰ AgentLoopOutput                                 â”‚
â”‚  - sleep() æ‰€æœ‰ LLM Servers (é‡Šæ”¾æ˜¾å­˜)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AgentLoopWorker 1   â”‚     â”‚  AgentLoopWorker N   â”‚
â”‚                      â”‚ ... â”‚                      â”‚
â”‚  è¿è¡Œå¤šä¸ªå¹¶å‘çš„       â”‚     â”‚  è¿è¡Œå¤šä¸ªå¹¶å‘çš„       â”‚
â”‚  AgentLoop åç¨‹      â”‚     â”‚  AgentLoop åç¨‹      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                            â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AsyncLLMServerManager                          â”‚
â”‚  - è´Ÿè½½å‡è¡¡ï¼ˆé€‰æ‹©è´Ÿè½½æœ€å°çš„ Serverï¼‰                         â”‚
â”‚  - Sticky Sessionï¼ˆåç»­è¯·æ±‚å‘åˆ°åŒä¸€ Serverï¼‰                â”‚
â”‚  - æä¾›ç»Ÿä¸€çš„ generate() æ¥å£                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AsyncServer 1       â”‚     â”‚  AsyncServer N       â”‚
â”‚  (vLLM/SGLang)       â”‚ ... â”‚  (vLLM/SGLang)       â”‚
â”‚                      â”‚     â”‚                      â”‚
â”‚  - è¿æ¥ä¸€ä¸ª DP group â”‚     â”‚  - è¿æ¥ä¸€ä¸ª DP group â”‚
â”‚  - åŒæ­¥è®­ç»ƒæƒé‡      â”‚     â”‚  - åŒæ­¥è®­ç»ƒæƒé‡      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 æ ¸å¿ƒç»„ä»¶è¯¦è§£

#### 2.2.1 AgentLoopManager

**ä½ç½®ï¼š** `verl/trainer/ppo/rollout/agent_loop/agent_loop_manager.py`

**èŒè´£ï¼š**
1. ç®¡ç†æ‰€æœ‰ LLM Server çš„ç”Ÿå‘½å‘¨æœŸï¼ˆwake_up/sleepï¼‰
2. åˆ†å‘ prompts åˆ°å¤šä¸ª Worker
3. æ”¶é›†å¹¶æ•´ç†æ‰€æœ‰ AgentLoopOutput

**å…³é”®æ–¹æ³•ï¼š**

```python
class AgentLoopManager:
    def wake_up(self):
        """
        å”¤é†’æ‰€æœ‰ LLM Servers
        - ä» FSDP/Megatron-LM åŒæ­¥æœ€æ–°çš„æ¨¡å‹æƒé‡åˆ° vLLM/SGLang
        - ä¸ºæ–°ä¸€è½® Rollout åšå‡†å¤‡
        """
        for server in self.servers:
            server.wake_up()

    async def generate_sequences(self, batch):
        """
        ä¸»å…¥å£ï¼šç”Ÿæˆä¸€ä¸ª batch çš„ sequences

        æµç¨‹ï¼š
        1. wake_up() æ‰€æœ‰ servers
        2. åˆ†å‘ prompts åˆ°å¤šä¸ª workers
        3. å¹¶å‘æ‰§è¡Œæ‰€æœ‰ agent loops
        4. æ”¶é›†æ‰€æœ‰ outputs
        5. sleep() æ‰€æœ‰ servers
        """
        # 1. å”¤é†’ servers
        self.wake_up()

        # 2. åˆ†å‘åˆ° workers
        chunks = split_batch(batch, num_workers=self.num_workers)

        # 3. å¹¶å‘æ‰§è¡Œ
        tasks = [
            worker.process_chunk(chunk)
            for worker, chunk in zip(self.workers, chunks)
        ]
        results = await asyncio.gather(*tasks)

        # 4. æ•´ç†è¾“å‡º
        outputs = self.merge_outputs(results)

        # 5. ä¼‘çœ  servers
        self.sleep()

        return outputs

    def sleep(self):
        """
        ä¼‘çœ æ‰€æœ‰ LLM Servers
        - é‡Šæ”¾ KV Cache
        - (å¯é€‰) Offload weights åˆ° CPU
        """
        for server in self.servers:
            server.sleep()
```

#### 2.2.2 AgentLoopWorker

**ä½ç½®ï¼š** `verl/trainer/ppo/rollout/agent_loop/agent_loop_worker.py`

**èŒè´£ï¼š**
1. æ¥æ”¶ä¸€ä¸ª chunk çš„ prompts
2. ä¸ºæ¯ä¸ª prompt åˆ›å»ºä¸€ä¸ª AgentLoop å®ä¾‹
3. å¹¶å‘æ‰§è¡Œæ‰€æœ‰ AgentLoop åç¨‹

**å…³é”®æ–¹æ³•ï¼š**

```python
class AgentLoopWorker:
    async def process_chunk(self, chunk):
        """
        å¤„ç†ä¸€ä¸ª chunk çš„ prompts

        Args:
            chunk: åŒ…å«å¤šä¸ª prompts çš„ batch chunk

        Returns:
            List[AgentLoopOutput]: æ¯ä¸ª prompt çš„è¾“å‡º
        """
        tasks = []

        for i, prompt_data in enumerate(chunk):
            # æ ¹æ® agent_name é€‰æ‹© AgentLoop ç±»
            agent_name = prompt_data.get("agent_name", "single_turn")

            if agent_name == "tool_agent":
                agent_loop = ToolAgentLoop(
                    llm_server=self.llm_server_manager,
                    tokenizer=self.tokenizer,
                    **prompt_data
                )
            else:
                agent_loop = SingleTurnAgentLoop(
                    llm_server=self.llm_server_manager,
                    tokenizer=self.tokenizer,
                    **prompt_data
                )

            # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
            task = agent_loop.run(sampling_params=self.sampling_params)
            tasks.append(task)

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ agent loops
        outputs = await asyncio.gather(*tasks)

        return outputs
```

#### 2.2.3 AsyncLLMServerManager

**ä½ç½®ï¼š** `verl/trainer/ppo/rollout/async_server/async_llm_server_manager.py`

**èŒè´£ï¼š**
1. ç®¡ç†å¤šä¸ª AsyncServer å®ä¾‹
2. è´Ÿè½½å‡è¡¡ï¼ˆé¦–æ¬¡è¯·æ±‚é€‰æ‹©è´Ÿè½½æœ€å°çš„ Serverï¼‰
3. Sticky Sessionï¼ˆåç»­è¯·æ±‚å‘åˆ°åŒä¸€ Serverï¼‰

**å…³é”®æ–¹æ³•ï¼š**

```python
class AsyncLLMServerManager:
    def __init__(self, servers: List[AsyncServerBase]):
        self.servers = servers
        self.request_to_server = {}  # request_id â†’ server_id

    async def generate(
        self,
        request_id: str,
        *,
        prompt_ids: list[int],
        sampling_params: dict[str, Any],
    ) -> list[int]:
        """
        ç”Ÿæˆ tokens

        Args:
            request_id: è¯·æ±‚ IDï¼ˆç”¨äº sticky sessionï¼‰
            prompt_ids: Prompt token IDs
            sampling_params: é‡‡æ ·å‚æ•°

        Returns:
            List[int]: ç”Ÿæˆçš„ token IDs
        """
        # 1. é€‰æ‹© Serverï¼ˆè´Ÿè½½å‡è¡¡ + sticky sessionï¼‰
        if request_id in self.request_to_server:
            # Sticky session: åç»­è¯·æ±‚å‘åˆ°åŒä¸€ server
            server_id = self.request_to_server[request_id]
        else:
            # è´Ÿè½½å‡è¡¡: é€‰æ‹©è´Ÿè½½æœ€å°çš„ server
            server_id = self._select_server_with_least_load()
            self.request_to_server[request_id] = server_id

        server = self.servers[server_id]

        # 2. è°ƒç”¨ server generate
        response_ids = await server.generate(
            prompt_ids=prompt_ids,
            sampling_params=sampling_params,
            request_id=request_id
        )

        return response_ids

    def _select_server_with_least_load(self) -> int:
        """
        é€‰æ‹©è´Ÿè½½æœ€å°çš„ server

        Returns:
            int: server_id
        """
        loads = [server.get_current_load() for server in self.servers]
        return loads.index(min(loads))
```

**Sticky Session çš„å¿…è¦æ€§ï¼š**

å¤šè½®å¯¹è¯ä¸­ï¼Œåç»­ turns éœ€è¦è®¿é—®ä¹‹å‰çš„ KV Cacheï¼š

```
Turn 1: Server 0 â†’ ç”Ÿæˆ response 1ï¼Œç¼“å­˜ KV Cache
Turn 2: Server 0 â†’ å¤ç”¨ KV Cacheï¼Œç”Ÿæˆ response 2 âœ“

å¦‚æœ Turn 2 å‘åˆ° Server 1 â†’ æ²¡æœ‰ KV Cacheï¼Œéœ€è¦é‡æ–°è®¡ç®— âœ—
```

#### 2.2.4 AsyncServer (vLLM/SGLang)

**ä½ç½®ï¼š** `verl/trainer/ppo/rollout/async_server/`

**èŒè´£ï¼š**
1. å°è£… vLLM/SGLang çš„æ¨ç†å¼•æ“
2. æä¾›ç»Ÿä¸€çš„ generate() æ¥å£
3. å¤„ç†æƒé‡åŒæ­¥ï¼ˆwake_up/sleepï¼‰

**vLLM vs SGLang æ¶æ„å·®å¼‚ï¼š**

**vLLM:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AsyncServer   â”‚ (è¿è¡Œåœ¨ç‹¬ç«‹è¿›ç¨‹)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ ZeroMQ
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AsyncLLMEngine â”‚ (è¿è¡Œåœ¨ç‹¬ç«‹è¿›ç¨‹)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ ZeroMQ
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ModelRunner   â”‚ (è¿è¡Œåœ¨ FSDP Worker è¿›ç¨‹)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**SGLang:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AsyncServer   â”‚ (è¿è¡Œåœ¨ç‹¬ç«‹è¿›ç¨‹)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Ray RPC
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AsyncLLMEngine â”‚ (è¿è¡Œåœ¨ FSDP Worker-0 è¿›ç¨‹)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ ZeroMQ
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ModelRunner   â”‚ (Subprocesses)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®å·®å¼‚ï¼š**
- vLLM: AsyncLLMEngine ç‹¬ç«‹è¿›ç¨‹ï¼Œé€šè¿‡ ZeroMQ é€šä¿¡
- SGLang: AsyncLLMEngine åœ¨ Worker-0ï¼ŒAsyncServer é€šè¿‡ Ray RPC è°ƒç”¨

### 2.3 å®Œæ•´æ•°æ®æµ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PPOTrainer â”‚
â”‚  Batch:     â”‚
â”‚  256 promptsâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“ generate_sequences(batch)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AgentLoopManagerâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ wake_up()
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Servers     â”‚ (åŒæ­¥æƒé‡)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ åˆ†å‘åˆ° Workers  â”‚
â”‚ Worker 1: 128   â”‚
â”‚ Worker 2: 128   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“ asyncio.gather()
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AgentLoopWorker 1       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ AgentLoop 1       â”‚   â”‚
â”‚  â”‚ AgentLoop 2       â”‚   â”‚
â”‚  â”‚ ...               â”‚   â”‚
â”‚  â”‚ AgentLoop 128     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  (å¹¶å‘æ‰§è¡Œ 128 ä¸ªåç¨‹)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“ AgentLoop.run()
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ¯ä¸ª AgentLoop:         â”‚
â”‚  1. LLM Generate        â”‚
â”‚  2. Parse Tool Call     â”‚
â”‚  3. Execute Tool        â”‚
â”‚  4. LLM Generate (ctx)  â”‚
â”‚  5. Return Output       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“ AgentLoopOutput
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  {                      â”‚
â”‚    prompt_ids: [...]    â”‚
â”‚    response_ids: [...]  â”‚
â”‚    response_mask: [...] â”‚
â”‚  }                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“ gather all outputs
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AgentLoopManager       â”‚
â”‚  æ”¶é›† 256 ä¸ª outputs     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ sleep()
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Servers (é‡Šæ”¾ KV)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“ return
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PPOTrainer             â”‚
â”‚  è¿›å…¥ Reward è®¡ç®—é˜¶æ®µ    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. AgentLoopBase æ¥å£è¯¦è§£

### 3.1 æ¥å£å®šä¹‰

**ä½ç½®ï¼š** `verl/trainer/ppo/rollout/agent_loop/agent_loop_base.py`

```python
from abc import ABC, abstractmethod
from typing import Any
from pydantic import BaseModel

class AgentLoopOutput(BaseModel):
    """Agent Loop çš„è¾“å‡º"""

    prompt_ids: list[int]
    """Prompt token IDs"""

    response_ids: list[int]
    """Response token IDs (åŒ…å« LLM ç”Ÿæˆ + Tool å“åº”)"""

    response_mask: list[int]
    """Response mask: 1=LLM ç”Ÿæˆï¼Œ0=Tool å“åº”"""


class AgentLoopBase(ABC):
    """Agent Loop åŸºç±»"""

    def __init__(
        self,
        llm_server: AsyncLLMServerManager,
        tokenizer,
        **kwargs
    ):
        """
        åˆå§‹åŒ– Agent Loop

        Args:
            llm_server: LLM Server ç®¡ç†å™¨
            tokenizer: Tokenizer
            **kwargs: æ•°æ®é›†å­—æ®µï¼ˆprompt, extra_info, etc.ï¼‰
        """
        self.llm_server = llm_server
        self.tokenizer = tokenizer

        # ä» kwargs æå–æ•°æ®
        self.prompt = kwargs.get("prompt")  # List[dict]
        self.extra_info = kwargs.get("extra_info", {})
        self.data_source = kwargs.get("data_source")

    @abstractmethod
    async def run(
        self,
        sampling_params: dict[str, Any],
        **kwargs
    ) -> AgentLoopOutput:
        """
        è¿è¡Œ Agent Loopï¼ˆéœ€è¦ç”¨æˆ·å®ç°ï¼‰

        Args:
            sampling_params: LLM é‡‡æ ·å‚æ•°
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            AgentLoopOutput: åŒ…å« prompt_ids, response_ids, response_mask
        """
        raise NotImplementedError
```

### 3.2 SingleTurnAgentLoop ç¤ºä¾‹

**æœ€ç®€å•çš„å®ç°ï¼šå•è½®ç”Ÿæˆ**

```python
class SingleTurnAgentLoop(AgentLoopBase):
    """å•è½® Agent Loopï¼ˆä¸ä½¿ç”¨å·¥å…·ï¼‰"""

    async def run(
        self,
        sampling_params: dict[str, Any],
        **kwargs
    ) -> AgentLoopOutput:
        """
        å•è½®ç”Ÿæˆ

        æµç¨‹ï¼š
        1. å°† prompt messages è½¬æ¢ä¸º token IDs
        2. è°ƒç”¨ LLM generate
        3. è¿”å› AgentLoopOutput
        """
        # 1. Apply chat template
        prompt_text = self.tokenizer.apply_chat_template(
            self.prompt,
            tokenize=False,
            add_generation_prompt=True
        )

        # 2. Tokenize
        prompt_ids = self.tokenizer.encode(prompt_text)

        # 3. Generate
        response_ids = await self.llm_server.generate(
            request_id=self._get_request_id(),
            prompt_ids=prompt_ids,
            sampling_params=sampling_params
        )

        # 4. Response mask (å…¨éƒ¨æ˜¯ LLM ç”Ÿæˆ)
        response_mask = [1] * len(response_ids)

        # 5. è¿”å›
        return AgentLoopOutput(
            prompt_ids=prompt_ids,
            response_ids=response_ids,
            response_mask=response_mask
        )

    def _get_request_id(self) -> str:
        """ç”Ÿæˆå”¯ä¸€çš„ request_id"""
        return f"{self.data_source}_{id(self)}"
```

### 3.3 ToolAgentLoop ç¤ºä¾‹

**å¸¦å·¥å…·è°ƒç”¨çš„å®ç°**

```python
class ToolAgentLoop(AgentLoopBase):
    """æ”¯æŒå·¥å…·è°ƒç”¨çš„ Agent Loop"""

    def __init__(self, llm_server, tokenizer, **kwargs):
        super().__init__(llm_server, tokenizer, **kwargs)

        # åˆå§‹åŒ–å·¥å…·
        self.tools = self._init_tools()

    def _init_tools(self):
        """
        ä» extra_info ä¸­åˆå§‹åŒ–å·¥å…·

        extra_info.tools_kwargs = {
            "tool_name": {
                "create_kwargs": {...},
                "execute_kwargs": {...}
            }
        }
        """
        tools = {}

        tools_kwargs = self.extra_info.get("tools_kwargs", {})

        for tool_name, tool_config in tools_kwargs.items():
            # åŠ¨æ€å¯¼å…¥å·¥å…·ç±»
            tool_class = self._get_tool_class(tool_name)

            # åˆ›å»ºå·¥å…·å®ä¾‹
            create_kwargs = tool_config.get("create_kwargs", {})
            tools[tool_name] = tool_class(**create_kwargs)

        return tools

    async def run(
        self,
        sampling_params: dict[str, Any],
        **kwargs
    ) -> AgentLoopOutput:
        """
        å¤šè½® Agent Loop with Tools

        æµç¨‹ï¼š
        1. LLM Generate
        2. Parse Tool Call
        3. Execute Tool (if needed)
        4. LLM Generate with context (if needed)
        5. Return AgentLoopOutput
        """
        # åˆå§‹åŒ–
        chat_history = list(self.prompt)  # å¤åˆ¶ prompt
        all_response_ids = []
        all_response_mask = []

        # Apply chat template
        prompt_text = self.tokenizer.apply_chat_template(
            chat_history,
            tokenize=False,
            add_generation_prompt=True
        )
        prompt_ids = self.tokenizer.encode(prompt_text)

        request_id = self._get_request_id()
        max_turns = 10  # æœ€å¤§è½®æ¬¡

        for turn in range(max_turns):
            # Turn N: LLM Generate
            response_ids = await self.llm_server.generate(
                request_id=request_id,
                prompt_ids=prompt_ids if turn == 0 else [],  # é¦–æ¬¡å‘ promptï¼Œåç»­ä¸ºç©º
                sampling_params=sampling_params
            )

            # Decode
            response_text = self.tokenizer.decode(response_ids)

            # æ·»åŠ åˆ°æ€»å“åº”
            all_response_ids.extend(response_ids)
            all_response_mask.extend([1] * len(response_ids))  # LLM ç”Ÿæˆ

            # æ›´æ–° chat history
            chat_history.append({
                "role": "assistant",
                "content": response_text
            })

            # Parse tool calls
            tool_calls = self._parse_tool_calls(response_text)

            if not tool_calls:
                # æ²¡æœ‰ tool callï¼Œç»“æŸ
                break

            # Execute tools
            for tool_call in tool_calls:
                tool_name = tool_call["name"]
                tool_args = tool_call["arguments"]

                # è°ƒç”¨å·¥å…·
                tool_result = self.tools[tool_name].execute(**tool_args)

                # å°† tool result è½¬æ¢ä¸º tokens
                tool_result_text = json.dumps(tool_result)
                tool_result_ids = self.tokenizer.encode(tool_result_text)

                # æ·»åŠ åˆ°æ€»å“åº”
                all_response_ids.extend(tool_result_ids)
                all_response_mask.extend([0] * len(tool_result_ids))  # Tool å“åº”

                # æ›´æ–° chat history
                chat_history.append({
                    "role": "tool",
                    "content": tool_result_text,
                    "tool_call_id": tool_call.get("id")
                })

            # ä¸‹ä¸€è½®ï¼šLLM ç»§ç»­ç”Ÿæˆï¼ˆåŸºäº tool resultsï¼‰
            # prompt_ids ä¸ºç©ºï¼Œå› ä¸º chat history å·²ç»åœ¨ server ç«¯

        return AgentLoopOutput(
            prompt_ids=prompt_ids,
            response_ids=all_response_ids,
            response_mask=all_response_mask
        )

    def _parse_tool_calls(self, text: str) -> list[dict]:
        """
        ä»æ–‡æœ¬ä¸­è§£æå·¥å…·è°ƒç”¨

        ç¤ºä¾‹æ ¼å¼:
        <tool_call>
        {"name": "calc", "arguments": {"expression": "2+3"}}
        </tool_call>
        """
        tool_calls = []

        # æ­£åˆ™æå– <tool_call>...</tool_call>
        pattern = r"<tool_call>(.*?)</tool_call>"
        matches = re.findall(pattern, text, re.DOTALL)

        for match in matches:
            try:
                tool_call = json.loads(match.strip())
                tool_calls.append(tool_call)
            except json.JSONDecodeError:
                # è§£æå¤±è´¥ï¼ˆæ ¼å¼é”™è¯¯ï¼‰
                print(f"Failed to parse tool call: {match}")
                continue

        return tool_calls

    def _get_request_id(self) -> str:
        return f"{self.data_source}_{id(self)}"

    def _get_tool_class(self, tool_name: str):
        """åŠ¨æ€å¯¼å…¥å·¥å…·ç±»"""
        # ç¤ºä¾‹ï¼šä»é¢„å®šä¹‰çš„å·¥å…·æ³¨å†Œè¡¨è·å–
        from my_tools import TOOL_REGISTRY
        return TOOL_REGISTRY[tool_name]
```

---

## 4. å·¥å…·è°ƒç”¨å®ç°

### 4.1 å·¥å…·å®šä¹‰å’Œæ³¨å†Œ

åœ¨ Agent RL ä¸­ï¼Œå·¥å…·æ˜¯ Agent å®Œæˆç‰¹å®šä»»åŠ¡çš„å…³é”®ã€‚æˆ‘ä»¬å…ˆçœ‹å¦‚ä½•å®šä¹‰å’Œæ³¨å†Œå·¥å…·ã€‚

#### å·¥å…·çš„æ•°æ®æ ¼å¼

ä» `gsm8k_tool_agent_loop.py` æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å·¥å…·æ˜¯å¦‚ä½•åœ¨æ•°æ®é›†ä¸­å®šä¹‰çš„ï¼š

```python
# examples/data_preprocess/gsm8k_tool_agent_loop.py:96-104
"extra_info": {
    "need_tools_kwargs": True,  # è¡¨ç¤ºéœ€è¦å·¥å…·
    "tools_kwargs": {
        "calc_gsm8k_reward": {  # å·¥å…·å
            "create_kwargs": {"ground_truth": solution},  # å·¥å…·åˆ›å»ºå‚æ•°
            # "execute_kwargs": {},    # å·¥å…·æ‰§è¡Œå‚æ•°ï¼ˆå¯é€‰ï¼‰
            # "calc_reward_kwargs": {},  # Reward è®¡ç®—å‚æ•°ï¼ˆå¯é€‰ï¼‰
            # "release_kwargs": {},    # å·¥å…·é‡Šæ”¾å‚æ•°ï¼ˆå¯é€‰ï¼‰
        },
    },
}
```

**å…³é”®å­—æ®µï¼š**
- `need_tools_kwargs`: æ˜¯å¦éœ€è¦å·¥å…·
- `tools_kwargs`: å·¥å…·é…ç½®å­—å…¸
  - æ¯ä¸ªå·¥å…·æœ‰ 4 ä¸ªç”Ÿå‘½å‘¨æœŸé’©å­ï¼š
    - `create_kwargs`: å·¥å…·åˆå§‹åŒ–å‚æ•°ï¼ˆå¦‚ ground_truthï¼‰
    - `execute_kwargs`: å·¥å…·æ‰§è¡Œå‚æ•°
    - `calc_reward_kwargs`: Reward è®¡ç®—å‚æ•°
    - `release_kwargs`: å·¥å…·æ¸…ç†å‚æ•°

#### å·¥å…·çš„ç”Ÿå‘½å‘¨æœŸ

æ¯ä¸ªå·¥å…·ç»å† 4 ä¸ªé˜¶æ®µï¼š

```python
# ä¼ªä»£ç ç¤ºä¾‹
class ToolBase:
    def create(self, **create_kwargs):
        """å·¥å…·åˆå§‹åŒ–ï¼ˆæ¯ä¸ª sample ä¸€æ¬¡ï¼‰"""
        pass

    def execute(self, **execute_kwargs):
        """å·¥å…·æ‰§è¡Œï¼ˆæ¯æ¬¡è°ƒç”¨ä¸€æ¬¡ï¼‰"""
        pass

    def calc_reward(self, trajectory, **calc_reward_kwargs):
        """è®¡ç®— Rewardï¼ˆrollout ç»“æŸåï¼‰"""
        pass

    def release(self, **release_kwargs):
        """æ¸…ç†èµ„æº"""
        pass
```

### 4.2 GSM8K å·¥å…·å®ç°åˆ†æ

è®©æˆ‘ä»¬æ·±å…¥åˆ†æ GSM8K çš„ `calc_gsm8k_reward` å·¥å…·å®ç°ã€‚

#### æ ¸å¿ƒ Reward è®¡ç®—é€»è¾‘

```python
# verl/utils/reward_score/gsm8k.py:52-72
def compute_score(solution_str, ground_truth, method="strict", format_score=0.0, score=1.0):
    """GSM8k çš„è¯„åˆ†å‡½æ•°

    Args:
        solution_str: æ¨¡å‹ç”Ÿæˆçš„è§£ç­”æ–‡æœ¬
        ground_truth: æ­£ç¡®ç­”æ¡ˆ
        method: 'strict' æˆ– 'flexible'
        format_score: æ ¼å¼æ­£ç¡®ä½†ç­”æ¡ˆé”™è¯¯çš„åˆ†æ•°
        score: ç­”æ¡ˆæ­£ç¡®çš„åˆ†æ•°
    """
    answer = extract_solution(solution_str=solution_str, method=method)
    if answer is None:
        return 0  # æ²¡æœ‰æ‰¾åˆ°ç­”æ¡ˆï¼Œ0 åˆ†
    else:
        if answer == ground_truth:
            return score  # ç­”æ¡ˆæ­£ç¡®ï¼Œæ»¡åˆ†
        else:
            return format_score  # æ ¼å¼æ­£ç¡®ï¼Œéƒ¨åˆ†åˆ†
```

#### ç­”æ¡ˆæå–é€»è¾‘

```python
# verl/utils/reward_score/gsm8k.py:20-49
def extract_solution(solution_str, method="strict"):
    # ä¼˜åŒ–ï¼šåªåœ¨æœ€å 300 å­—ç¬¦ä¸­æœç´¢ï¼ˆé¿å…æ­£åˆ™æ€§èƒ½é—®é¢˜ï¼‰
    if len(solution_str) > _SOLUTION_CLIP_CHARS:
        solution_str = solution_str[-_SOLUTION_CLIP_CHARS:]

    if method == "strict":
        # ä¸¥æ ¼æ¨¡å¼ï¼šå¿…é¡»æœ‰ `#### ç­”æ¡ˆ` æ ¼å¼
        solutions = re.findall("#### (\\-?[0-9\\.\\,]+)", solution_str)
        if len(solutions) == 0:
            final_answer = None
        else:
            # å–æœ€åä¸€ä¸ªç­”æ¡ˆ
            final_answer = solutions[-1].replace(",", "").replace("$", "")

    elif method == "flexible":
        # çµæ´»æ¨¡å¼ï¼šæå–æœ€åä¸€ä¸ªæ•°å­—
        answer = re.findall("(\\-?[0-9\\.\\,]+)", solution_str)
        final_answer = None
        if len(answer) == 0:
            pass
        else:
            # æ‰¾åˆ°æœ€åä¸€ä¸ªéç©ºæ•°å­—
            for final_answer in reversed(answer):
                if final_answer not in ["", "."]:
                    break

    return final_answer
```

**ä¸¤ç§æ¨¡å¼å¯¹æ¯”ï¼š**

| æ¨¡å¼ | è¦æ±‚ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|------|------|
| **strict** | å¿…é¡»æœ‰ `#### <answer>` æ ¼å¼ | åŒæ—¶æµ‹è¯•ç­”æ¡ˆå’Œæ ¼å¼ | å¯èƒ½å› æ ¼å¼é”™è¯¯ä¸¢åˆ† |
| **flexible** | æå–æœ€åä¸€ä¸ªæ•°å­— | æ›´å®½å®¹ï¼Œå…³æ³¨ç­”æ¡ˆæœ¬èº« | å¯èƒ½æå–é”™è¯¯çš„æ•°å­— |

### 4.3 å·¥å…·è°ƒç”¨æµç¨‹å®Œæ•´ç¤ºä¾‹

ç°åœ¨æˆ‘ä»¬è¿½è¸ªä¸€ä¸ªå®Œæ•´çš„ GSM8K é—®é¢˜çš„å·¥å…·è°ƒç”¨æµç¨‹ã€‚

#### Step 1: æ•°æ®å‡†å¤‡

```python
# examples/data_preprocess/gsm8k_tool_agent_loop.py:73-88
{
    "prompt": [
        {
            "role": "system",
            "content": (
                "You are a math expert. You are given a question and you need to solve it step by step. "
                "Reasoning step by step before any tool call. "
                "You should use the `calc_gsm8k_reward` tool after step by step solving the question, "
                "before generate final answer at least once and refine your answer if necessary. "
                "Put your final answer in the format of `#### <answer>`."
            ),
        },
        {
            "role": "user",
            "content": "Janet's ducks lay 16 eggs per day. She eats three for breakfast... Let's think step by step and output the final answer after `####`.",
        },
    ],
}
```

#### Step 2: LLM ç¬¬ä¸€è½®ç”Ÿæˆï¼ˆæ¨ç†ï¼‰

```python
# Agent Loop è°ƒç”¨ LLM
response_1 = await server_manager.generate(
    request_id=request_id,
    prompt_ids=prompt_ids,  # system + user çš„ token ids
    sampling_params={...}
)

# æ¨¡å‹è¾“å‡ºï¼ˆç¤ºä¾‹ï¼‰ï¼š
"""
Let me solve this step by step:
1. Janet's ducks lay 16 eggs per day
2. She eats 3 for breakfast every morning
3. She bakes muffins for her friends every day with 4 eggs
4. So she uses 3 + 4 = 7 eggs per day
5. She has 16 - 7 = 9 eggs left
6. She sells them at the farmers' market for $2 per egg
7. So she makes 9 * $2 = $18 per day

<tool_call>
{"name": "calc_gsm8k_reward", "arguments": {"solution": "#### 18"}}
</tool_call>
"""
```

#### Step 3: è§£æå·¥å…·è°ƒç”¨

```python
# verl/experimental/agent_loop/agent_loop.py ä¸­çš„ _parse_tool_calls
tool_calls = self._parse_tool_calls(response_text)

# ç»“æœï¼š
[
    {
        "name": "calc_gsm8k_reward",
        "arguments": {"solution": "#### 18"}
    }
]
```

#### Step 4: æ‰§è¡Œå·¥å…·

```python
# å·¥å…·æ‰§è¡Œ
tool_result = calc_gsm8k_reward.execute(solution="#### 18")

# å†…éƒ¨è°ƒç”¨ compute_score
answer = extract_solution("#### 18", method="strict")  # "18"
if answer == ground_truth:  # "18" == "18"
    return 1.0  # æ­£ç¡®ï¼
else:
    return 0.0

# tool_result = {"score": 1.0, "extracted_answer": "18", "correct": True}
```

#### Step 5: å·¥å…·ç»“æœæ³¨å…¥ trajectory

```python
# å°† tool result è½¬ä¸º tokens å¹¶æ·»åŠ åˆ°å“åº”
tool_result_text = json.dumps(tool_result)
# '{"score": 1.0, "extracted_answer": "18", "correct": true}'

tool_result_ids = tokenizer.encode(tool_result_text)
# [123, 456, 789, ...]  # token ids

# æ·»åŠ åˆ° response_ids
all_response_ids.extend(tool_result_ids)
all_response_mask.extend([0] * len(tool_result_ids))  # ï¼å·¥å…·å“åº” mask=0
```

**å…³é”®ç‚¹ï¼šå·¥å…·å“åº”çš„ response_mask ä¸º 0**
- `response_mask=1`: LLM ç”Ÿæˆçš„ tokenï¼ˆéœ€è¦è®¡ç®— lossï¼‰
- `response_mask=0`: å·¥å…·å“åº” tokenï¼ˆä¸è®¡ç®— lossï¼Œè§†ä¸ºç¯å¢ƒè§‚å¯Ÿï¼‰

#### Step 6: LLM ç¬¬äºŒè½®ç”Ÿæˆï¼ˆåŸºäºå·¥å…·åé¦ˆï¼‰

```python
# æ›´æ–° chat history
chat_history.append({
    "role": "tool",
    "content": '{"score": 1.0, "extracted_answer": "18", "correct": true}',
    "tool_call_id": "..."
})

# LLM ç»§ç»­ç”Ÿæˆ
response_2 = await server_manager.generate(
    request_id=request_id,  # åŒä¸€ä¸ª request_idï¼
    prompt_ids=[],  # ç©ºçš„ï¼Œå› ä¸º chat history åœ¨ server ç«¯
    sampling_params={...}
)

# æ¨¡å‹è¾“å‡ºï¼ˆç¤ºä¾‹ï¼‰ï¼š
"""
Great! The calculation is correct. Let me finalize the answer.

#### 18
"""
```

#### Step 7: æœ€ç»ˆ trajectory ç»“æ„

```python
AgentLoopOutput(
    prompt_ids=[101, 102, 103, ...],  # system + user çš„ token ids

    response_ids=[
        # ç¬¬ä¸€è½® LLM ç”Ÿæˆ
        104, 105, 106, ..., 200,  # "Let me solve..."
        201, 202, 203,            # "<tool_call>..."

        # å·¥å…·å“åº”ï¼ˆmask=0ï¼‰
        300, 301, 302, ..., 350,  # '{"score": 1.0, ...}'

        # ç¬¬äºŒè½® LLM ç”Ÿæˆ
        400, 401, 402, ..., 450,  # "Great! The calculation..."
        451, 452, 453,            # "#### 18"
    ],

    response_mask=[
        # ç¬¬ä¸€è½® LLMï¼ˆmask=1ï¼‰
        1, 1, 1, ..., 1,

        # å·¥å…·å“åº”ï¼ˆmask=0ï¼‰
        0, 0, 0, ..., 0,

        # ç¬¬äºŒè½® LLMï¼ˆmask=1ï¼‰
        1, 1, 1, ..., 1,
    ],

    num_turns=3  # user, assistant(å«tool call), tool, assistant
)
```

### 4.4 Sticky Session æœºåˆ¶è¯¦è§£

åœ¨å¤šè½®å¯¹è¯ä¸­ï¼Œ**åŒä¸€ä¸ª request_id çš„æ‰€æœ‰è¯·æ±‚å¿…é¡»å‘é€åˆ°åŒä¸€ä¸ª vLLM server**ï¼Œä»¥åˆ©ç”¨ **Prefix Caching**ã€‚

#### AsyncLLMServerManager çš„å®ç°

```python
# verl/experimental/agent_loop/agent_loop.py:57-92
class AsyncLLMServerManager:
    def __init__(self, config, server_handles, max_cache_size=10000):
        self.server_handles = server_handles

        # Least requests load balancingï¼ˆæœ€å°‘è¯·æ±‚æ•°è´Ÿè½½å‡è¡¡ï¼‰
        self.weighted_serveres = [[0, idx, server] for idx, server in enumerate(server_handles)]
        heapq.heapify(self.weighted_serveres)  # æœ€å°å †

        # LRU cache: request_id -> server
        self.request_id_to_server = LRUCache(maxsize=max_cache_size)

    def _choose_server(self, request_id: str):
        # 1. å¦‚æœ request_id å·²ç»æ˜ å°„åˆ°æŸä¸ª serverï¼Œè¿”å›è¯¥ server
        if request_id in self.request_id_to_server:
            return self.request_id_to_server[request_id]

        # 2. é€‰æ‹©è¯·æ±‚æ•°æœ€å°‘çš„ server
        _, _, server = self.weighted_serveres[0]
        self.weighted_serveres[0][0] += 1  # è¯·æ±‚æ•° +1
        heapq.heapreplace(self.weighted_serveres, self.weighted_serveres[0])

        # 3. ç¼“å­˜æ˜ å°„
        self.request_id_to_server[request_id] = server
        return server
```

**å·¥ä½œæµç¨‹ï¼š**

```
Sample 1 (request_id="gsm8k_001"):
    Turn 1 â†’ é€‰æ‹© Server Aï¼ˆè¯·æ±‚æ•°æœ€å°‘ï¼‰â†’ ç¼“å­˜ "gsm8k_001" â†’ Server A
    Turn 2 â†’ æŸ¥æ‰¾ç¼“å­˜ â†’ Server Aï¼ˆå¤ç”¨ KV Cacheï¼ï¼‰
    Turn 3 â†’ æŸ¥æ‰¾ç¼“å­˜ â†’ Server A

Sample 2 (request_id="gsm8k_002"):
    Turn 1 â†’ é€‰æ‹© Server Bï¼ˆè¯·æ±‚æ•°æœ€å°‘ï¼‰â†’ ç¼“å­˜ "gsm8k_002" â†’ Server B
    Turn 2 â†’ æŸ¥æ‰¾ç¼“å­˜ â†’ Server B
```

**æ€§èƒ½æå‡ï¼š**

| åœºæ™¯ | æ—  Sticky Session | æœ‰ Sticky Session |
|------|-------------------|-------------------|
| **KV Cache å‘½ä¸­ç‡** | 0% | ~90%+ |
| **å»¶è¿Ÿï¼ˆç¬¬ 2+ è½®ï¼‰** | 500ms | 100ms |
| **ååé‡** | ä½ | é«˜ 5 å€+ |

---

## 5. å¤šè½®å¯¹è¯è®­ç»ƒ

### 5.1 Token-based API vs Chat Completion API

è¿™æ˜¯ Agent RL ä¸­æœ€å…³é”®çš„è®¾è®¡å†³ç­–ä¹‹ä¸€ã€‚

#### é—®é¢˜ï¼šä¸ºä»€ä¹ˆéœ€è¦ Token-based APIï¼Ÿ

åœ¨ RL è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬éœ€è¦ï¼š
1. **å®Œæ•´çš„ trajectory token ids**ï¼ˆç”¨äºè®¡ç®— log_probï¼‰
2. **ç²¾ç¡®çš„ response_mask**ï¼ˆåŒºåˆ† LLM ç”Ÿæˆ vs å·¥å…·å“åº”ï¼‰

**Chat Completion API çš„é—®é¢˜ï¼š**

```python
# ä½¿ç”¨ OpenAI Chat Completion APIï¼ˆvLLM å…¼å®¹ï¼‰
response = client.chat.completions.create(
    model="Qwen2.5-7B",
    messages=[
        {"role": "system", "content": "You are a math expert..."},
        {"role": "user", "content": "Solve: 2+3=?"},
        {"role": "assistant", "content": "<tool_call>...</tool_call>"},
        {"role": "tool", "content": '{"result": 5}'},
    ]
)

# è¿”å›ï¼š
# {
#   "choices": [{"message": {"role": "assistant", "content": "The answer is 5"}}]
# }
```

**é—®é¢˜ï¼š**
1. âŒ æ— æ³•è·å–å®Œæ•´çš„ token idsï¼ˆåªæœ‰æœ€åä¸€è½®çš„æ–‡æœ¬ï¼‰
2. âŒ æ— æ³•åŒºåˆ†å“ªäº› token æ˜¯ LLM ç”Ÿæˆï¼Œå“ªäº›æ˜¯å·¥å…·å“åº”
3. âŒ æ— æ³•è®¡ç®— old_log_probï¼ˆå› ä¸ºç¼ºå°‘ token idsï¼‰

#### è§£å†³æ–¹æ¡ˆï¼šToken-based API

```python
# verl çš„ Agent Loop ä½¿ç”¨ Token-based API
# verl/experimental/agent_loop/agent_loop.py:94-122
@rollout_trace_op
async def generate(
    self,
    request_id,
    *,
    prompt_ids: list[int],  # ï¼è¾“å…¥æ˜¯ token ids
    sampling_params: dict[str, Any],
    image_data: Optional[list[Any]] = None,
    video_data: Optional[list[Any]] = None,
) -> TokenOutput:
    server = self._choose_server(request_id)
    output = await server.generate.remote(
        request_id=uuid4().hex,  # æ¯æ¬¡ç”Ÿæˆç”¨æ–° request_id
        prompt_ids=prompt_ids,
        sampling_params=sampling_params,
        ...
    )
    return output  # è¿”å› token ids + log_probs
```

**TokenOutput ç»“æ„ï¼š**

```python
class TokenOutput:
    output_token_ids: list[int]  # ç”Ÿæˆçš„ token ids
    logprobs: list[float]        # æ¯ä¸ª token çš„ log_prob
    finish_reason: str           # "stop" / "length"
```

### 5.2 å¤šè½®å¯¹è¯çš„ Trajectory ä¸€è‡´æ€§

#### æ ¸å¿ƒæŒ‘æˆ˜

åœ¨å¤šè½®å¯¹è¯ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ä¿è¯ï¼š

```
è®­ç»ƒæ—¶çš„ input_ids == Rollout æ—¶çš„ input_ids
```

å¦åˆ™ï¼Œ`old_log_prob` å’Œ `new_log_prob` ä¼šä¸åŒ¹é…ï¼Œå¯¼è‡´ PPO ratio è®¡ç®—é”™è¯¯ã€‚

#### å®Œæ•´ç¤ºä¾‹ï¼šè¿½è¸ª Token Flow

**Rollout é˜¶æ®µï¼š**

```python
# Turn 1: User æé—®
prompt_ids = tokenizer.encode([
    {"role": "system", "content": "You are a math expert."},
    {"role": "user", "content": "Solve: 2+3=?"}
])
# [101, 102, ..., 200]  # å‡è®¾ 100 ä¸ª token

# LLM ç”Ÿæˆ
response_1 = await server.generate(request_id="req_001", prompt_ids=prompt_ids)
# output_token_ids: [201, 202, ..., 250]  # "Let me calculate... <tool_call>..."

# Turn 2: å·¥å…·å“åº”
tool_result_text = '{"result": 5}'
tool_result_ids = tokenizer.encode(tool_result_text)
# [300, 301, 302]  # 3 ä¸ª token

# æ„é€  Turn 3 çš„ prompt
# æ–¹æ³• 1ï¼ˆé”™è¯¯ï¼‰ï¼šé‡æ–° apply_chat_template
prompt_ids_turn3 = tokenizer.apply_chat_template([
    {"role": "system", "content": "You are a math expert."},
    {"role": "user", "content": "Solve: 2+3=?"},
    {"role": "assistant", "content": response_1_text},
    {"role": "tool", "content": tool_result_text}
])
# âŒ é—®é¢˜ï¼štoken ids å¯èƒ½å’ŒåŸå§‹ä¸ä¸€è‡´ï¼
# åŸå› ï¼šchat_template å¯èƒ½æ’å…¥é¢å¤–çš„ tokenï¼ˆå¦‚ç©ºæ ¼ã€æ¢è¡Œï¼‰

# æ–¹æ³• 2ï¼ˆæ­£ç¡®ï¼‰ï¼šè¿½åŠ  token ids
prompt_ids_turn3 = (
    prompt_ids +              # [101, ..., 200]
    response_1.output_token_ids +  # [201, ..., 250]
    tool_result_ids           # [300, 301, 302]
)
# [101, ..., 200, 201, ..., 250, 300, 301, 302]  # 353 ä¸ª token

# LLM ç»§ç»­ç”Ÿæˆ
response_2 = await server.generate(request_id="req_001", prompt_ids=prompt_ids_turn3)
# output_token_ids: [400, 401, ..., 420]  # "The answer is 5"
```

**æœ€ç»ˆ Trajectoryï¼š**

```python
AgentLoopOutput(
    prompt_ids=[101, 102, ..., 200],  # åˆå§‹ promptï¼ˆ100 tokensï¼‰

    response_ids=[
        201, 202, ..., 250,  # Turn 1: LLM ç”Ÿæˆï¼ˆ50 tokensï¼‰
        300, 301, 302,       # Turn 2: å·¥å…·å“åº”ï¼ˆ3 tokensï¼‰
        400, 401, ..., 420,  # Turn 3: LLM ç”Ÿæˆï¼ˆ21 tokensï¼‰
    ],  # æ€»å…± 74 ä¸ª response tokens

    response_mask=[
        1, 1, ..., 1,  # Turn 1: LLMï¼ˆ50 ä¸ª 1ï¼‰
        0, 0, 0,       # Turn 2: å·¥å…·ï¼ˆ3 ä¸ª 0ï¼‰
        1, 1, ..., 1,  # Turn 3: LLMï¼ˆ21 ä¸ª 1ï¼‰
    ],
)
```

**è®­ç»ƒé˜¶æ®µï¼š**

```python
# Actor Update æ—¶é‡æ–°å‰å‘ä¼ æ’­
# verl/trainer/ppo/ray_trainer.py ä¸­çš„ update_policy

input_ids = torch.cat([batch.prompt_ids, batch.response_ids], dim=1)
# Shape: [batch_size, 100 + 74] = [batch_size, 174]

# å‰å‘ä¼ æ’­
outputs = actor_model(input_ids=input_ids, attention_mask=attention_mask)
logits = outputs.logits  # [batch_size, 174, vocab_size]

# è®¡ç®— new_log_prob
new_log_prob = compute_log_prob(logits, input_ids, response_mask)

# âœ… å› ä¸º input_ids å®Œå…¨ä¸€è‡´ï¼Œnew_log_prob å’Œ old_log_prob å¯ä»¥æ­£ç¡®å¯¹é½
```

### 5.3 Chat History ç®¡ç†

åœ¨ Agent Loop ä¸­ï¼Œchat history æœ‰ä¸¤ç§ç®¡ç†æ–¹å¼ï¼š

#### æ–¹å¼ 1ï¼šServer ç«¯ç®¡ç†ï¼ˆvLLM Prefix Cachingï¼‰

```python
# Turn 1
await server.generate(
    request_id="req_001",
    prompt_ids=[101, ..., 200],  # system + user
    ...
)
# vLLM ç¼“å­˜ KV Cache for request_id="req_001"

# Turn 2
await server.generate(
    request_id="req_001",  # åŒä¸€ä¸ª request_id
    prompt_ids=[101, ..., 200, 201, ..., 250, 300, 301, 302],  # è¿½åŠ  tool response
    ...
)
# vLLM å¤ç”¨å‰ 200 ä¸ª token çš„ KV Cache
```

**ä¼˜ç‚¹ï¼š**
- è‡ªåŠ¨å¤ç”¨ KV Cache
- æ— éœ€æ‰‹åŠ¨ç®¡ç† history

**ç¼ºç‚¹ï¼š**
- ä¾èµ– server ç«¯å®ç°
- è°ƒè¯•å›°éš¾

#### æ–¹å¼ 2ï¼šClient ç«¯ç®¡ç†

```python
class ToolAgentLoop(AgentLoopBase):
    async def run(self, sampling_params, **kwargs):
        chat_history = [
            {"role": "system", "content": "..."},
            {"role": "user", "content": "..."}
        ]

        all_response_ids = []
        all_response_mask = []

        for turn in range(max_turns):
            # Apply chat template
            prompt_ids = await self.apply_chat_template(
                messages=chat_history,
                remove_system_prompt=(turn > 0)  # ç¬¬ 2+ è½®ç§»é™¤ system prompt
            )

            # Generate
            response = await self.server_manager.generate(
                request_id=self._get_request_id(),
                prompt_ids=prompt_ids,
                ...
            )

            # Update history
            chat_history.append({
                "role": "assistant",
                "content": self.tokenizer.decode(response.output_token_ids)
            })

            # è¿½åŠ  response
            all_response_ids.extend(response.output_token_ids)
            all_response_mask.extend([1] * len(response.output_token_ids))

            # ... å·¥å…·è°ƒç”¨é€»è¾‘ ...
```

**ä¼˜ç‚¹ï¼š**
- çµæ´»ï¼Œå¯è‡ªå®šä¹‰
- æ˜“äºè°ƒè¯•

**ç¼ºç‚¹ï¼š**
- éœ€è¦æ‰‹åŠ¨ç®¡ç† history
- å¯èƒ½å¼•å…¥ token ä¸ä¸€è‡´é—®é¢˜

### 5.4 response_mask çš„å…³é”®ä½œç”¨

`response_mask` åœ¨è®­ç»ƒä¸­æœ‰ 3 ä¸ªå…³é”®ä½œç”¨ï¼š

#### ä½œç”¨ 1ï¼šLoss è®¡ç®—

```python
# verl/trainer/ppo/ray_trainer.py ä¸­çš„ compute_loss

# åªå¯¹ LLM ç”Ÿæˆçš„ token è®¡ç®— loss
loss = -advantages * log_ratio  # [batch_size, response_length]
loss = (loss * response_mask).sum() / response_mask.sum()
```

**åŸå› ï¼š**
- å·¥å…·å“åº”æ˜¯ç¯å¢ƒç»™çš„ï¼Œä¸æ˜¯ LLM ç”Ÿæˆçš„
- å¯¹å·¥å…·å“åº”è®¡ç®— loss æ²¡æœ‰æ„ä¹‰ï¼ˆä¼šå¼•å…¥å™ªå£°ï¼‰

#### ä½œç”¨ 2ï¼šAdvantage å¹¿æ’­ï¼ˆGRPOï¼‰

```python
# verl/trainer/ppo/core_algos.py:266-330

# Reward åªåœ¨æœ€åä¸€ä¸ª token
# [batch_size, response_length] â†’ åªæœ‰æœ€åä¸€ä¸ªä½ç½®é 0

# GRPO éœ€è¦å¹¿æ’­åˆ°æ‰€æœ‰ LLM token
advantages = advantages.unsqueeze(-1)  # [batch_size, group_size, 1]
advantages = advantages * response_mask  # åªä¿ç•™ LLM token
```

#### ä½œç”¨ 3ï¼šMetrics è®¡ç®—

```python
# è®¡ç®— LLM ç”Ÿæˆçš„å¹³å‡é•¿åº¦
llm_lengths = response_mask.sum(dim=1)  # [batch_size]
avg_llm_length = llm_lengths.float().mean()

# è®¡ç®— tool token æ¯”ä¾‹
tool_ratio = (1 - response_mask).sum() / response_mask.numel()
```

---

## 6. å®Œæ•´è®­ç»ƒæµç¨‹è¿½è¸ª

### 6.1 ç«¯åˆ°ç«¯ç¤ºä¾‹ï¼šGSM8K Tool Agent

è®©æˆ‘ä»¬è¿½è¸ªä¸€ä¸ªå®Œæ•´çš„ batch ä» Rollout åˆ° Training çš„å…¨æµç¨‹ã€‚

#### Step 1: æ•°æ®åŠ è½½

```python
# è®­ç»ƒå¼€å§‹ï¼Œä» Parquet åŠ è½½æ•°æ®
dataset = RLHFDataset.load("~/data/gsm8k/train.parquet")

# Batch size = 2
batch = dataset.sample(2)
# batch.non_tensor_batch["prompt"]:
# [
#   [{"role": "system", ...}, {"role": "user", "content": "é—®é¢˜ 1"}],
#   [{"role": "system", ...}, {"role": "user", "content": "é—®é¢˜ 2"}]
# ]
# batch.non_tensor_batch["agent_name"]: ["tool_agent", "tool_agent"]
# batch.non_tensor_batch["extra_info"]:
# [
#   {"tools_kwargs": {"calc_gsm8k_reward": {...}}},
#   {"tools_kwargs": {"calc_gsm8k_reward": {...}}}
# ]
```

#### Step 2: Rolloutï¼ˆç”Ÿæˆ trajectoriesï¼‰

```python
# AgentLoopManager.generate_sequences()

# åˆ†å‘åˆ° AgentLoopWorker
outputs = await asyncio.gather(
    worker_1.generate_sequences(batch[0]),  # Sample 1
    worker_2.generate_sequences(batch[1]),  # Sample 2
)

# æ¯ä¸ª worker è¿è¡Œ ToolAgentLoop
# Sample 1 çš„ trajectory:
{
    "prompt_ids": [101, ..., 200],  # 100 tokens
    "response_ids": [
        201, ..., 250,  # Turn 1: LLM (50 tokens)
        300, 301, 302,  # Tool response (3 tokens)
        400, ..., 420,  # Turn 2: LLM (21 tokens)
    ],  # 74 tokens
    "response_mask": [
        1, ..., 1,  # 50 ä¸ª 1
        0, 0, 0,    # 3 ä¸ª 0
        1, ..., 1,  # 21 ä¸ª 1
    ],
    "rollout_log_probs": [0.1, 0.2, ..., 0.3],  # 74 ä¸ªå€¼ï¼ˆå¯¹åº” response_idsï¼‰
}
```

#### Step 3: Reward è®¡ç®—

```python
# RewardManager.compute_reward()

# å¯¹äº GSM8Kï¼ŒReward åœ¨ Agent Loop å†…éƒ¨å·²è®¡ç®—
# calc_gsm8k_reward.execute() è¿”å› {"score": 1.0, "correct": True}

# Reward æ”¾ç½®åœ¨æœ€åä¸€ä¸ª token
rm_scores = torch.zeros_like(response_mask, dtype=torch.float32)
response_length = attention_mask[:, prompt_length:].sum(dim=1) - 1
rm_scores[0, response_length[0]] = 1.0  # Sample 1: æ­£ç¡®
rm_scores[1, response_length[1]] = 0.0  # Sample 2: é”™è¯¯

# Shape: [2, 74]
# Sample 1: [0, 0, ..., 0, 1.0, 0, ..., 0]  # æœ€åä¸€ä¸ª LLM token = 1.0
# Sample 2: [0, 0, ..., 0, 0.0, 0, ..., 0]
```

#### Step 4: Reference Log Prob è®¡ç®—

```python
# RefModelWorker.forward_step()

# ä½¿ç”¨ Ref Model é‡æ–°è®¡ç®— log_prob
ref_log_probs = ref_model(input_ids=input_ids, attention_mask=attention_mask)

# Shape: [2, 74]
# è¿™äº› log_prob ç”¨äº KL æƒ©ç½š
```

#### Step 5: Value ä¼°è®¡ï¼ˆä»… PPOï¼‰

å¦‚æœä½¿ç”¨ PPO ç®—æ³•ï¼Œéœ€è¦ Critic Modelï¼š

```python
# CriticModelWorker.forward_step()

values = critic_model(input_ids=input_ids, attention_mask=attention_mask)

# Shape: [2, 74]
# æ¯ä¸ª token çš„ä»·å€¼ä¼°è®¡
```

#### Step 6: Advantage è®¡ç®—

**GRPO ç®—æ³•ï¼š**

```python
# verl/trainer/ppo/core_algos.py:compute_grpo_outcome_advantage

# Step 1: Group samples
# Batch [Sample 1, Sample 2] â†’ Group [Sample 1, Sample 2]
# (å‡è®¾ group_size=2)

# Step 2: KL penalty
kl_penalty = (rollout_log_probs - ref_log_probs).sum(dim=-1)  # [2]
# Sample 1: 0.5
# Sample 2: 0.6

kl_rewards = rm_scores.sum(dim=-1) - beta * kl_penalty
# Sample 1: 1.0 - 0.01 * 0.5 = 0.995
# Sample 2: 0.0 - 0.01 * 0.6 = -0.006

# Step 3: Group baseline
group_mean = kl_rewards.mean()  # (0.995 - 0.006) / 2 = 0.4945

# Step 4: Advantage
advantages = kl_rewards - group_mean
# Sample 1: 0.995 - 0.4945 = 0.5005
# Sample 2: -0.006 - 0.4945 = -0.5005

# Step 5: å¹¿æ’­åˆ°æ‰€æœ‰ token
advantages = advantages.unsqueeze(-1) * response_mask
# Shape: [2, 74]
# Sample 1: [0.5005, 0.5005, ..., 0.5005, 0, 0, 0, 0.5005, ...]  # mask=0 çš„ä½ç½®ä¸º 0
```

**PPO ç®—æ³•ï¼š**

```python
# verl/trainer/ppo/core_algos.py:compute_gae

# GAE é€’å½’è®¡ç®—
advantages = []
gae = 0
for t in reversed(range(response_length)):
    delta = rewards[t] + gamma * values[t+1] - values[t]
    gae = delta + gamma * lam * gae
    advantages.insert(0, gae)

# Shape: [2, 74]
```

#### Step 7: Actor Update

```python
# ActorModelWorker.update_policy()

# å‰å‘ä¼ æ’­
outputs = actor_model(input_ids=input_ids, attention_mask=attention_mask)
logits = outputs.logits  # [2, 174, vocab_size]

# è®¡ç®— new_log_prob
new_log_probs = compute_log_prob(logits, response_ids, response_mask)
# Shape: [2, 74]

# PPO ratio
ratio = torch.exp(new_log_probs - old_log_probs)
# Shape: [2, 74]

# Clipped objective
loss_1 = ratio * advantages
loss_2 = torch.clamp(ratio, 1 - eps, 1 + eps) * advantages
loss = -torch.min(loss_1, loss_2)

# åªå¯¹ LLM token è®¡ç®— loss
loss = (loss * response_mask).sum() / response_mask.sum()

# åå‘ä¼ æ’­
loss.backward()
optimizer.step()
```

#### Step 8: Critic Updateï¼ˆä»… PPOï¼‰

```python
# CriticModelWorker.update_critic()

# è®¡ç®— TD target
returns = advantages + values  # [2, 74]

# Critic loss
new_values = critic_model(input_ids=input_ids, attention_mask=attention_mask)
critic_loss = F.mse_loss(new_values, returns, reduction='none')

# åªå¯¹ LLM token è®¡ç®— loss
critic_loss = (critic_loss * response_mask).sum() / response_mask.sum()

# åå‘ä¼ æ’­
critic_loss.backward()
critic_optimizer.step()
```

### 6.2 Metrics æ”¶é›†å’Œå¯è§†åŒ–

#### AgentLoopWorker æ”¶é›†çš„ Metrics

```python
# verl/experimental/agent_loop/agent_loop.py:125-154

class AgentLoopMetrics(BaseModel):
    generate_sequences: float = 0.0  # LLM ç”Ÿæˆæ€»è€—æ—¶
    tool_calls: float = 0.0          # å·¥å…·è°ƒç”¨æ€»è€—æ—¶
    num_preempted: int = -1          # è¢«æŠ¢å æ¬¡æ•°ï¼ˆvLLMï¼‰
```

**ç¤ºä¾‹è¾“å‡ºï¼š**

```python
metrics = [
    {"generate_sequences": 1.2, "tool_calls": 0.3, "num_preempted": 0},  # Sample 1
    {"generate_sequences": 2.5, "tool_calls": 0.5, "num_preempted": 1},  # Sample 2
]

# AgentLoopManager æ±‡æ€»
timing = {
    "agent_loop/generate_sequences/min": 1.2,
    "agent_loop/generate_sequences/max": 2.5,
    "agent_loop/generate_sequences/mean": 1.85,
    "agent_loop/tool_calls/min": 0.3,
    "agent_loop/tool_calls/max": 0.5,
    "agent_loop/tool_calls/mean": 0.4,
    "agent_loop/num_preempted/mean": 0.5,
    "agent_loop/slowest/generate_sequences": 2.5,  # æœ€æ…¢æ ·æœ¬
    "agent_loop/slowest/tool_calls": 0.5,
    "agent_loop/slowest/prompt_length": 100,
    "agent_loop/slowest/response_length": 74,
}
```

#### RayPPOTrainer æ”¶é›†çš„ Metrics

```python
# verl/trainer/ppo/ray_trainer.py ä¸­çš„ fit()

metrics = {
    # Rollout
    "throughput/rollout": batch_size / rollout_time,
    "time/rollout": rollout_time,

    # Reward
    "reward/mean": rewards.mean().item(),
    "reward/max": rewards.max().item(),
    "reward/min": rewards.min().item(),

    # Advantage
    "advantage/mean": advantages.mean().item(),
    "advantage/std": advantages.std().item(),

    # Actor
    "policy/approx_kl": approx_kl.mean().item(),
    "policy/ratio/mean": ratio.mean().item(),
    "policy/ratio/max": ratio.max().item(),
    "policy/clipfrac": (torch.abs(ratio - 1) > eps).float().mean().item(),
    "loss/actor": actor_loss.item(),

    # Criticï¼ˆä»… PPOï¼‰
    "loss/critic": critic_loss.item(),

    # Agent Loopï¼ˆä» AgentLoopManager ä¼ é€’ï¼‰
    **agent_loop_timing,
}

# TensorBoard è®°å½•
logger.log_metrics(metrics, step=global_step)
```

#### TensorBoard å¯è§†åŒ–

```bash
tensorboard --logdir ~/experiments/gsm8k_tool_agent/logs
```

**å…³é”®æŒ‡æ ‡ï¼š**

1. **Reward Curve**
   - `reward/mean`: å¹³å‡ Rewardï¼ˆåº”è¯¥é€æ¸ä¸Šå‡ï¼‰
   - `reward/max`: æœ€å¤§ Rewardï¼ˆ1.0 è¡¨ç¤ºæœ‰æ ·æœ¬å®Œå…¨æ­£ç¡®ï¼‰

2. **Policy Metrics**
   - `policy/approx_kl`: è¿‘ä¼¼ KL æ•£åº¦ï¼ˆåº”è¯¥ < 0.1ï¼‰
   - `policy/ratio/mean`: PPO ratio å‡å€¼ï¼ˆåº”è¯¥æ¥è¿‘ 1.0ï¼‰
   - `policy/clipfrac`: Clipping æ¯”ä¾‹ï¼ˆ10-30% æ­£å¸¸ï¼‰

3. **Agent Loop Metrics**
   - `agent_loop/generate_sequences/mean`: å¹³å‡ç”Ÿæˆè€—æ—¶
   - `agent_loop/tool_calls/mean`: å¹³å‡å·¥å…·è°ƒç”¨è€—æ—¶
   - `agent_loop/num_preempted/mean`: å¹³å‡æŠ¢å æ¬¡æ•°

---

## 7. è°ƒè¯•æŠ€å·§

### 7.1 å¸¸è§é—®é¢˜è¯Šæ–­

#### é—®é¢˜ 1ï¼šReward å§‹ç»ˆä¸º 0

**ç—‡çŠ¶ï¼š**
```python
reward/mean: 0.0
reward/max: 0.0
reward/min: 0.0
```

**å¯èƒ½åŸå› ï¼š**

1. **å·¥å…·æœªæ­£ç¡®è°ƒç”¨**
   ```python
   # æ£€æŸ¥ LLM è¾“å‡º
   print(response_text)
   # åº”è¯¥çœ‹åˆ°: <tool_call>{"name": "calc_gsm8k_reward", ...}</tool_call>

   # å¦‚æœæ²¡æœ‰ï¼Œå¯èƒ½æ˜¯ï¼š
   # - System prompt æ²¡æœ‰æŒ‡ç¤ºä½¿ç”¨å·¥å…·
   # - æ¨¡å‹æœªç»è¿‡å·¥å…·è°ƒç”¨è®­ç»ƒ
   ```

2. **ç­”æ¡ˆæ ¼å¼é”™è¯¯**
   ```python
   # æ£€æŸ¥ç­”æ¡ˆæå–
   from verl.utils.reward_score.gsm8k import extract_solution

   solution_str = "The answer is 18"  # âŒ æ²¡æœ‰ #### æ ¼å¼
   answer = extract_solution(solution_str, method="strict")
   print(answer)  # None â†’ Reward = 0

   solution_str = "#### 18"  # âœ… æ­£ç¡®æ ¼å¼
   answer = extract_solution(solution_str, method="strict")
   print(answer)  # "18" â†’ Reward = 1.0ï¼ˆå¦‚æœæ­£ç¡®ï¼‰
   ```

3. **ground_truth ä¸åŒ¹é…**
   ```python
   # æ£€æŸ¥æ•°æ®
   print(batch.non_tensor_batch["extra_info"][0])
   # {"tools_kwargs": {"calc_gsm8k_reward": {"create_kwargs": {"ground_truth": "18"}}}}

   # æ£€æŸ¥æ¨¡å‹è¾“å‡º
   print(extracted_answer)  # "18.0" â‰  "18" â†’ Reward = 0
   # è§£å†³ï¼šç»Ÿä¸€æ ¼å¼ï¼ˆå»é™¤å°æ•°ç‚¹ï¼‰
   ```

**è°ƒè¯•è„šæœ¬ï¼š**

```python
# debug_reward.py
import re
from verl.utils.reward_score.gsm8k import compute_score, extract_solution

# æµ‹è¯•ç­”æ¡ˆæå–
test_cases = [
    "The answer is #### 18",  # âœ… æ­£ç¡®
    "#### 18",                # âœ… æ­£ç¡®
    "The answer is 18",       # âŒ æ— æ ¼å¼
    "Let's calculate: #### 18.0",  # âš ï¸ 18.0 vs 18
]

ground_truth = "18"

for case in test_cases:
    answer = extract_solution(case, method="strict")
    score = compute_score(case, ground_truth, method="strict")
    print(f"Input: {case!r}")
    print(f"  Extracted: {answer}")
    print(f"  Score: {score}\n")
```

#### é—®é¢˜ 2ï¼šPPO ratio çˆ†ç‚¸

**ç—‡çŠ¶ï¼š**
```python
policy/ratio/mean: 5.2  # âŒ åº”è¯¥æ¥è¿‘ 1.0
policy/ratio/max: 20.3
policy/clipfrac: 0.85    # âŒ åº”è¯¥ < 0.5
```

**å¯èƒ½åŸå› ï¼š**

1. **old_log_prob å’Œ new_log_prob ä¸å¯¹é½**
   ```python
   # æ£€æŸ¥ token ids ä¸€è‡´æ€§
   # rollout_input_ids vs training_input_ids

   assert torch.equal(
       batch["input_ids"],
       torch.cat([batch["prompts"], batch["responses"]], dim=1)
   ), "Input IDs mismatch!"
   ```

2. **å­¦ä¹ ç‡è¿‡å¤§**
   ```python
   # é™ä½å­¦ä¹ ç‡
   # config/actor.yaml
   actor:
     optim:
       lr: 1e-6  # ä» 1e-5 é™ä½åˆ° 1e-6
   ```

3. **Clipping é˜ˆå€¼è¿‡å¤§**
   ```python
   # config/ppo_trainer.yaml
   algorithm:
     clip_ratio: 0.1  # ä» 0.2 é™ä½åˆ° 0.1
   ```

**è°ƒè¯•è„šæœ¬ï¼š**

```python
# debug_ppo_ratio.py

# æ£€æŸ¥ log_prob åˆ†å¸ƒ
import torch

old_log_probs = batch["rollout_log_probs"]  # Rollout æ—¶çš„
new_log_probs = compute_log_prob(logits, response_ids, response_mask)

# Ratio
ratio = torch.exp(new_log_probs - old_log_probs)

print(f"old_log_probs: mean={old_log_probs.mean():.3f}, std={old_log_probs.std():.3f}")
print(f"new_log_probs: mean={new_log_probs.mean():.3f}, std={new_log_probs.std():.3f}")
print(f"ratio: mean={ratio.mean():.3f}, max={ratio.max():.3f}")

# æ£€æŸ¥å¼‚å¸¸å€¼
outliers = (ratio > 2.0) | (ratio < 0.5)
if outliers.any():
    print(f"Found {outliers.sum()} outliers!")
    print(f"Positions: {outliers.nonzero()}")
```

#### é—®é¢˜ 3ï¼šTool å“åº” Token å‚ä¸äº† Loss è®¡ç®—

**ç—‡çŠ¶ï¼š**
è®­ç»ƒä¸ç¨³å®šï¼Œloss éœ‡è¡ã€‚

**åŸå› ï¼š**
`response_mask` é”™è¯¯ï¼Œå·¥å…·å“åº”è¢«å½“ä½œ LLM ç”Ÿæˆè®¡ç®—äº† lossã€‚

**æ£€æŸ¥ï¼š**

```python
# debug_response_mask.py

# æ£€æŸ¥ response_mask
response_ids = batch["responses"][0]  # ç¬¬ä¸€ä¸ªæ ·æœ¬
response_mask = batch["response_mask"][0]

print("Response IDs:", response_ids)
print("Response Mask:", response_mask)

# è§£ç æŸ¥çœ‹
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B")
for i, (token_id, mask) in enumerate(zip(response_ids, response_mask)):
    if token_id == 0:  # padding
        break
    token_text = tokenizer.decode([token_id])
    print(f"{i:3d}: {token_id:5d} mask={mask} {token_text!r}")

# é¢„æœŸï¼šå·¥å…·å“åº”éƒ¨åˆ† mask=0
# Turn 1 LLM: mask=1
# Tool response: mask=0  # â† é‡ç‚¹æ£€æŸ¥è¿™éƒ¨åˆ†
# Turn 2 LLM: mask=1
```

**ä¿®å¤ï¼š**

```python
# agent_loop.py ä¸­ç¡®ä¿æ­£ç¡®è®¾ç½® mask

# LLM ç”Ÿæˆ
all_response_ids.extend(response_ids)
all_response_mask.extend([1] * len(response_ids))  # âœ… LLM = 1

# å·¥å…·å“åº”
tool_result_ids = tokenizer.encode(tool_result_text)
all_response_ids.extend(tool_result_ids)
all_response_mask.extend([0] * len(tool_result_ids))  # âœ… Tool = 0
```

### 7.2 Tracing å’Œ Logging

#### RolloutTraceConfig

verl æä¾›äº†å®Œæ•´çš„ Trace ç³»ç»Ÿç”¨äºè°ƒè¯•ã€‚

```python
# config/ppo_trainer.yaml
actor_rollout_ref:
  rollout:
    trace:
      backend: "mlflow"  # æˆ– "simple"
      token2text: true   # å°† token ids è½¬æ¢ä¸ºæ–‡æœ¬
      max_samples_per_step_per_worker: 5  # æ¯ä¸ª step åª trace 5 ä¸ªæ ·æœ¬
```

**ä½¿ç”¨ MLflow Traceï¼š**

```python
# verl/utils/rollout_trace.py

# åœ¨ Agent Loop ä¸­è‡ªåŠ¨è®°å½•
with rollout_trace_attr(
    step=global_step,
    sample_index=i,
    rollout_n=0,
    validate=False,
    name="agent_loop",
    trace=True,
):
    # æ‰€æœ‰æ“ä½œéƒ½ä¼šè¢«è®°å½•
    output = await agent_loop.run(sampling_params, **kwargs)
```

**æŸ¥çœ‹ Traceï¼š**

```bash
mlflow ui --backend-store-uri ~/experiments/gsm8k_tool_agent/mlruns
```

åœ¨ MLflow UI ä¸­å¯ä»¥çœ‹åˆ°ï¼š
- æ¯ä¸ªæ ·æœ¬çš„å®Œæ•´ trajectory
- æ¯æ¬¡å·¥å…·è°ƒç”¨çš„è¾“å…¥è¾“å‡º
- æ¯è½® LLM ç”Ÿæˆçš„è€—æ—¶

#### è‡ªå®šä¹‰ Logging

```python
# åœ¨ Agent Loop ä¸­æ·»åŠ  logging

import logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

class ToolAgentLoop(AgentLoopBase):
    async def run(self, sampling_params, **kwargs):
        logger.info(f"Starting Agent Loop for sample {kwargs.get('index')}")

        for turn in range(max_turns):
            logger.debug(f"Turn {turn}: Generating...")
            response = await self.server_manager.generate(...)
            logger.debug(f"Turn {turn}: Generated {len(response.output_token_ids)} tokens")

            tool_calls = self._parse_tool_calls(response_text)
            if tool_calls:
                logger.info(f"Turn {turn}: Calling {len(tool_calls)} tools")
                for tool_call in tool_calls:
                    logger.debug(f"  Tool: {tool_call['name']}, Args: {tool_call['arguments']}")
                    tool_result = ...
                    logger.debug(f"  Result: {tool_result}")

        logger.info(f"Finished Agent Loop: {len(all_response_ids)} total response tokens")
        return AgentLoopOutput(...)
```

### 7.3 å•å…ƒæµ‹è¯•

**æµ‹è¯• Tool é€»è¾‘ï¼š**

```python
# tests/test_gsm8k_tool.py
import pytest
from verl.utils.reward_score.gsm8k import compute_score, extract_solution

def test_extract_solution_strict():
    assert extract_solution("#### 18", method="strict") == "18"
    assert extract_solution("The answer is #### 18", method="strict") == "18"
    assert extract_solution("No answer here", method="strict") is None

def test_compute_score():
    assert compute_score("#### 18", ground_truth="18", method="strict") == 1.0
    assert compute_score("#### 20", ground_truth="18", method="strict") == 0.0
    assert compute_score("No answer", ground_truth="18", method="strict") == 0.0
```

**æµ‹è¯• Agent Loopï¼š**

```python
# tests/test_agent_loop.py
import pytest
import asyncio
from unittest.mock import AsyncMock, MagicMock

@pytest.mark.asyncio
async def test_tool_agent_loop():
    # Mock server
    mock_server = AsyncMock()
    mock_server.generate.return_value = TokenOutput(
        output_token_ids=[201, 202, 203],
        logprobs=[0.1, 0.2, 0.3],
        finish_reason="stop"
    )

    # Mock tools
    mock_tool = MagicMock()
    mock_tool.execute.return_value = {"score": 1.0, "correct": True}

    # Create Agent Loop
    agent_loop = ToolAgentLoop(
        server_manager=mock_server,
        tools={"calc_gsm8k_reward": mock_tool},
        ...
    )

    # Run
    output = await agent_loop.run(
        sampling_params={},
        prompt=[...],
        ...
    )

    # Assertions
    assert len(output.response_ids) > 0
    assert len(output.response_mask) == len(output.response_ids)
    assert 0 in output.response_mask  # Tool response exists
    assert 1 in output.response_mask  # LLM generation exists
```

---

## 8. æœ€ä½³å®è·µ

### 8.1 System Prompt è®¾è®¡

**å¥½çš„ System Promptï¼š**

```python
# examples/data_preprocess/gsm8k_tool_agent_loop.py:76-82
{
    "role": "system",
    "content": (
        "You are a math expert. You are given a question and you need to solve it step by step. "
        # âœ… æ˜ç¡®æŒ‡ç¤ºï¼šå…ˆæ¨ç†
        "Reasoning step by step before any tool call. "
        # âœ… æ˜ç¡®æŒ‡ç¤ºï¼šä½•æ—¶ä½¿ç”¨å·¥å…·
        "You should use the `calc_gsm8k_reward` tool after step by step solving the question, "
        # âœ… æ˜ç¡®æŒ‡ç¤ºï¼šå¯ä»¥å¤šæ¬¡è°ƒç”¨
        "before generate final answer at least once and refine your answer if necessary. "
        # âœ… æ˜ç¡®æŒ‡ç¤ºï¼šè¾“å‡ºæ ¼å¼
        "Put your final answer in the format of `#### <answer>`."
    ),
}
```

**å…³é”®è¦ç´ ï¼š**
1. **è§’è‰²å®šä¹‰**ï¼š"You are a math expert"
2. **ä»»åŠ¡æè¿°**ï¼š"solve it step by step"
3. **å·¥å…·ä½¿ç”¨æ—¶æœº**ï¼š"after step by step solving"
4. **è¾“å‡ºæ ¼å¼**ï¼š"`#### <answer>`"
5. **è¿­ä»£ä¼˜åŒ–**ï¼š"refine your answer if necessary"

**é¿å…çš„é”™è¯¯ï¼š**

```python
# âŒ é”™è¯¯ç¤ºä¾‹ 1ï¼šè¿‡äºæ¨¡ç³Š
"You are a helpful assistant. Answer the question."

# âŒ é”™è¯¯ç¤ºä¾‹ 2ï¼šæ²¡æœ‰æŒ‡ç¤ºå·¥å…·ä½¿ç”¨
"You are a math expert. Solve the problem step by step."

# âŒ é”™è¯¯ç¤ºä¾‹ 3ï¼šæ²¡æœ‰è¾“å‡ºæ ¼å¼è¦æ±‚
"Use the tool to check your answer."
```

### 8.2 å·¥å…·è®¾è®¡åŸåˆ™

#### åŸåˆ™ 1ï¼šå·¥å…·åº”è¯¥æ˜¯ç¡®å®šæ€§çš„

```python
# âœ… å¥½çš„å·¥å…·
def calc_gsm8k_reward(solution: str, ground_truth: str) -> dict:
    answer = extract_solution(solution)
    return {
        "score": 1.0 if answer == ground_truth else 0.0,
        "extracted_answer": answer,
        "correct": answer == ground_truth
    }

# âŒ ä¸å¥½çš„å·¥å…·ï¼ˆéç¡®å®šæ€§ï¼‰
def calc_reward_with_llm(solution: str) -> dict:
    # ä½¿ç”¨å¦ä¸€ä¸ª LLM è¯„åˆ†ï¼ˆå¯èƒ½æ¯æ¬¡ä¸åŒï¼‰
    score = llm_judge(solution)  # â† éç¡®å®šæ€§
    return {"score": score}
```

#### åŸåˆ™ 2ï¼šå·¥å…·åº”è¯¥æä¾›è¯¦ç»†åé¦ˆ

```python
# âœ… å¥½çš„å·¥å…·
{
    "score": 0.0,
    "extracted_answer": "20",
    "correct": False,
    "ground_truth": "18",
    "error_type": "calculation_error"
}

# âŒ ä¸å¥½çš„å·¥å…·
{
    "score": 0.0  # æ²¡æœ‰å‘Šè¯‰æ¨¡å‹å“ªé‡Œé”™äº†
}
```

#### åŸåˆ™ 3ï¼šå·¥å…·åº”è¯¥å¿«é€Ÿæ‰§è¡Œ

```python
# âœ… å¥½çš„å·¥å…·ï¼ˆ< 100msï¼‰
def extract_solution(solution_str):
    return re.findall("#### (\\-?[0-9\\.\\,]+)", solution_str)

# âŒ ä¸å¥½çš„å·¥å…·ï¼ˆ> 1sï¼‰
def complex_verification(solution_str):
    # è°ƒç”¨å¤–éƒ¨ API
    result = requests.post("https://api.example.com/verify", ...)
    return result.json()
```

### 8.3 å¤šè½®å¯¹è¯ç­–ç•¥

#### ç­–ç•¥ 1ï¼šé™åˆ¶æœ€å¤§è½®æ•°

```python
max_turns = 5  # é˜²æ­¢æ— é™å¾ªç¯

for turn in range(max_turns):
    response = await server.generate(...)
    tool_calls = parse_tool_calls(response)

    if not tool_calls:
        break  # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç»“æŸ

    # æ‰§è¡Œå·¥å…·...
```

#### ç­–ç•¥ 2ï¼šEarly Stopping

```python
for turn in range(max_turns):
    response = await server.generate(...)

    # æ£€æŸ¥æ˜¯å¦å·²ç»ç”Ÿæˆäº†æœ€ç»ˆç­”æ¡ˆ
    if "####" in response_text:
        # å·²æœ‰æœ€ç»ˆç­”æ¡ˆï¼Œå¯ä»¥æå‰ç»“æŸ
        break

    tool_calls = parse_tool_calls(response)
    ...
```

#### ç­–ç•¥ 3ï¼šå·¥å…·è°ƒç”¨é¢„ç®—

```python
max_tool_calls = 3  # æœ€å¤šè°ƒç”¨ 3 æ¬¡å·¥å…·
tool_call_count = 0

for turn in range(max_turns):
    response = await server.generate(...)
    tool_calls = parse_tool_calls(response)

    if tool_calls:
        tool_call_count += len(tool_calls)
        if tool_call_count > max_tool_calls:
            logger.warning("Exceeded tool call budget")
            break
    ...
```

### 8.4 Reward Shaping for Agent RL

#### æŠ€å·§ 1ï¼šä¸­é—´æ­¥éª¤å¥–åŠ±

```python
def calc_gsm8k_reward_with_steps(solution: str, ground_truth: str):
    score = 0.0

    # åŸºç¡€åˆ†ï¼šç­”æ¡ˆæ­£ç¡®
    answer = extract_solution(solution)
    if answer == ground_truth:
        score += 1.0

    # é¢å¤–åˆ†ï¼šä½¿ç”¨äº†å·¥å…·
    if "<tool_call>" in solution:
        score += 0.1

    # é¢å¤–åˆ†ï¼šæ¨ç†æ­¥éª¤æ•°é‡
    steps = solution.count("Step")
    score += min(steps * 0.05, 0.3)  # æœ€å¤š +0.3

    return {"score": score}
```

#### æŠ€å·§ 2ï¼šæ ¼å¼åŒ–å¥–åŠ±

```python
def calc_reward_with_format(solution: str, ground_truth: str):
    # ç­”æ¡ˆæ­£ç¡®ï¼š1.0
    # æ ¼å¼æ­£ç¡®ä½†ç­”æ¡ˆé”™è¯¯ï¼š0.2
    # æ ¼å¼é”™è¯¯ï¼š0.0
    return compute_score(
        solution,
        ground_truth,
        method="strict",
        format_score=0.2,  # â† æ ¼å¼åˆ†
        score=1.0
    )
```

#### æŠ€å·§ 3ï¼šé•¿åº¦æƒ©ç½š

```python
def calc_reward_with_length_penalty(solution: str, ground_truth: str, max_length=500):
    base_score = compute_score(solution, ground_truth)

    # è¿‡é•¿æƒ©ç½š
    if len(solution) > max_length:
        penalty = (len(solution) - max_length) / 1000
        base_score -= penalty

    return {"score": max(0.0, base_score)}
```

### 8.5 è¶…å‚æ•°è°ƒä¼˜

#### å…³é”®è¶…å‚æ•°

| å‚æ•° | ä½œç”¨ | æ¨èå€¼ï¼ˆGSM8Kï¼‰| è°ƒä¼˜å»ºè®® |
|------|------|----------------|----------|
| `learning_rate` | Actor å­¦ä¹ ç‡ | 1e-6 ~ 1e-5 | ä»å°å¼€å§‹ï¼ˆ1e-6ï¼‰ |
| `clip_ratio` | PPO Clipping | 0.1 ~ 0.2 | GRPO å¯ä»¥æ›´å¤§ï¼ˆ0.3ï¼‰ |
| `beta` | KL æƒ©ç½šç³»æ•° | 0.01 ~ 0.05 | æ ¹æ® approx_kl è°ƒæ•´ |
| `gamma` | Discount factor | 1.0 | Episodic ä»»åŠ¡ç”¨ 1.0 |
| `lam` | GAE lambda | 0.95 | PPO ä¸“ç”¨ |
| `batch_size` | Batch å¤§å° | 128 ~ 512 | è¶Šå¤§è¶Šç¨³å®š |
| `max_turns` | æœ€å¤§è½®æ•° | 3 ~ 5 | é¿å…è¿‡é•¿ |

#### è°ƒä¼˜æµç¨‹

```
1. å›ºå®šå…¶ä»–å‚æ•°ï¼Œè°ƒ learning_rate
   - è§‚å¯Ÿ policy/approx_kl
   - ç›®æ ‡ï¼šapprox_kl < 0.1

2. è°ƒæ•´ clip_ratio
   - è§‚å¯Ÿ policy/clipfrac
   - ç›®æ ‡ï¼šclipfrac = 0.1 ~ 0.3

3. è°ƒæ•´ betaï¼ˆKL æƒ©ç½šï¼‰
   - è§‚å¯Ÿ reward/mean
   - ç›®æ ‡ï¼šå¹³è¡¡ reward å’Œ KL

4. å¢å¤§ batch_sizeï¼ˆå¦‚æœèµ„æºå…è®¸ï¼‰
   - æå‡ç¨³å®šæ€§
```

---

## 9. æ€»ç»“

### 9.1 æ ¸å¿ƒè¦ç‚¹å›é¡¾

1. **Agent Loop æ¶æ„**
   - AsyncLLMServerManagerï¼šè´Ÿè½½å‡è¡¡ + Sticky Session
   - AgentLoopWorkerï¼šå¹¶å‘æ‰§è¡Œå¤šä¸ª Agent Loop
   - AgentLoopManagerï¼šåè°ƒ Workers å’Œ LLM Servers

2. **å·¥å…·è°ƒç”¨æœºåˆ¶**
   - å·¥å…·ç”Ÿå‘½å‘¨æœŸï¼šcreate â†’ execute â†’ calc_reward â†’ release
   - response_mask åŒºåˆ† LLM token (1) å’Œ Tool token (0)
   - å·¥å…·å“åº”ä¸å‚ä¸ loss è®¡ç®—

3. **å¤šè½®å¯¹è¯**
   - Token-based API ä¿è¯ trajectory ä¸€è‡´æ€§
   - Sticky Session æå‡ KV Cache å‘½ä¸­ç‡
   - Chat history å¯ä»¥ Server ç«¯æˆ– Client ç«¯ç®¡ç†

4. **è®­ç»ƒæµç¨‹**
   - Rollout â†’ Reward â†’ Ref â†’ Value â†’ Advantage â†’ Actor Update â†’ Critic Update
   - GRPOï¼šæ—  Criticï¼ŒGroup Baseline
   - PPOï¼šæœ‰ Criticï¼ŒGAE Advantage

5. **è°ƒè¯•å’Œæœ€ä½³å®è·µ**
   - æ£€æŸ¥ response_mask æ­£ç¡®æ€§
   - ä½¿ç”¨ Trace ç³»ç»Ÿè°ƒè¯•
   - System Prompt æ˜ç¡®æŒ‡ç¤ºå·¥å…·ä½¿ç”¨
   - Reward Shaping æå‡è®­ç»ƒæ•ˆæœ

### 9.2 è¿›é˜¶æ–¹å‘

1. **è‡ªå®šä¹‰ Agent Loop**
   - å®ç°å¤æ‚çš„å¤šå·¥å…·è°ƒç”¨
   - æ”¯æŒå¹¶è¡Œå·¥å…·æ‰§è¡Œ
   - é›†æˆå¤–éƒ¨ç¯å¢ƒï¼ˆå¦‚ä»£ç æ‰§è¡Œæ²™ç®±ï¼‰

2. **å·¥å…·å­¦ä¹ **
   - Few-shot å·¥å…·ä½¿ç”¨ç¤ºä¾‹
   - å·¥å…·é€‰æ‹©ç­–ç•¥ä¼˜åŒ–
   - å·¥å…·ç»„åˆä¼˜åŒ–

3. **é«˜çº§ Reward**
   - è¿‡ç¨‹å¥–åŠ±ï¼ˆProcess Reward Modelï¼‰
   - å¯¹æ¯”å­¦ä¹ ï¼ˆContrastive Learningï¼‰
   - è‡ªæˆ‘ä¿®æ­£å¥–åŠ±

4. **åˆ†å¸ƒå¼ä¼˜åŒ–**
   - å¤šèŠ‚ç‚¹ Agent Loop
   - å¼‚æ­¥ Reward è®¡ç®—
   - Pipeline å¹¶è¡Œ

---

**ğŸ‰ æ­å–œï¼ä½ å·²ç»å®Œæˆäº† Agent Loop çš„æ·±åº¦å­¦ä¹ ï¼**

ä¸‹ä¸€æ­¥å»ºè®®ï¼š
1. å®è·µï¼šè¿è¡Œ GSM8K Tool Agent è®­ç»ƒ
2. å®éªŒï¼šä¿®æ”¹ System Prompt è§‚å¯Ÿæ•ˆæœå˜åŒ–
3. æ‰©å±•ï¼šå®ç°è‡ªå·±çš„ Agent Loop å’Œå·¥å…·

ç»§ç»­åŠ æ²¹ï¼ğŸš€
