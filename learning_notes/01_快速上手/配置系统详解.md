# Hydra é…ç½®ç³»ç»Ÿè¯¦è§£

> æ·±å…¥ç†è§£ verl çš„ Hydra é…ç½®ç³»ç»Ÿï¼ŒæŒæ¡å‚æ•°é…ç½®å’Œè°ƒä¼˜

---

## ğŸ“‹ æœ¬æ–‡å†…å®¹

1. Hydra é…ç½®ç³»ç»Ÿæ¦‚è§ˆ
2. é…ç½®æ–‡ä»¶ç»“æ„
3. æ ¸å¿ƒé…ç½®é¡¹è¯¦è§£
4. é…ç½®è¦†ç›–å’Œç»„åˆ
5. å®é™…æ¡ˆä¾‹è§£æ
6. é…ç½®è°ƒä¼˜æŠ€å·§

---

## 1. Hydra é…ç½®ç³»ç»Ÿæ¦‚è§ˆ

### 1.1 ä¸ºä»€ä¹ˆä½¿ç”¨ Hydraï¼Ÿ

**ä¼ ç»Ÿæ–¹å¼çš„é—®é¢˜ï¼š**
```python
# ä¼ ç»Ÿæ–¹å¼ï¼šargparseï¼ˆç¹çï¼‰
parser.add_argument('--actor_lr', type=float, default=1e-6)
parser.add_argument('--critic_lr', type=float, default=1e-5)
parser.add_argument('--rollout_temperature', type=float, default=0.6)
# ... å‡ ç™¾ä¸ªå‚æ•°
```

**Hydra çš„ä¼˜åŠ¿ï¼š**
- âœ… é…ç½®æ–‡ä»¶åŒ–ï¼ˆYAMLï¼‰
- âœ… å±‚æ¬¡åŒ–ç»„ç»‡
- âœ… æ¨¡å—åŒ–ç»„åˆï¼ˆdefaultsï¼‰
- âœ… å‘½ä»¤è¡Œè¦†ç›–
- âœ… é…ç½®éªŒè¯

### 1.2 é…ç½®æ–‡ä»¶ä½ç½®

```
verl/trainer/config/
â”œâ”€â”€ ppo_trainer.yaml              # PPO/GRPO ä¸»é…ç½®
â”œâ”€â”€ ppo_megatron_trainer.yaml     # Megatron PPO é…ç½®
â”œâ”€â”€ sft_trainer.yaml              # SFT é…ç½®
â”‚
â”œâ”€â”€ actor/                        # Actor ç»„ä»¶é…ç½®
â”‚   â”œâ”€â”€ dp_actor.yaml
â”‚   â””â”€â”€ megatron_actor.yaml
â”‚
â”œâ”€â”€ critic/                       # Critic ç»„ä»¶é…ç½®
â”‚   â””â”€â”€ dp_critic.yaml
â”‚
â”œâ”€â”€ rollout/                      # Rollout ç»„ä»¶é…ç½®
â”‚   â”œâ”€â”€ rollout.yaml
â”‚   â””â”€â”€ megatron_rollout.yaml
â”‚
â”œâ”€â”€ data/                         # æ•°æ®é…ç½®
â”‚   â””â”€â”€ legacy_data.yaml
â”‚
â”œâ”€â”€ model/                        # æ¨¡å‹é…ç½®
â”‚   â”œâ”€â”€ hf_model.yaml
â”‚   â””â”€â”€ megatron_model.yaml
â”‚
â””â”€â”€ algorithm/                    # ç®—æ³•é…ç½®
    â””â”€â”€ rollout_correction.yaml
```

---

## 2. é…ç½®æ–‡ä»¶ç»“æ„

### 2.1 ä¸»é…ç½®æ–‡ä»¶ï¼ˆppo_trainer.yamlï¼‰

**æ–‡ä»¶ä½ç½®ï¼š** `verl/trainer/config/ppo_trainer.yaml`

```yaml
# ========== æ ¸å¿ƒéƒ¨åˆ† 1: defaultsï¼ˆé…ç½®ç»„åˆï¼‰==========
defaults:
  # è¯­æ³•ï¼š- <folder>@<target_path>: <yaml_file>
  # å«ä¹‰ï¼šä» config/<folder>/<yaml_file>.yaml åŠ è½½é…ç½®ï¼Œ
  #      æ”¾åˆ° config.<target_path>

  - actor@actor_rollout_ref.actor: dp_actor
    # ä» config/actor/dp_actor.yaml åŠ è½½
    # åˆå¹¶åˆ° config.actor_rollout_ref.actor

  - rollout@actor_rollout_ref.rollout: rollout
    # ä» config/rollout/rollout.yaml åŠ è½½

  - data@data: legacy_data
    # ä» config/data/legacy_data.yaml åŠ è½½

  - critic@critic: dp_critic
    # ä» config/critic/dp_critic.yaml åŠ è½½

  - _self_
    # è¡¨ç¤ºå½“å‰æ–‡ä»¶çš„é…ç½®æœ€ååº”ç”¨ï¼ˆè¦†ç›–ä¼˜å…ˆçº§æœ€é«˜ï¼‰

# ========== æ ¸å¿ƒéƒ¨åˆ† 2: actor_rollout_refï¼ˆè®­ç»ƒ+æ¨ç†ï¼‰==========
actor_rollout_ref:
  hybrid_engine: true  # è®­ç»ƒå’Œæ¨ç†å…±äº« GPU

  # è¿™é‡Œåªå®šä¹‰é¡¶å±‚é…ç½®ï¼Œå­é…ç½®ä» defaults åŠ è½½ï¼š
  # - actor: ä» config/actor/dp_actor.yaml
  # - rollout: ä» config/rollout/rollout.yaml
  # - ref: ä» config/ref/dp_ref.yaml
  # - model: ä» config/model/hf_model.yaml

# ========== æ ¸å¿ƒéƒ¨åˆ† 3: algorithmï¼ˆç®—æ³•é…ç½®ï¼‰==========
algorithm:
  gamma: 1.0           # æŠ˜æ‰£å› å­
  lam: 1.0             # GAE lambda
  adv_estimator: gae   # ä¼˜åŠ¿ä¼°è®¡å™¨ï¼šgae, grpo, rloo, ...

  kl_ctrl:
    type: fixed        # KL æ§åˆ¶ï¼šfixed æˆ– adaptive
    kl_coef: 0.001     # KL ç³»æ•°

# ========== æ ¸å¿ƒéƒ¨åˆ† 4: trainerï¼ˆè®­ç»ƒå™¨é…ç½®ï¼‰==========
trainer:
  total_epochs: 30
  project_name: verl_examples
  logger: ["console", "wandb"]

  n_gpus_per_node: 8   # æ¯ä¸ªèŠ‚ç‚¹ GPU æ•°
  nnodes: 1            # èŠ‚ç‚¹æ•°
```

### 2.2 é…ç½®åŠ è½½é¡ºåº

```
ç¬¬ 1 æ­¥ï¼šåŠ è½½ defaults ä¸­çš„æ‰€æœ‰å­é…ç½®
  â†“
  actor/dp_actor.yaml â†’ actor_rollout_ref.actor
  rollout/rollout.yaml â†’ actor_rollout_ref.rollout
  data/legacy_data.yaml â†’ data
  critic/dp_critic.yaml â†’ critic
  ...

ç¬¬ 2 æ­¥ï¼šåŠ è½½ _self_ï¼ˆå½“å‰æ–‡ä»¶ï¼‰
  â†“
  è¦†ç›–æˆ–æ–°å¢å­—æ®µ

ç¬¬ 3 æ­¥ï¼šå‘½ä»¤è¡Œå‚æ•°è¦†ç›–
  â†“
  python main_ppo.py actor_rollout_ref.actor.optim.lr=1e-6
```

---

## 3. æ ¸å¿ƒé…ç½®é¡¹è¯¦è§£

### 3.1 Data é…ç½®

**æ–‡ä»¶ï¼š** `verl/trainer/config/data/legacy_data.yaml`

```yaml
# æ•°æ®é…ç½®
data:
  # ========== æ•°æ®æ–‡ä»¶ ==========
  train_files:
    - ~/data/gsm8k/train.parquet  # è®­ç»ƒæ•°æ®è·¯å¾„åˆ—è¡¨

  val_files:
    - ~/data/gsm8k/test.parquet   # éªŒè¯æ•°æ®è·¯å¾„åˆ—è¡¨

  # ========== Batch Size ==========
  train_batch_size: 1024          # å…¨å±€è®­ç»ƒ batch sizeï¼ˆprompts æ•°é‡ï¼‰
  # å®é™…å“åº”æ•° = train_batch_size * rollout.nï¼ˆGRPO ä¸­ n > 1ï¼‰

  val_batch_size: 1312            # éªŒè¯ batch size

  # ========== é•¿åº¦é™åˆ¶ ==========
  max_prompt_length: 1024         # Prompt æœ€å¤§é•¿åº¦
  max_response_length: 512        # Response æœ€å¤§é•¿åº¦

  # ========== DataLoader å‚æ•° ==========
  train_shuffle: true             # æ˜¯å¦æ‰“ä¹±è®­ç»ƒæ•°æ®
  val_shuffle: false              # éªŒè¯æ•°æ®ä¸æ‰“ä¹±

  # ========== Prompt Key ==========
  prompt_key: prompt              # Parquet ä¸­ prompt å­—æ®µå
  # å¦‚æœä½ çš„æ•°æ®å­—æ®µåä¸æ˜¯ "prompt"ï¼Œéœ€è¦ä¿®æ”¹è¿™é‡Œ
```

**å®é™…ä½¿ç”¨ï¼š**
```bash
# å‘½ä»¤è¡Œè¦†ç›–
python3 -m verl.trainer.main_ppo \
    data.train_files="['my_data/train.parquet']" \
    data.train_batch_size=512 \
    data.max_response_length=256
```

### 3.2 Actor é…ç½®

**æ–‡ä»¶ï¼š** `verl/trainer/config/actor/dp_actor.yaml`

```yaml
actor:
  # ========== ä¼˜åŒ–å™¨é…ç½® ==========
  optim:
    lr: 1e-6              # å­¦ä¹ ç‡ï¼ˆæœ€é‡è¦çš„è¶…å‚æ•°ï¼ï¼‰
    min_lr: 0.0           # æœ€å°å­¦ä¹ ç‡ï¼ˆlr scheduler ç”¨ï¼‰
    weight_decay: 0.0     # æƒé‡è¡°å‡
    beta: [0.9, 0.95]     # Adam beta1, beta2

  # ========== Scheduler é…ç½® ==========
  lr_scheduler:
    type: cosine          # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šcosine, constant, linear
    warmup_steps: 100     # é¢„çƒ­æ­¥æ•°

  # ========== PPO å‚æ•° ==========
  ppo_mini_batch_size: 256        # PPO mini-batch å¤§å°ï¼ˆå…¨å±€ï¼‰
  ppo_micro_batch_size_per_gpu: 16  # æ¯ä¸ª GPU çš„ micro-batch

  ppo_epochs: 1           # PPO æ›´æ–°è½®æ•°
  # è¶Šå¤§è¶Šç¨³å®šï¼Œä½†è¶Šæ…¢

  clip_ratio: 0.2         # PPO clip èŒƒå›´
  # é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ï¼Œé˜²æ­¢å´©æºƒ

  # ========== KL Lossï¼ˆGRPO ç”¨ï¼‰==========
  use_kl_loss: false      # æ˜¯å¦ä½¿ç”¨ KL loss
  kl_loss_coef: 0.001     # KL loss ç³»æ•°
  kl_loss_type: kl        # KL è®¡ç®—æ–¹å¼ï¼škl, abs, mse

  # ========== Loss Aggregation ==========
  loss_agg_mode: token-mean
  # é€‰é¡¹ï¼š
  # - token-mean: æ‰€æœ‰ token çš„å¹³å‡æŸå¤±ï¼ˆæ¨èï¼‰
  # - seq-mean-token-sum: åºåˆ—ç»´åº¦å¹³å‡ï¼Œtoken ç»´åº¦æ±‚å’Œ
  # - seq-mean-token-mean: åºåˆ—å’Œ token éƒ½å¹³å‡ï¼ˆGRPO åŸå§‹è®ºæ–‡ï¼‰

  # ========== FSDP é…ç½® ==========
  strategy: fsdp          # è®­ç»ƒç­–ç•¥ï¼šfsdp, fsdp2
  fsdp_config:
    param_offload: false  # æ˜¯å¦ offload å‚æ•°åˆ° CPU
    optimizer_offload: false  # æ˜¯å¦ offload ä¼˜åŒ–å™¨çŠ¶æ€
```

**å‚æ•°è°ƒä¼˜å»ºè®®ï¼š**
```yaml
# å°æ¨¡å‹ï¼ˆ7Bï¼‰
lr: 1e-6
ppo_micro_batch_size_per_gpu: 16
ppo_epochs: 1

# å¤§æ¨¡å‹ï¼ˆ70B+ï¼‰
lr: 5e-7                        # é™ä½å­¦ä¹ ç‡
ppo_micro_batch_size_per_gpu: 4  # å‡å° batch size
param_offload: true             # å¯ç”¨ CPU offload
```

### 3.3 Rollout é…ç½®

**æ–‡ä»¶ï¼š** `verl/trainer/config/rollout/rollout.yaml`

```yaml
rollout:
  # ========== æ¨ç†å¼•æ“ ==========
  name: vllm              # æ¨ç†å¼•æ“ï¼švllm æˆ– sglang

  # ========== é‡‡æ ·å‚æ•° ==========
  temperature: 0.6        # é‡‡æ ·æ¸©åº¦
  # è¶Šé«˜è¶Šéšæœºï¼Œè¶Šä½è¶Šç¡®å®š

  top_p: 1.0              # Top-pï¼ˆnucleusï¼‰é‡‡æ ·
  # < 1.0 å¯ä»¥å‡å°‘ä½æ¦‚ç‡ token

  top_k: null             # Top-k é‡‡æ ·ï¼ˆé€šå¸¸ä¸ç”¨ï¼‰

  n: 1                    # æ¯ä¸ª prompt é‡‡æ ·å‡ æ¬¡
  # GRPO: n >= 2ï¼ˆé€šå¸¸ 4-8ï¼‰
  # PPO: n = 1

  # ========== vLLM ç‰¹å®šå‚æ•° ==========
  gpu_memory_utilization: 0.6   # GPU æ˜¾å­˜åˆ©ç”¨ç‡
  # è®­ç»ƒå’Œæ¨ç†å…±äº« GPUï¼Œæ¨ç†ä¸èƒ½å æ»¡

  tensor_model_parallel_size: 1  # Tensor å¹¶è¡Œåº¦
  # å¤§æ¨¡å‹å¯èƒ½éœ€è¦ > 1

  # ========== æ€§èƒ½å‚æ•° ==========
  max_num_batched_tokens: null  # æœ€å¤§æ‰¹å¤„ç† token æ•°
  max_num_seqs: 256             # æœ€å¤§å¹¶å‘åºåˆ—æ•°

  # ========== å¼•æ“é€‰é¡¹ ==========
  enforce_eager: false    # æ˜¯å¦å¼ºåˆ¶ eager æ¨¡å¼
  disable_log_stats: false  # æ˜¯å¦ç¦ç”¨ç»Ÿè®¡æ—¥å¿—
```

**GRPO vs PPO é…ç½®å·®å¼‚ï¼š**
```yaml
# GRPO é…ç½®
rollout:
  n: 4                    # æ¯ä¸ª prompt ç”Ÿæˆ 4 ä¸ªå“åº”
  temperature: 0.7        # éœ€è¦ä¸€å®šéšæœºæ€§

# PPO é…ç½®
rollout:
  n: 1                    # æ¯ä¸ª prompt ç”Ÿæˆ 1 ä¸ªå“åº”
  temperature: 0.6
```

### 3.4 Algorithm é…ç½®

```yaml
algorithm:
  # ========== ä¼˜åŠ¿ä¼°è®¡å™¨ ==========
  adv_estimator: gae
  # é€‰é¡¹ï¼š
  # - gae: Generalized Advantage Estimation (PPO)
  # - grpo: Group Relative Policy Optimization
  # - rloo: REINFORCE Leave-One-Out
  # - remax: Reward Maximization
  # - reinforce_plus_plus: REINFORCE++

  # ========== GAE å‚æ•°ï¼ˆPPO ç”¨ï¼‰==========
  gamma: 1.0              # æŠ˜æ‰£å› å­
  # æ•°å­¦é¢˜é€šå¸¸ç”¨ 1.0ï¼ˆåªçœ‹æœ€ç»ˆç»“æœï¼‰

  lam: 1.0                # GAE lambda
  # å¹³è¡¡ bias å’Œ variance

  # ========== GRPO å‚æ•° ==========
  norm_adv_by_std_in_grpo: true  # æ˜¯å¦æ ‡å‡†åŒ– advantage

  # ========== KL æ§åˆ¶ ==========
  use_kl_in_reward: false  # æ˜¯å¦åœ¨ reward ä¸­åŠ  KL æƒ©ç½š
  kl_penalty: kl          # KL è®¡ç®—æ–¹å¼

  kl_ctrl:
    type: fixed           # fixed æˆ– adaptive
    kl_coef: 0.001        # KL ç³»æ•°
```

---

## 4. é…ç½®è¦†ç›–å’Œç»„åˆ

### 4.1 å‘½ä»¤è¡Œè¦†ç›–

```bash
# åŸºç¡€ç”¨æ³•
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    actor_rollout_ref.actor.optim.lr=1e-6

# ä¿®æ”¹åµŒå¥—é…ç½®
python3 -m verl.trainer.main_ppo \
    actor_rollout_ref.actor.ppo_mini_batch_size=128 \
    algorithm.kl_ctrl.kl_coef=0.01

# ä¿®æ”¹åˆ—è¡¨
python3 -m verl.trainer.main_ppo \
    data.train_files="['data1.parquet','data2.parquet']"
```

### 4.2 åˆ‡æ¢ç®—æ³•

```bash
# ä» PPO åˆ‡æ¢åˆ° GRPO
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    actor_rollout_ref.rollout.n=4 \
    actor_rollout_ref.actor.use_kl_loss=true \
    critic.enable=false

# ä» GRPO åˆ‡æ¢åˆ° RLOO
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=rloo \
    actor_rollout_ref.rollout.n=8
```

### 4.3 åˆ‡æ¢ Rollout å¼•æ“

```bash
# ä» vLLM åˆ‡æ¢åˆ° SGLang
python3 -m verl.trainer.main_ppo \
    actor_rollout_ref.rollout.name=sglang

# åˆ‡æ¢åˆ° Megatron Rolloutï¼ˆå¤§æ¨¡å‹ï¼‰
python3 -m verl.trainer.main_ppo \
    --config-name=ppo_megatron_trainer \
    actor_rollout_ref.rollout.tensor_model_parallel_size=4
```

---

## 5. å®é™…æ¡ˆä¾‹è§£æ

### 5.1 æ¡ˆä¾‹ 1ï¼šGSM8K GRPO è®­ç»ƒ

**ç›®æ ‡ï¼š** åœ¨ 8 å¼  A100 ä¸Šè®­ç»ƒ Qwen2.5-7B

```bash
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \                     # ä½¿ç”¨ GRPO
    algorithm.norm_adv_by_std_in_grpo=true \           # æ ‡å‡†åŒ– advantage
    \
    data.train_files="['~/data/gsm8k/train.parquet']" \
    data.val_files="['~/data/gsm8k/test.parquet']" \
    data.train_batch_size=256 \                        # å…¨å±€ 256 ä¸ª prompts
    data.max_prompt_length=512 \
    data.max_response_length=512 \
    \
    actor_rollout_ref.model.path=Qwen/Qwen2.5-7B-Instruct \
    \
    actor_rollout_ref.actor.optim.lr=1e-6 \            # å­¦ä¹ ç‡
    actor_rollout_ref.actor.ppo_mini_batch_size=64 \   # Mini-batch
    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 \
    actor_rollout_ref.actor.ppo_epochs=1 \
    actor_rollout_ref.actor.use_kl_loss=true \         # GRPO ç”¨ KL loss
    actor_rollout_ref.actor.kl_loss_coef=0.001 \
    \
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.n=4 \                    # æ¯ä¸ª prompt ç”Ÿæˆ 4 ä¸ª
    actor_rollout_ref.rollout.temperature=0.6 \
    actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \
    \
    trainer.n_gpus_per_node=8 \
    trainer.nnodes=1 \
    trainer.total_epochs=10 \
    trainer.logger='["console","tensorboard"]' \
    trainer.project_name=gsm8k_grpo
```

**å…³é”®é…ç½®è§£é‡Šï¼š**

```yaml
# Batch Size è®¡ç®—
train_batch_size: 256        # 256 ä¸ª prompts
rollout.n: 4                 # æ¯ä¸ª prompt ç”Ÿæˆ 4 ä¸ªå“åº”
â†’ æ€»å“åº”æ•° = 256 * 4 = 1024

# Mini-batch åˆ†å‰²
ppo_mini_batch_size: 64      # 1024 ä¸ªå“åº”åˆ†æˆ 16 ä¸ª mini-batch
â†’ æ¯ä¸ª mini-batch: 64 ä¸ªå“åº”
â†’ æ›´æ–°æ¬¡æ•°: 16 æ¬¡

# Micro-batchï¼ˆæ¯ä¸ª GPUï¼‰
ppo_micro_batch_size_per_gpu: 4
n_gpus: 8
â†’ æ¯æ¬¡å‰å‘/åå‘: 4 * 8 = 32 ä¸ªå“åº”
â†’ æ¢¯åº¦ç´¯ç§¯: 64 / 32 = 2 æ­¥
```

### 5.2 æ¡ˆä¾‹ 2ï¼šPPO with Critic

```bash
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=gae \                      # ä½¿ç”¨ GAEï¼ˆPPOï¼‰
    algorithm.gamma=1.0 \
    algorithm.lam=0.95 \
    \
    critic.enable=true \                               # å¯ç”¨ Critic
    critic.model.path=Qwen/Qwen2.5-7B-Instruct \
    critic.optim.lr=1e-5 \                             # Critic lr é€šå¸¸æ›´é«˜
    critic.ppo_mini_batch_size=64 \
    \
    actor_rollout_ref.rollout.n=1 \                    # PPO é€šå¸¸ n=1
    \
    # ... å…¶ä»–é…ç½®
```

### 5.3 æ¡ˆä¾‹ 3ï¼šå¤šæ•°æ®é›†æ··åˆè®­ç»ƒ

```bash
python3 -m verl.trainer.main_ppo \
    data.train_files="[
        '~/data/gsm8k/train.parquet',
        '~/data/math/train.parquet',
        '~/data/code/train.parquet'
    ]" \
    # verl ä¼šè‡ªåŠ¨åˆå¹¶æ•°æ®é›†
```

---

## 6. é…ç½®è°ƒä¼˜æŠ€å·§

### 6.1 å­¦ä¹ ç‡è°ƒä¼˜

```yaml
# ç°è±¡ï¼šReward ä¸ä¸Šå‡
# å¯èƒ½åŸå› ï¼šå­¦ä¹ ç‡å¤ªå°
actor.optim.lr: 5e-6  # ä» 1e-6 å¢åŠ åˆ° 5e-6

# ç°è±¡ï¼šReward çªç„¶ä¸‹é™ï¼ˆè®­ç»ƒå´©æºƒï¼‰
# å¯èƒ½åŸå› ï¼šå­¦ä¹ ç‡å¤ªå¤§
actor.optim.lr: 5e-7  # ä» 1e-6 é™ä½åˆ° 5e-7

# ç°è±¡ï¼šå¼€å§‹å‡ ä¸ª epoch ä¸ç¨³å®š
# è§£å†³ï¼šå¢åŠ  warmup
actor.lr_scheduler.warmup_steps: 200  # ä» 100 å¢åŠ 
```

### 6.2 Batch Size è°ƒä¼˜

```yaml
# OOM é”™è¯¯
# è§£å†³æ–¹æ¡ˆ 1ï¼šå‡å° micro batch
ppo_micro_batch_size_per_gpu: 2  # ä» 4 é™ä½

# è§£å†³æ–¹æ¡ˆ 2ï¼šå‡å°å…¨å±€ batch
train_batch_size: 128  # ä» 256 é™ä½

# è§£å†³æ–¹æ¡ˆ 3ï¼šå‡å°å“åº”é•¿åº¦
max_response_length: 256  # ä» 512 é™ä½
```

### 6.3 KL Divergence è°ƒä¼˜

```yaml
# KL å¤ªå¤§ï¼ˆæ¨¡å‹è¾“å‡ºå˜å¾—å¾ˆå¥‡æ€ªï¼‰
kl_loss_coef: 0.01      # ä» 0.001 å¢åŠ 

# KL å¤ªå°ï¼ˆå­¦ä¹ ä¸å……åˆ†ï¼‰
kl_loss_coef: 0.0005    # ä» 0.001 é™ä½
```

### 6.4 GRPO Group Size è°ƒä¼˜

```yaml
# è®­ç»ƒå¤ªæ…¢
rollout.n: 2            # ä» 4 é™ä½ï¼ˆä½†å¯èƒ½å½±å“æ•ˆæœï¼‰

# æƒ³è¦æ›´å¥½çš„æ•ˆæœ
rollout.n: 8            # å¢åŠ  group sizeï¼ˆä½†æ›´æ…¢ï¼‰
```

---

## 7. é…ç½®éªŒè¯å’Œè°ƒè¯•

### 7.1 æ‰“å°å®Œæ•´é…ç½®

```python
# åœ¨ main_ppo.py å¼€å¤´æ·»åŠ 
import yaml
print("="*60)
print("å®Œæ•´é…ç½®:")
print("="*60)
print(yaml.dump(OmegaConf.to_container(config), default_flow_style=False))
```

### 7.2 éªŒè¯é…ç½®

```python
# æ£€æŸ¥å…³é”®é…ç½®
assert config.data.train_batch_size > 0
assert config.actor_rollout_ref.actor.optim.lr > 0
assert config.actor_rollout_ref.rollout.n >= 1

if config.algorithm.adv_estimator == 'grpo':
    assert config.actor_rollout_ref.rollout.n > 1, "GRPO éœ€è¦ n > 1"
    assert config.actor_rollout_ref.actor.use_kl_loss, "GRPO éœ€è¦ use_kl_loss=true"
```

### 7.3 ä¿å­˜ä½¿ç”¨çš„é…ç½®

```python
# verl ä¼šè‡ªåŠ¨ä¿å­˜é…ç½®åˆ° outputs/
# è®­ç»ƒå®ŒæˆåæŸ¥çœ‹ï¼š
outputs/
â””â”€â”€ run_2026_01_25_10_00_00/
    â”œâ”€â”€ config.yaml         # å®Œæ•´é…ç½®
    â”œâ”€â”€ overrides.yaml      # å‘½ä»¤è¡Œè¦†ç›–
    â””â”€â”€ checkpoints/
```

---

## 8. å¸¸è§é…ç½®é”™è¯¯

### é”™è¯¯ 1ï¼šGRPO é…ç½®é”™è¯¯

```yaml
# âŒ é”™è¯¯é…ç½®
algorithm.adv_estimator: grpo
actor_rollout_ref.rollout.n: 1        # åº”è¯¥ > 1
actor_rollout_ref.actor.use_kl_loss: false  # åº”è¯¥ true

# âœ… æ­£ç¡®é…ç½®
algorithm.adv_estimator: grpo
actor_rollout_ref.rollout.n: 4
actor_rollout_ref.actor.use_kl_loss: true
```

### é”™è¯¯ 2ï¼šBatch Size ä¸åŒ¹é…

```yaml
# âŒ é”™è¯¯ï¼šmini_batch > train_batch * n
train_batch_size: 64
rollout.n: 2
ppo_mini_batch_size: 256              # 128 < 256ï¼Œé”™è¯¯ï¼

# âœ… æ­£ç¡®
train_batch_size: 256
rollout.n: 4
ppo_mini_batch_size: 256              # 1024 >= 256ï¼Œæ­£ç¡®
```

### é”™è¯¯ 3ï¼šGPU å†…å­˜é…ç½®ä¸å½“

```yaml
# âŒ Rollout å ç”¨å¤ªå¤š GPU
rollout.gpu_memory_utilization: 0.9   # è®­ç»ƒä¼š OOM

# âœ… ç•™ç»™è®­ç»ƒè¶³å¤Ÿç©ºé—´
rollout.gpu_memory_utilization: 0.4   # Actor å¯ç”¨ 0.6
```

---

## 9. é…ç½®æ¨¡æ¿

### 9.1 å¿«é€Ÿå¼€å§‹æ¨¡æ¿ï¼ˆå•æœº 8 å¡ï¼‰

```yaml
# my_config.yaml
defaults:
  - ppo_trainer

algorithm:
  adv_estimator: grpo

data:
  train_files: ['~/data/gsm8k/train.parquet']
  train_batch_size: 256

actor_rollout_ref:
  model:
    path: Qwen/Qwen2.5-7B-Instruct

  actor:
    optim:
      lr: 1e-6

  rollout:
    name: vllm
    n: 4

trainer:
  n_gpus_per_node: 8
  total_epochs: 10
```

### 9.2 å¤§æ¨¡å‹æ¨¡æ¿ï¼ˆå¤šæœºå¤šå¡ï¼‰

```yaml
# large_model.yaml
defaults:
  - ppo_megatron_trainer

actor_rollout_ref:
  model:
    path: Qwen/Qwen3-235B

  actor:
    megatron:
      tensor_model_parallel_size: 8
      pipeline_model_parallel_size: 4

trainer:
  n_gpus_per_node: 8
  nnodes: 4                           # 4 ä¸ªèŠ‚ç‚¹
```

---

## 10. æ€»ç»“

**é…ç½®ä¼˜å…ˆçº§ï¼ˆä»ä½åˆ°é«˜ï¼‰ï¼š**
```
defaults ä¸­çš„é…ç½®
    â†“
å½“å‰æ–‡ä»¶ï¼ˆ_self_ï¼‰
    â†“
å‘½ä»¤è¡Œå‚æ•°
```

**æ ¸å¿ƒé…ç½®è®°å¿†å£è¯€ï¼š**
- **Data**: batch_size, max_length
- **Actor**: lr, ppo_epochs, clip_ratio
- **Rollout**: name, n, temperature
- **Algorithm**: adv_estimator, gamma, lam

**è°ƒè¯•æ¸…å•ï¼š**
1. æ‰“å°å®Œæ•´é…ç½®
2. æ£€æŸ¥ batch size è®¡ç®—
3. éªŒè¯ç®—æ³•ç‰¹å®šé…ç½®
4. ç›‘æ§ GPU å†…å­˜ä½¿ç”¨

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [Hydra å®˜æ–¹æ–‡æ¡£](https://hydra.cc/docs/intro/)
- [RayPPOTrainer è¯¦è§£](./ray_trainer_è¯¦è§£.md)
- [å®˜æ–¹æ–‡æ¡£ï¼šConfig Explanation](https://verl.readthedocs.io/en/latest/examples/config.html)
