# RayPPOTrainer è®­ç»ƒä¸»å¾ªç¯è¯¦è§£

> æ·±å…¥è§£æ verl çš„æ ¸å¿ƒè®­ç»ƒæµç¨‹ï¼Œç†è§£æ¯ä¸€æ­¥çš„å·¥ä½œåŸç†

---

## ğŸ“‹ æœ¬æ–‡å†…å®¹

1. RayPPOTrainer ç±»æ¦‚è§ˆ
2. åˆå§‹åŒ–æµç¨‹ï¼ˆ__init__ï¼‰
3. è®­ç»ƒä¸»å¾ªç¯ï¼ˆfitï¼‰
4. å•æ­¥è®­ç»ƒï¼ˆ_train_stepï¼‰
5. æ•°æ®æµè½¬è¯¦è§£
6. å®é™…ä¾‹å­è¿½è¸ª

---

## 1. RayPPOTrainer ç±»æ¦‚è§ˆ

### 1.1 æ–‡ä»¶ä½ç½®

**ä¸»æ–‡ä»¶ï¼š** `verl/trainer/ppo/ray_trainer.py` (çº¦2500è¡Œ)

**å…³é”®ç±»ï¼š**
```python
class RayPPOTrainer:
    """Ray-based PPO/GRPO trainer

    æ ¸å¿ƒèŒè´£ï¼š
    1. ç®¡ç†åˆ†å¸ƒå¼èµ„æºï¼ˆGPUï¼‰
    2. åˆ›å»ºå’Œåè°ƒå¤šä¸ª WorkerGroupï¼ˆActor, Critic, Rolloutï¼‰
    3. æ‰§è¡Œè®­ç»ƒå¾ªç¯
    4. æ”¶é›†å’Œè®°å½•æŒ‡æ ‡
    """
```

### 1.2 ç±»çš„æ•´ä½“ç»“æ„

```python
# verl/trainer/ppo/ray_trainer.py

class RayPPOTrainer:
    def __init__(self, config):
        """åˆå§‹åŒ–ï¼šåˆ›å»ºèµ„æºæ± å’Œ Worker ç»„"""

    def fit(self):
        """ä¸»è®­ç»ƒå¾ªç¯ï¼šè¿­ä»£ epoch å’Œ batch"""

    def _train_step(self, batch):
        """å•æ­¥è®­ç»ƒï¼šrollout â†’ reward â†’ advantage â†’ update"""

    def _compute_reward(self, data):
        """è®¡ç®— reward"""

    def _validate(self):
        """éªŒè¯å’Œè¯„ä¼°"""
```

---

## 2. åˆå§‹åŒ–æµç¨‹è¯¦è§£

### 2.1 åˆå§‹åŒ–ä»£ç ï¼ˆç®€åŒ–ç‰ˆï¼‰

```python
# verl/trainer/ppo/ray_trainer.py: ç¬¬ 100-400 è¡Œï¼ˆç®€åŒ–ï¼‰

def __init__(self, config):
    self.config = config

    # ========== æ­¥éª¤ 1: åˆ›å»ºèµ„æºæ±  ==========
    print("[Step 1] åˆ›å»º GPU èµ„æºæ± ...")
    self.resource_pool_manager = ResourcePoolManager(
        process_on_nodes=ray.cluster_resources(),
        use_gpu=True
    )
    self.resource_pool_manager.create_resource_pool()

    # ========== æ­¥éª¤ 2: åˆ›å»º Actor+Rollout+Ref WorkerGroup ==========
    print("[Step 2] åˆ›å»º Actor+Rollout+Ref WorkerGroup...")
    self.actor_rollout_wg = self._create_actor_rollout_worker_group()
    # å†…éƒ¨åŒ…å«ï¼š
    # - Actor æ¨¡å‹ï¼ˆè®­ç»ƒä¸­ï¼‰
    # - Rollout å¼•æ“ï¼ˆvLLM/SGLangï¼Œç”¨äºç”Ÿæˆï¼‰
    # - Reference æ¨¡å‹ï¼ˆå›ºå®šï¼Œç”¨äº KL è®¡ç®—ï¼‰

    # ========== æ­¥éª¤ 3: åˆ›å»º Critic WorkerGroupï¼ˆå¦‚æœå¯ç”¨ï¼‰==========
    if config.critic.enable:
        print("[Step 3] åˆ›å»º Critic WorkerGroup...")
        self.critic_wg = self._create_critic_worker_group()

    # ========== æ­¥éª¤ 4: åˆ›å»º RewardManager ==========
    print("[Step 4] åˆ›å»º RewardManager...")
    self.reward_manager = RewardManager(...)

    # ========== æ­¥éª¤ 5: åˆ›å»º DataLoader ==========
    print("[Step 5] åˆ›å»º DataLoader...")
    self.train_dataloader = self._create_dataloader()

    print("åˆå§‹åŒ–å®Œæˆï¼")
```

### 2.2 èµ„æºæ± åˆ›å»ºï¼ˆResourcePoolManagerï¼‰

**ä»£ç ä½ç½®ï¼š** `verl/single_controller/ray/base.py`

```python
# èµ„æºæ± çš„ä½œç”¨ï¼šç®¡ç†æ‰€æœ‰ GPUï¼Œåˆ†é…ç»™ä¸åŒçš„ WorkerGroup

class ResourcePoolManager:
    def create_resource_pool(self):
        """
        å‡è®¾æœ‰ 8 å¼  GPUï¼š

        èµ„æºæ± åˆ†é…ç¤ºä¾‹ï¼ˆColocate æ¨¡å¼ï¼‰ï¼š
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ GPU 0-3: Actor + Rollout + Ref      â”‚  â† å…±äº« GPU
        â”‚ GPU 4-7: Critic                     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        é Colocate æ¨¡å¼ï¼š
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ GPU 0-3: Actor                      â”‚
        â”‚ GPU 4-5: Rollout                    â”‚
        â”‚ GPU 6-7: Critic                     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """
        # å®é™…ä»£ç ä¼šæ ¹æ®é…ç½®æ™ºèƒ½åˆ†é…
```

### 2.3 Actor+Rollout WorkerGroup åˆ›å»º

```python
# verl/trainer/ppo/ray_trainer.py: ç¬¬ 450-550 è¡Œï¼ˆç®€åŒ–ï¼‰

def _create_actor_rollout_worker_group(self):
    """åˆ›å»º Actor+Rollout+Ref WorkerGroup

    è¿™æ˜¯ verl çš„æ ¸å¿ƒåˆ›æ–°ï¼šå°†è®­ç»ƒï¼ˆActorï¼‰å’Œæ¨ç†ï¼ˆRolloutï¼‰
    å…±äº«åŒä¸€ç»„ GPUï¼Œé¿å…æƒé‡æ‹·è´
    """

    # 1. ä»é…ç½®åˆ›å»º Worker ç±»
    from verl.workers.fsdp_workers import ActorRolloutRefWorker

    # 2. åˆ›å»º WorkerGroupï¼ˆåˆ†å¸ƒå¼ï¼‰
    worker_group = RayWorkerGroup(
        resource_pool=self.actor_rollout_pool,  # GPU èµ„æº
        ray_cls_with_init=ActorRolloutRefWorker,
        num_workers=4,  # ä¾‹å¦‚ 4 ä¸ª workerï¼Œæ¯ä¸ªç®¡ç† 1-2 å¼  GPU
    )

    # 3. åˆå§‹åŒ– workerï¼ˆåŠ è½½æ¨¡å‹ï¼‰
    worker_group.init_model(
        model_path=self.config.actor_rollout_ref.model.path,
        enable_gradient_checkpointing=True,
    )

    return worker_group
```

**å…³é”®ç‚¹ï¼šActorã€Rolloutã€Ref åœ¨åŒä¸€ä¸ª Worker ä¸­ï¼**

```
ActorRolloutRefWorker å†…éƒ¨ç»“æ„ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ActorRolloutRefWorker (å•ä¸ª Worker)     â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚ Actor Model â”‚ â† FSDP åŒ…è£…ï¼Œå¯è®­ç»ƒ   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚         â†• æƒé‡åŒæ­¥                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚Rollout(vLLM)â”‚ â† æ¨ç†å¼•æ“            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚         â†•                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚ Ref Model   â”‚ â† å›ºå®šï¼Œç”¨äºKLè®¡ç®—    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. è®­ç»ƒä¸»å¾ªç¯ï¼ˆfitï¼‰

### 3.1 å®Œæ•´çš„ fit æ–¹æ³•

```python
# verl/trainer/ppo/ray_trainer.py: ç¬¬ 1000-1100 è¡Œï¼ˆç®€åŒ–ï¼‰

def fit(self):
    """ä¸»è®­ç»ƒå¾ªç¯"""

    for epoch in range(self.config.trainer.total_epochs):
        print(f"\n{'='*60}")
        print(f"Epoch {epoch + 1}/{self.config.trainer.total_epochs}")
        print('='*60)

        # éå† DataLoader
        for batch_idx, batch in enumerate(self.train_dataloader):
            # æ ¸å¿ƒï¼šå•æ­¥è®­ç»ƒ
            metrics = self._train_step(batch)

            # è®°å½•æŒ‡æ ‡
            self._log_metrics(metrics, self.global_step)

            # æ›´æ–°æ­¥æ•°
            self.global_step += 1

            # å®šæœŸä¿å­˜ checkpoint
            if self.global_step % self.config.trainer.save_freq == 0:
                self._save_checkpoint()

        # æ¯ä¸ª epoch ç»“æŸåéªŒè¯
        if epoch % self.config.trainer.test_freq == 0:
            self._validate()
```

### 3.2 DataLoader çš„æ•°æ®æ ¼å¼

```python
# DataLoader äº§ç”Ÿçš„ batch æ ¼å¼ï¼š

batch = {
    'input_ids': tensor([batch_size, max_prompt_length]),  # Prompt tokens
    'attention_mask': tensor([batch_size, max_prompt_length]),
    'data_source': ['gsm8k', 'gsm8k', ...],  # æ•°æ®æ¥æº
    'ground_truth': ['42', '100', ...],  # æ ‡å‡†ç­”æ¡ˆ
}
```

---

## 4. å•æ­¥è®­ç»ƒè¯¦è§£ï¼ˆ_train_stepï¼‰

è¿™æ˜¯ **æœ€æ ¸å¿ƒ** çš„å‡½æ•°ï¼æ¯æ¬¡è°ƒç”¨å®Œæˆä¸€è½®å®Œæ•´çš„ RL æ›´æ–°ã€‚

### 4.1 _train_step å®Œæ•´æµç¨‹

```python
# verl/trainer/ppo/ray_trainer.py: ç¬¬ 1200-1500 è¡Œï¼ˆå¸¦è¯¦ç»†æ³¨é‡Šï¼‰

def _train_step(self, batch: Dict) -> Dict[str, Any]:
    """å•æ­¥è®­ç»ƒçš„å®Œæ•´æµç¨‹

    è¾“å…¥ï¼š
        batch: ä» DataLoader æ¥çš„ä¸€æ‰¹ prompts

    è¾“å‡ºï¼š
        metrics: è®­ç»ƒæŒ‡æ ‡å­—å…¸
    """

    # ==========================================
    # é˜¶æ®µ 1: Rolloutï¼ˆç”Ÿæˆå“åº”ï¼‰
    # ==========================================
    print("[Phase 1] Rollout: ç”Ÿæˆå“åº”...")

    # è°ƒç”¨ Actor+Rollout WorkerGroup çš„ generate_sequences æ–¹æ³•
    # è¿™ä¼šåœ¨æ‰€æœ‰ worker ä¸Šå¹¶è¡Œç”Ÿæˆ
    rollout_output = self.actor_rollout_wg.generate_sequences(
        prompts=batch,  # è¾“å…¥ prompts
        temperature=self.config.actor_rollout_ref.rollout.temperature,
        top_p=self.config.actor_rollout_ref.rollout.top_p,
        max_new_tokens=self.config.data.max_response_length,
    )

    # rollout_output åŒ…å«ï¼š
    # - responses: ç”Ÿæˆçš„ token IDs
    # - response_mask: å“ªäº› token æ˜¯æœ‰æ•ˆçš„
    # - old_log_probs: å½“å‰ç­–ç•¥çš„ log probï¼ˆç”¨äº PPO ratio è®¡ç®—ï¼‰

    # ==========================================
    # é˜¶æ®µ 2: Reward Computationï¼ˆè®¡ç®—å¥–åŠ±ï¼‰
    # ==========================================
    print("[Phase 2] Reward: è®¡ç®—å¥–åŠ±...")

    rollout_output = self._compute_reward(rollout_output)

    # ç°åœ¨ rollout_output æ–°å¢äº†ï¼š
    # - rewards: æ¯ä¸ª token çš„ reward åˆ†æ•°

    # ==========================================
    # é˜¶æ®µ 3: Reference Log Probï¼ˆå¯é€‰ï¼Œç”¨äº KL æƒ©ç½šï¼‰
    # ==========================================
    if self.config.algorithm.use_kl_in_reward:
        print("[Phase 3] Ref: è®¡ç®—å‚è€ƒæ¨¡å‹ log prob...")

        rollout_output = self.actor_rollout_wg.compute_ref_log_prob(
            rollout_output
        )

        # æ–°å¢ï¼š
        # - ref_log_probs: å‚è€ƒæ¨¡å‹çš„ log prob

    # ==========================================
    # é˜¶æ®µ 4: Value Computationï¼ˆPPO éœ€è¦ï¼ŒGRPO ä¸éœ€è¦ï¼‰
    # ==========================================
    if self.config.critic.enable:
        print("[Phase 4] Critic: è®¡ç®— value...")

        rollout_output = self.critic_wg.compute_values(rollout_output)

        # æ–°å¢ï¼š
        # - values: Critic é¢„æµ‹çš„ value

    # ==========================================
    # é˜¶æ®µ 5: Advantage Computationï¼ˆä¼˜åŠ¿ä¼°è®¡ï¼‰
    # ==========================================
    print("[Phase 5] Advantage: è®¡ç®—ä¼˜åŠ¿å€¼...")

    rollout_output = self._compute_advantage(rollout_output)

    # æ–°å¢ï¼š
    # - advantages: ä¼˜åŠ¿å€¼ï¼ˆæ ¸å¿ƒï¼ï¼‰
    # - returns: å›æŠ¥å€¼ï¼ˆç”¨äº critic è®­ç»ƒï¼‰

    # ==========================================
    # é˜¶æ®µ 6: Actor Updateï¼ˆæ›´æ–°ç­–ç•¥ï¼‰
    # ==========================================
    print("[Phase 6] Actor Update: æ›´æ–°ç­–ç•¥æ¨¡å‹...")

    actor_metrics = self.actor_rollout_wg.update_actor(
        data=rollout_output,
        ppo_epochs=self.config.actor_rollout_ref.actor.ppo_epochs,
        ppo_mini_batch_size=self.config.actor_rollout_ref.actor.ppo_mini_batch_size,
    )

    # actor_metrics åŒ…å«ï¼š
    # - actor/loss: Actor æŸå¤±
    # - actor/policy_loss: ç­–ç•¥æŸå¤±
    # - actor/entropy: ç†µï¼ˆæ¢ç´¢ç¨‹åº¦ï¼‰
    # - actor/approx_kl: è¿‘ä¼¼ KL æ•£åº¦

    # ==========================================
    # é˜¶æ®µ 7: Critic Updateï¼ˆæ›´æ–°ä»·å€¼å‡½æ•°ï¼ŒPPO éœ€è¦ï¼‰
    # ==========================================
    critic_metrics = {}
    if self.config.critic.enable:
        print("[Phase 7] Critic Update: æ›´æ–°ä»·å€¼å‡½æ•°...")

        critic_metrics = self.critic_wg.update_critic(
            data=rollout_output,
            ppo_epochs=self.config.critic.ppo_epochs,
        )

        # critic_metrics åŒ…å«ï¼š
        # - critic/loss: Critic æŸå¤±
        # - critic/value_loss: ä»·å€¼æŸå¤±

    # ==========================================
    # é˜¶æ®µ 8: æ”¶é›†æ‰€æœ‰æŒ‡æ ‡
    # ==========================================
    metrics = {
        **actor_metrics,
        **critic_metrics,
        'reward/mean': rollout_output.batch['rewards'].mean().item(),
        'kl/mean': (rollout_output.batch['old_log_probs'] -
                    rollout_output.batch['ref_log_probs']).mean().item(),
    }

    return metrics
```

### 4.2 æ•°æ®åœ¨å„é˜¶æ®µçš„å˜åŒ–

```
é˜¶æ®µ 0ï¼ˆè¾“å…¥ï¼‰:
  DataProto {
    batch: {
      'input_ids': [B, L_prompt]
    }
  }

é˜¶æ®µ 1ï¼ˆRolloutåï¼‰:
  DataProto {
    batch: {
      'input_ids': [B, L_prompt]
      'responses': [B, L_response]          â† æ–°å¢
      'response_mask': [B, L_response]      â† æ–°å¢
      'old_log_probs': [B, L_response]      â† æ–°å¢
    }
  }

é˜¶æ®µ 2ï¼ˆRewardåï¼‰:
  DataProto {
    ...ï¼ˆä¸Šé¢çš„æ‰€æœ‰å­—æ®µï¼‰
    batch: {
      'rewards': [B, L_response]            â† æ–°å¢
    }
  }

é˜¶æ®µ 3ï¼ˆRefåï¼Œå¦‚æœå¯ç”¨ï¼‰:
  DataProto {
    ...
    batch: {
      'ref_log_probs': [B, L_response]      â† æ–°å¢
    }
  }

é˜¶æ®µ 4ï¼ˆValueåï¼ŒPPOï¼‰:
  DataProto {
    ...
    batch: {
      'values': [B, L_response]             â† æ–°å¢
    }
  }

é˜¶æ®µ 5ï¼ˆAdvantageåï¼‰:
  DataProto {
    ...
    batch: {
      'advantages': [B, L_response]         â† æ–°å¢
      'returns': [B, L_response]            â† æ–°å¢ï¼ˆPPOï¼‰
    }
  }
```

---

## 5. è¯¦ç»†ä¾‹å­ï¼šGSM8K è®­ç»ƒä¸€æ­¥

è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªå…·ä½“ä¾‹å­è¿½è¸ªæ•´ä¸ªæµç¨‹ï¼š

### 5.1 åˆå§‹çŠ¶æ€

```python
# å‡è®¾ batch_size = 2
batch = {
    'input_ids': tensor([
        [123, 456, 789, ...],  # "What is 25 * 4?"
        [234, 567, 890, ...],  # "Calculate 100 / 5"
    ]),
    'data_source': ['gsm8k', 'gsm8k'],
    'ground_truth': ['100', '20'],
}
```

### 5.2 é˜¶æ®µ 1ï¼šRollout

```python
# è°ƒç”¨ vLLM ç”Ÿæˆ
rollout_output = self.actor_rollout_wg.generate_sequences(batch)

# ç”Ÿæˆçš„å“åº”ï¼ˆç¤ºä¾‹ï¼‰
rollout_output.batch = {
    'input_ids': [...],  # åŸå§‹ prompt
    'responses': tensor([
        [345, 678, 901, ...],  # "Let me think... 25 * 4 = 100"
        [456, 789, 012, ...],  # "100 / 5 = 20"
    ]),
    'response_mask': tensor([
        [1, 1, 1, ..., 1],
        [1, 1, 1, ..., 1],
    ]),
    'old_log_probs': tensor([
        [-0.5, -0.3, -0.4, ...],  # æ¯ä¸ª token çš„ log prob
        [-0.6, -0.4, -0.5, ...],
    ]),
}
```

### 5.3 é˜¶æ®µ 2ï¼šReward

```python
# è®¡ç®— reward
rollout_output = self._compute_reward(rollout_output)

# RewardManager å†…éƒ¨ï¼š
# 1. Decode responses: "25 * 4 = 100", "100 / 5 = 20"
# 2. è°ƒç”¨ GSM8K reward å‡½æ•°
# 3. æå–ç­”æ¡ˆ: "100", "20"
# 4. å¯¹æ¯” ground_truth: "100" vs "100" âœ“, "20" vs "20" âœ“
# 5. ç”Ÿæˆ reward: 1.0, 1.0

rollout_output.batch['rewards'] = tensor([
    [0, 0, 0, ..., 1.0],  # æœ€åä¸€ä¸ª token ç»™ reward
    [0, 0, 0, ..., 1.0],
])
```

### 5.4 é˜¶æ®µ 5ï¼šAdvantageï¼ˆGRPOï¼‰

```python
# GRPO è®¡ç®—ï¼šç›¸å¯¹äºç»„å¹³å‡çš„ä¼˜åŠ¿
# å‡è®¾ group_size=4ï¼Œæ¯ä¸ª prompt ç”Ÿæˆ 4 ä¸ªå“åº”

# å¯¹äºç¬¬ä¸€ä¸ª prompt çš„ 4 ä¸ªå“åº”ï¼š
group_rewards = [1.0, 0.0, 1.0, 0.0]  # 2 ä¸ªå¯¹ï¼Œ2 ä¸ªé”™
group_mean = 0.5

advantages = [
    1.0 - 0.5 = 0.5,   # å¥½äºå¹³å‡
    0.0 - 0.5 = -0.5,  # å·®äºå¹³å‡
    1.0 - 0.5 = 0.5,   # å¥½äºå¹³å‡
    0.0 - 0.5 = -0.5,  # å·®äºå¹³å‡
]

# è¿™å‘Šè¯‰æ¨¡å‹ï¼š
# - å¼ºåŒ–å‰ä¸¤ä¸ªå’Œç¬¬ä¸‰ä¸ªå“åº”ï¼ˆæ­£ä¼˜åŠ¿ï¼‰
# - æŠ‘åˆ¶ç¬¬äºŒä¸ªå’Œç¬¬å››ä¸ªå“åº”ï¼ˆè´Ÿä¼˜åŠ¿ï¼‰
```

### 5.6 é˜¶æ®µ 6ï¼šActor Update

```python
# è®¡ç®— PPO loss
for mini_batch in split(rollout_output, mini_batch_size):
    # 1. å‰å‘ä¼ æ’­ï¼Œè·å–æ–°çš„ log_probs
    new_log_probs = actor_model(mini_batch)

    # 2. è®¡ç®— ratio
    ratio = torch.exp(new_log_probs - old_log_probs)

    # 3. PPO clip
    clipped_ratio = torch.clamp(ratio, 1-clip, 1+clip)

    # 4. Policy loss
    loss = -torch.min(
        ratio * advantages,
        clipped_ratio * advantages
    ).mean()

    # 5. åå‘ä¼ æ’­
    loss.backward()
    optimizer.step()
```

---

## 6. å…³é”®ä»£ç ä½ç½®é€ŸæŸ¥

| åŠŸèƒ½ | æ–‡ä»¶ | è¡Œå·èŒƒå›´ |
|------|------|---------|
| RayPPOTrainer ç±»å®šä¹‰ | `verl/trainer/ppo/ray_trainer.py` | 50-100 |
| __init__ æ–¹æ³• | `verl/trainer/ppo/ray_trainer.py` | 100-400 |
| fit æ–¹æ³• | `verl/trainer/ppo/ray_trainer.py` | 1000-1100 |
| _train_step æ–¹æ³• | `verl/trainer/ppo/ray_trainer.py` | 1200-1500 |
| _compute_reward | `verl/trainer/ppo/ray_trainer.py` | 1600-1700 |
| _compute_advantage | `verl/trainer/ppo/ray_trainer.py` | 1800-1900 |
| ActorRolloutRefWorker | `verl/workers/fsdp_workers.py` | 100-800 |
| generate_sequences | `verl/workers/fsdp_workers.py` | 300-400 |
| update_actor | `verl/workers/fsdp_workers.py` | 500-600 |
| PPO loss è®¡ç®— | `verl/trainer/ppo/core_algos.py` | 200-300 |
| GRPO advantage è®¡ç®— | `verl/trainer/ppo/core_algos.py` | 400-500 |

---

## 7. è°ƒè¯•æŠ€å·§

### 7.1 æ‰“å°æ•°æ®æµ

```python
# åœ¨ _train_step çš„å„ä¸ªé˜¶æ®µæ·»åŠ  print

def _train_step(self, batch):
    print(f"[Debug] Input batch shape: {batch['input_ids'].shape}")

    rollout_output = self.actor_rollout_wg.generate_sequences(batch)
    print(f"[Debug] After rollout: responses shape = {rollout_output.batch['responses'].shape}")

    rollout_output = self._compute_reward(rollout_output)
    print(f"[Debug] Reward mean: {rollout_output.batch['rewards'].mean()}")

    # ... æ›´å¤šè°ƒè¯•è¾“å‡º
```

### 7.2 ä½¿ç”¨ Ray Dashboard

```bash
# Ray ä¼šè‡ªåŠ¨å¯åŠ¨ Dashboard
# è®¿é—® http://localhost:8265

# å¯ä»¥çœ‹åˆ°ï¼š
# - æ¯ä¸ª Worker çš„ GPU ä½¿ç”¨ç‡
# - æ¯ä¸ªå‡½æ•°çš„æ‰§è¡Œæ—¶é—´
# - èµ„æºåˆ†é…æƒ…å†µ
```

### 7.3 ä¿å­˜ä¸­é—´ç»“æœ

```python
# åœ¨ _train_step ä¸­ä¿å­˜ä¸­é—´æ•°æ®
def _train_step(self, batch):
    rollout_output = self.actor_rollout_wg.generate_sequences(batch)

    # ä¿å­˜åˆ°æ–‡ä»¶
    torch.save({
        'responses': rollout_output.batch['responses'],
        'old_log_probs': rollout_output.batch['old_log_probs'],
    }, f'debug_step_{self.global_step}.pt')

    # åç»­å¯ä»¥åŠ è½½åˆ†æ
```

---

## 8. å¸¸è§é—®é¢˜

### Q1: Actor å’Œ Rollout å¦‚ä½•å…±äº« GPUï¼Ÿ

é€šè¿‡ **HybridEngine** å’Œ **ShardingManager**ï¼š
- è®­ç»ƒæ—¶ï¼šæƒé‡åœ¨ FSDP æ ¼å¼
- æ¨ç†æ—¶ï¼šè‡ªåŠ¨ reshard åˆ° vLLM æ ¼å¼
- æ— éœ€æ‰‹åŠ¨æ‹·è´æƒé‡

è¯¦è§ï¼š`verl/workers/sharding_manager/`

### Q2: WorkerGroup æ˜¯å¦‚ä½•å¹¶è¡Œçš„ï¼Ÿ

Ray è‡ªåŠ¨ç®¡ç†ï¼š
```python
# å½“è°ƒç”¨ worker_group.generate_sequences() æ—¶
# Ray ä¼šå¹¶è¡Œè°ƒç”¨æ‰€æœ‰ worker çš„æ–¹æ³•
# æ¯ä¸ª worker å¤„ç†ä¸€éƒ¨åˆ†æ•°æ®
```

### Q3: è®­ç»ƒä¸ºä»€ä¹ˆè¿™ä¹ˆæ…¢ï¼Ÿ

ä¸»è¦ç“¶é¢ˆï¼š
1. **Rollout é˜¶æ®µ**ï¼švLLM ç”Ÿæˆå“åº”ï¼ˆæœ€æ…¢ï¼‰
2. **Weight Resharding**ï¼šè®­ç»ƒâ†”æ¨ç†æƒé‡è½¬æ¢
3. **æ•°æ®ä¼ è¾“**ï¼šWorker ä¹‹é—´çš„æ•°æ®é€šä¿¡

ä¼˜åŒ–æ–¹æ³•ï¼š
- å¢åŠ  GPU æ•°é‡
- å‡å° response é•¿åº¦
- ä½¿ç”¨æ›´å¿«çš„æ¨ç†å¼•æ“ï¼ˆSGLangï¼‰

---

## 9. æ€»ç»“

RayPPOTrainer çš„æ ¸å¿ƒæµç¨‹ï¼š

```
åˆå§‹åŒ– â†’ åˆ›å»ºèµ„æºæ±  â†’ åˆ›å»º WorkerGroup
    â†“
ä¸»å¾ªç¯ â†’ éå† DataLoader
    â†“
å•æ­¥è®­ç»ƒ â†’ Rollout â†’ Reward â†’ Advantage â†’ Update
    â†“
è®°å½•æŒ‡æ ‡ â†’ ä¿å­˜ Checkpoint â†’ éªŒè¯
```

ç†è§£è¿™ä¸ªæµç¨‹åï¼Œä½ å°±æŒæ¡äº† verl è®­ç»ƒçš„æ ¸å¿ƒï¼

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [Single Controller æ¶æ„](../é…ç½®ç³»ç»Ÿè¯¦è§£.md)
- [Reward ç³»ç»Ÿè¯¦è§£](../../02_æ•°æ®å‡†å¤‡/reward_ç³»ç»Ÿè¯¦è§£.md)
- [å®˜æ–¹æ–‡æ¡£ï¼šProgramming Guide](https://verl.readthedocs.io/en/latest/hybrid_flow.html)
