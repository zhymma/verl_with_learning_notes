# 01 - å¿«é€Ÿä¸Šæ‰‹ï¼šç¬¬ä¸€æ¬¡è¿è¡Œ verl

> ç›®æ ‡ï¼šå®Œæˆç¯å¢ƒå®‰è£…ï¼Œè·‘é€šç¬¬ä¸€ä¸ª GRPO è®­ç»ƒç¤ºä¾‹ï¼Œç†è§£è®­ç»ƒè¿‡ç¨‹

---

## ğŸ“‹ å‡†å¤‡å·¥ä½œæ£€æŸ¥æ¸…å•

åœ¨å¼€å§‹ä¹‹å‰ï¼Œç¡®ä¿ä½ æœ‰ï¼š
- [ ] è‡³å°‘ 1 å¼  NVIDIA GPUï¼ˆæ¨è 24GB+ æ˜¾å­˜ï¼‰
- [ ] Python 3.9+ ç¯å¢ƒ
- [ ] èƒ½å¤Ÿè®¿é—® HuggingFaceï¼ˆæˆ–ä½¿ç”¨ ModelScope é•œåƒï¼‰
- [ ] çº¦ 50GB ç£ç›˜ç©ºé—´ï¼ˆæ¨¡å‹ + æ•°æ® + checkpointï¼‰

---

## 1. ç¯å¢ƒå®‰è£…

### 1.1 å®‰è£… verl

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd ~/projects/agents-a03e385ef6

# å®‰è£… verl + vLLMï¼ˆæ¨èå…¥é—¨ä½¿ç”¨ï¼‰
pip install -e .[test,vllm]

# å¦‚æœä½ æƒ³ç”¨ SGLangï¼ˆæ›´é€‚åˆ Agent ä»»åŠ¡ï¼‰ï¼Œå¯ä»¥å®‰è£…ï¼š
# pip install -e .[test,sglang]
```

**è¯´æ˜ï¼š**
- `-e` è¡¨ç¤ºå¯ç¼–è¾‘æ¨¡å¼å®‰è£…ï¼Œæ–¹ä¾¿åç»­ä¿®æ”¹ä»£ç 
- `[test,vllm]` æ˜¯å¯é€‰ä¾èµ–ï¼ŒåŒ…å«æµ‹è¯•å·¥å…·å’Œ vLLM æ¨ç†å¼•æ“
- vLLM æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„ LLM æ¨ç†å¼•æ“ï¼Œç”¨äºç”Ÿæˆé˜¶æ®µ

### 1.2 éªŒè¯å®‰è£…

```bash
# éªŒè¯ verl å¯¼å…¥
python -c "import verl; print('verl:', verl.__file__)"

# éªŒè¯ Rayï¼ˆåˆ†å¸ƒå¼æ¡†æ¶ï¼‰
python -c "import ray; ray.init(); print('Ray version:', ray.__version__); ray.shutdown()"

# éªŒè¯ vLLM
python -c "import vllm; print('vLLM version:', vllm.__version__)"

# éªŒè¯ PyTorch å’Œ CUDA
python -c "import torch; print('PyTorch:', torch.__version__); print('CUDA available:', torch.cuda.is_available()); print('GPU count:', torch.cuda.device_count())"
```

**é¢„æœŸè¾“å‡ºç¤ºä¾‹ï¼š**
```
verl: C:\Users\...\agents-a03e385ef6\verl\__init__.py
Ray version: 2.x.x
vLLM version: 0.6.x
PyTorch: 2.x.x+cu118
CUDA available: True
GPU count: 8
```

### 1.3 å¸¸è§å®‰è£…é—®é¢˜

**é—®é¢˜1ï¼švLLM å®‰è£…å¤±è´¥**
```bash
# è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨ --no-build-isolation
pip install vllm --no-build-isolation

# æˆ–è€…æŒ‡å®šç‰ˆæœ¬
pip install vllm==0.6.3.post1
```

**é—®é¢˜2ï¼šCUDA ç‰ˆæœ¬ä¸åŒ¹é…**
```bash
# æŸ¥çœ‹ CUDA ç‰ˆæœ¬
nvidia-smi

# å®‰è£…å¯¹åº”çš„ PyTorchï¼Œä¾‹å¦‚ CUDA 11.8ï¼š
pip install torch==2.1.0+cu118 -f https://download.pytorch.org/whl/torch_stable.html
```

---

## 2. ä¸‹è½½æ¨¡å‹

### 2.1 é€‰æ‹©æ¨¡å‹

å¯¹äºå…¥é—¨å­¦ä¹ ï¼Œæ¨èä½¿ç”¨ **Qwen2.5-7B-Instruct**ï¼š
- 7B å‚æ•°é‡ï¼Œå•å¼  A100/A6000 å¯ä»¥è®­ç»ƒ
- æ•°å­¦èƒ½åŠ›å¼ºï¼Œé€‚åˆ GSM8K ä»»åŠ¡
- æ”¯æŒä¸­è‹±åŒè¯­

### 2.2 ä¸‹è½½æ–¹å¼

#### æ–¹å¼ä¸€ï¼šHuggingFace CLIï¼ˆå›½å¤–ï¼‰

```bash
# åˆ›å»ºæ¨¡å‹ç›®å½•
mkdir -p ~/models

# ä¸‹è½½æ¨¡å‹
huggingface-cli download Qwen/Qwen2.5-7B-Instruct --local-dir ~/models/Qwen2.5-7B-Instruct

# ä¸‹è½½è¿›åº¦ä¼šæ˜¾ç¤ºï¼Œå¤§çº¦éœ€è¦ 15-30 åˆ†é’Ÿ
```

#### æ–¹å¼äºŒï¼šModelScopeï¼ˆå›½å†…æ¨èï¼‰

```bash
# å®‰è£… modelscope
pip install modelscope

# ä¸‹è½½æ¨¡å‹
python -c "
from modelscope import snapshot_download
snapshot_download('Qwen/Qwen2.5-7B-Instruct', cache_dir='~/models')
"
```

#### æ–¹å¼ä¸‰ï¼šæ‰‹åŠ¨ä¸‹è½½

è®¿é—® https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/tree/main
ä¸‹è½½æ‰€æœ‰æ–‡ä»¶åˆ° `~/models/Qwen2.5-7B-Instruct/`

### 2.3 éªŒè¯æ¨¡å‹

```bash
# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
ls ~/models/Qwen2.5-7B-Instruct/

# åº”è¯¥çœ‹åˆ°ï¼š
# - config.json
# - model.safetensorsï¼ˆæˆ– pytorch_model.binï¼‰
# - tokenizer.json
# - tokenizer_config.json
# - special_tokens_map.json
```

### 2.4 æµ‹è¯•æ¨¡å‹åŠ è½½

```python
# test_model.py
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_path = "~/models/Qwen2.5-7B-Instruct"

print("åŠ è½½ tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

print("åŠ è½½æ¨¡å‹...")
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

# ç®€å•æµ‹è¯•
prompt = "1 + 1 = ?"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=20)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(f"\næé—®: {prompt}")
print(f"å›ç­”: {response}")
```

```bash
# è¿è¡Œæµ‹è¯•
python test_model.py
```

---

## 3. å‡†å¤‡æ•°æ®

### 3.1 ä¸‹è½½ GSM8K æ•°æ®é›†

GSM8K æ˜¯ä¸€ä¸ªå°å­¦æ•°å­¦é¢˜æ•°æ®é›†ï¼ŒåŒ…å« 8.5K è®­ç»ƒæ ·æœ¬å’Œ 1.3K æµ‹è¯•æ ·æœ¬ã€‚

```bash
# åˆ›å»ºæ•°æ®ç›®å½•
mkdir -p ~/data/gsm8k

# è¿è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬
python examples/data_preprocess/gsm8k.py --local_dir ~/data/gsm8k

# è„šæœ¬ä¼šä¸‹è½½å¹¶å¤„ç†æ•°æ®ï¼Œç”Ÿæˆ Parquet æ–‡ä»¶
```

**è„šæœ¬ä½ç½®ï¼š** `examples/data_preprocess/gsm8k.py`

### 3.2 ç†è§£æ•°æ®æ ¼å¼

è®©æˆ‘ä»¬æŸ¥çœ‹ç”Ÿæˆçš„æ•°æ®ï¼š

```python
# check_data.py
import pandas as pd

# è¯»å–è®­ç»ƒæ•°æ®
train_df = pd.read_parquet("~/data/gsm8k/train.parquet")

print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_df)}")
print(f"\nå­—æ®µåç§°: {train_df.columns.tolist()}")
print(f"\nç¬¬ä¸€ä¸ªæ ·æœ¬:")
print(train_df.iloc[0])

# æŸ¥çœ‹æ›´å¤šæ ·æœ¬
print("\n\n=== å‰ 3 ä¸ªæ ·æœ¬ ===")
for idx in range(3):
    row = train_df.iloc[idx]
    print(f"\næ ·æœ¬ {idx + 1}:")
    print(f"é—®é¢˜: {row['prompt'][:100]}...")
    if 'reward_model' in row:
        print(f"ç­”æ¡ˆ: {row['reward_model'].get('ground_truth', 'N/A')}")
```

```bash
python check_data.py
```

**é¢„æœŸè¾“å‡ºï¼š**
```
è®­ç»ƒé›†æ ·æœ¬æ•°: 7473

å­—æ®µåç§°: ['data_source', 'prompt', 'ability', 'reward_model']

æ ·æœ¬ 1:
é—®é¢˜: Natalie's father has saved up $10,000 to split up between his kids...
ç­”æ¡ˆ: 2500
```

### 3.3 æ•°æ®æ ¼å¼è¯´æ˜

GSM8K æ•°æ®çš„ Parquet æ ¼å¼ï¼š

```python
{
    "data_source": "gsm8k",              # æ•°æ®æ¥æºæ ‡è¯†
    "prompt": "é—®é¢˜æ–‡æœ¬...",              # æ•°å­¦é¢˜ç›®
    "ability": "math",                   # èƒ½åŠ›ç±»å‹
    "reward_model": {                    # ç”¨äº reward è®¡ç®—çš„ä¿¡æ¯
        "ground_truth": "42",            # æ ‡å‡†ç­”æ¡ˆï¼ˆæ•°å­—ï¼‰
        "solution": "å®Œæ•´è§£é¢˜è¿‡ç¨‹..."     # å®Œæ•´çš„è§£é¢˜æ­¥éª¤
    }
}
```

**å…³é”®å­—æ®µï¼š**
- `prompt`: å¿…éœ€ï¼Œæ¨¡å‹çš„è¾“å…¥
- `reward_model.ground_truth`: ç”¨äºè®¡ç®— reward çš„æ ‡å‡†ç­”æ¡ˆ
- `data_source`: å¸®åŠ© reward å‡½æ•°è¯†åˆ«æ•°æ®ç±»å‹

**ä»£ç ä½ç½®åˆ†æï¼š**
```
examples/data_preprocess/gsm8k.py
â”œâ”€â”€ ç¬¬ 50-80 è¡Œ: ä¸‹è½½ GSM8K æ•°æ®é›†
â”œâ”€â”€ ç¬¬ 100-150 è¡Œ: æ ¼å¼åŒ–ä¸º verl éœ€è¦çš„æ ¼å¼
â””â”€â”€ ç¬¬ 180-200 è¡Œ: ä¿å­˜ä¸º Parquet
```

---

## 4. ç¬¬ä¸€æ¬¡è®­ç»ƒ

### 4.1 ç†è§£è®­ç»ƒå‘½ä»¤

è®©æˆ‘ä»¬ä»æœ€ç®€å•çš„ GRPO è®­ç»ƒå¼€å§‹ï¼š

```bash
# åŸºç¡€è®­ç»ƒå‘½ä»¤
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    data.train_files="['~/data/gsm8k/train.parquet']" \
    data.val_files="['~/data/gsm8k/test.parquet']" \
    actor_rollout_ref.model.path=~/models/Qwen2.5-7B-Instruct \
    actor_rollout_ref.rollout.name=vllm \
    trainer.total_epochs=3 \
    trainer.logger='["tensorboard"]' \
    trainer.n_gpus_per_node=1
```

**å‚æ•°è§£é‡Šï¼š**

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `algorithm.adv_estimator=grpo` | ä½¿ç”¨ GRPO ç®—æ³•ï¼ˆæœ€ç®€å•çš„ RL ç®—æ³•ï¼‰ | - |
| `data.train_files` | è®­ç»ƒæ•°æ®è·¯å¾„ | - |
| `data.val_files` | éªŒè¯æ•°æ®è·¯å¾„ | - |
| `actor_rollout_ref.model.path` | æ¨¡å‹è·¯å¾„ | - |
| `actor_rollout_ref.rollout.name` | æ¨ç†å¼•æ“ï¼ˆvllm æˆ– sglangï¼‰ | vllm |
| `trainer.total_epochs` | è®­ç»ƒè½®æ•° | 20 |
| `trainer.logger` | æ—¥å¿—å·¥å…· | ["console"] |
| `trainer.n_gpus_per_node` | æ¯ä¸ªèŠ‚ç‚¹çš„ GPU æ•° | 8 |

### 4.2 å®Œæ•´è®­ç»ƒè„šæœ¬

åˆ›å»ºè®­ç»ƒè„šæœ¬ä»¥ä¾¿é‡å¤ä½¿ç”¨ï¼š

```bash
# run_first_training.sh
#!/bin/bash

# è®¾ç½®è·¯å¾„
MODEL_PATH=~/models/Qwen2.5-7B-Instruct
DATA_DIR=~/data/gsm8k

# è¿è¡Œè®­ç»ƒ
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    algorithm.group_size=4 \
    algorithm.kl_penalty=0.001 \
    data.train_files="['${DATA_DIR}/train.parquet']" \
    data.val_files="['${DATA_DIR}/test.parquet']" \
    data.train_batch_size=256 \
    data.max_prompt_length=512 \
    data.max_response_length=512 \
    actor_rollout_ref.model.path=${MODEL_PATH} \
    actor_rollout_ref.actor.optim.lr=1e-6 \
    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 \
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \
    actor_rollout_ref.rollout.temperature=0.6 \
    trainer.n_gpus_per_node=1 \
    trainer.nnodes=1 \
    trainer.total_epochs=3 \
    trainer.save_freq=1 \
    trainer.test_freq=1 \
    trainer.logger='["console","tensorboard"]' \
    trainer.project_name=my_first_verl_training \
    trainer.experiment_name=grpo_gsm8k_qwen7b
```

```bash
# èµ‹äºˆæ‰§è¡Œæƒé™
chmod +x run_first_training.sh

# è¿è¡Œ
./run_first_training.sh
```

### 4.3 è®­ç»ƒè¿‡ç¨‹è§£è¯»

**è®­ç»ƒå…¥å£ç‚¹ï¼š** `verl/trainer/main_ppo.py`

```python
# verl/trainer/main_ppo.py (ç®€åŒ–ç‰ˆ)
# ç¬¬ 50-100 è¡Œï¼š
def main(config):
    # 1. åˆå§‹åŒ– Ray åˆ†å¸ƒå¼ç¯å¢ƒ
    ray.init()

    # 2. åˆ›å»º RayPPOTrainer
    trainer = RayPPOTrainer(config)

    # 3. å¼€å§‹è®­ç»ƒ
    trainer.fit()
```

**è®­ç»ƒå¾ªç¯ï¼š** `verl/trainer/ppo/ray_trainer.py`

è®­ç»ƒæµç¨‹ï¼ˆæ¯ä¸ª stepï¼‰ï¼š
```
1. Rollout ç”Ÿæˆ
   â†“ ä»æ•°æ®é›†é‡‡æ · prompts
   â†“ vLLM ç”Ÿæˆ responses

2. Reward è®¡ç®—
   â†“ è°ƒç”¨ reward å‡½æ•°ï¼ˆGSM8K æå–ç­”æ¡ˆå¹¶æ¯”è¾ƒï¼‰

3. Advantage ä¼°è®¡
   â†“ GRPO: ç»„å†…ç›¸å¯¹ä¼˜åŠ¿

4. Actor æ›´æ–°
   â†“ PPO loss æ›´æ–°ç­–ç•¥

5. ä¿å­˜ checkpoint & è®°å½•æŒ‡æ ‡
```

### 4.4 è§‚å¯Ÿè®­ç»ƒæ—¥å¿—

è®­ç»ƒå¼€å§‹åï¼Œä½ ä¼šçœ‹åˆ°ç±»ä¼¼çš„è¾“å‡ºï¼š

```
[2026-01-25 10:00:00] Initializing Ray...
[2026-01-25 10:00:05] Ray cluster started with 1 nodes, 8 GPUs
[2026-01-25 10:00:10] Creating RayPPOTrainer...
[2026-01-25 10:00:30] Loading model from ~/models/Qwen2.5-7B-Instruct
[2026-01-25 10:01:00] Model loaded successfully
[2026-01-25 10:01:05] Starting training...

Epoch 1/3:
  Step 1/29: reward_mean=0.15, kl_mean=0.05, actor_loss=2.31 [15.2s]
  Step 2/29: reward_mean=0.18, kl_mean=0.08, actor_loss=2.25 [14.8s]
  Step 3/29: reward_mean=0.22, kl_mean=0.12, actor_loss=2.18 [15.0s]
  ...
```

**é‡è¦æŒ‡æ ‡è§£è¯»ï¼š**

| æŒ‡æ ‡ | å«ä¹‰ | ç†æƒ³è¶‹åŠ¿ |
|------|------|----------|
| `reward_mean` | å¹³å‡ rewardï¼ˆ0-1ï¼‰ | æŒç»­ä¸Šå‡ |
| `accuracy` | å‡†ç¡®ç‡ | æŒç»­ä¸Šå‡ |
| `kl_mean` | KL æ•£åº¦ | ä¿æŒåœ¨ 0.1-5 ä¹‹é—´ |
| `actor_loss` | Actor æŸå¤± | é€æ­¥ä¸‹é™ |
| `response_length/mean` | å¹³å‡å“åº”é•¿åº¦ | ä¿æŒåˆç†èŒƒå›´ |

---

## 5. ä½¿ç”¨ TensorBoard ç›‘æ§

### 5.1 å¯åŠ¨ TensorBoard

```bash
# åœ¨æ–°çš„ç»ˆç«¯çª—å£
tensorboard --logdir=./outputs --port=6006

# è¾“å‡ºç±»ä¼¼ï¼š
# TensorBoard 2.x at http://localhost:6006/ (Press CTRL+C to quit)
```

åœ¨æµè§ˆå™¨æ‰“å¼€ http://localhost:6006

### 5.2 é‡è¦å›¾è¡¨

TensorBoard ä¸­æŸ¥çœ‹çš„å…³é”®å›¾è¡¨ï¼š

**1. Reward æ›²çº¿**ï¼ˆæœ€é‡è¦ï¼‰
- ä½ç½®ï¼šSCALARS â†’ reward/mean
- æœŸæœ›ï¼šæŒç»­ä¸Šå‡
- è­¦å‘Šï¼šå¦‚æœçªç„¶ä¸‹é™ï¼Œå¯èƒ½æ˜¯å­¦ä¹ ç‡å¤ªå¤§

**2. Accuracy æ›²çº¿**
- ä½ç½®ï¼šSCALARS â†’ accuracy
- GSM8K ä»»åŠ¡ä¸­ï¼Œaccuracy åº”è¯¥ä»åˆå§‹çš„ 10-20% æå‡åˆ° 40-60%

**3. KL Divergence**
- ä½ç½®ï¼šSCALARS â†’ kl/mean
- æœŸæœ›ï¼šå°äº 5ï¼ˆå¦åˆ™åç¦»åŸå§‹æ¨¡å‹å¤ªè¿œï¼‰

**4. Loss æ›²çº¿**
- ä½ç½®ï¼šSCALARS â†’ actor/loss
- æœŸæœ›ï¼šé€æ­¥ä¸‹é™å¹¶è¶‹äºç¨³å®š

### 5.3 ä¿å­˜é‡è¦å›¾è¡¨

åœ¨ TensorBoard ç•Œé¢ï¼š
- ç‚¹å‡»å³ä¸Šè§’çš„ä¸‹è½½å›¾æ ‡å¯ä»¥å¯¼å‡ºæ•°æ®
- æˆ–æˆªå›¾ä¿å­˜é‡è¦çš„è®­ç»ƒæ›²çº¿

---

## 6. è¯„ä¼°æ¨¡å‹

### 6.1 æŸ¥çœ‹ checkpoint

è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹ä¿å­˜åœ¨ï¼š

```bash
ls outputs/

# ç›®å½•ç»“æ„ï¼š
outputs/
â”œâ”€â”€ checkpoint-epoch-1/
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ model.safetensors
â”‚   â””â”€â”€ ...
â”œâ”€â”€ checkpoint-epoch-2/
â””â”€â”€ checkpoint-epoch-3/
```

### 6.2 æ‰‹åŠ¨æµ‹è¯•æ¨¡å‹

```python
# test_trained_model.py
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# åŠ è½½è®­ç»ƒåçš„æ¨¡å‹
model_path = "outputs/checkpoint-epoch-3"
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

# æµ‹è¯•é¢˜ç›®
test_prompts = [
    "If a car travels 60 miles per hour for 2.5 hours, how far does it travel?",
    "A store has 150 apples. If 60% are sold, how many apples are left?",
    "What is 15% of 200?",
]

print("=== æµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹ ===\n")
for prompt in test_prompts:
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=256,
        temperature=0.6,
        do_sample=True,
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    print(f"é—®é¢˜: {prompt}")
    print(f"å›ç­”: {response[len(prompt):]}\n")
    print("-" * 80 + "\n")
```

```bash
python test_trained_model.py
```

### 6.3 æ‰¹é‡è¯„ä¼°

```python
# evaluate.py
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import re

def extract_answer(text):
    """ä»å“åº”ä¸­æå–æ•°å­—ç­”æ¡ˆ"""
    # åŒ¹é…æœ€åä¸€ä¸ªæ•°å­—
    numbers = re.findall(r'-?\d+\.?\d*', text)
    if numbers:
        return numbers[-1]
    return None

# åŠ è½½æ¨¡å‹
model_path = "outputs/checkpoint-epoch-3"
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

# åŠ è½½æµ‹è¯•é›†
test_df = pd.read_parquet("~/data/gsm8k/test.parquet")

# è¯„ä¼°å‰ 100 ä¸ªæ ·æœ¬
correct = 0
total = 100

print("å¼€å§‹è¯„ä¼°...")
for idx in range(total):
    row = test_df.iloc[idx]
    prompt = row['prompt']
    ground_truth = row['reward_model']['ground_truth']

    # ç”Ÿæˆ
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.6)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # æå–ç­”æ¡ˆ
    predicted = extract_answer(response)

    # æ¯”è¾ƒ
    if predicted == str(ground_truth):
        correct += 1

    if (idx + 1) % 10 == 0:
        print(f"å·²è¯„ä¼° {idx + 1}/{total}, å½“å‰å‡†ç¡®ç‡: {correct/(idx+1):.2%}")

print(f"\næœ€ç»ˆå‡†ç¡®ç‡: {correct/total:.2%}")
```

```bash
python evaluate.py
```

---

## 7. å¸¸è§é—®é¢˜æ’æŸ¥

### é—®é¢˜1: CUDA Out of Memory

**ç—‡çŠ¶ï¼š**
```
RuntimeError: CUDA out of memory. Tried to allocate X MB
```

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# æ–¹æ¡ˆ1: å‡å° batch size
data.train_batch_size=128
actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=2

# æ–¹æ¡ˆ2: å‡å°æ¨ç†æ˜¾å­˜
actor_rollout_ref.rollout.gpu_memory_utilization=0.3

# æ–¹æ¡ˆ3: å¼€å¯ gradient checkpointing
actor_rollout_ref.model.enable_gradient_checkpointing=true
```

### é—®é¢˜2: Reward ä¸€ç›´æ˜¯ 0

**æ£€æŸ¥ç‚¹ï¼š**

1. æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®
```python
# æ£€æŸ¥æ•°æ®
import pandas as pd
df = pd.read_parquet("~/data/gsm8k/train.parquet")
print(df.iloc[0]['reward_model'])  # åº”è¯¥æœ‰ ground_truth
```

2. Reward å‡½æ•°æ˜¯å¦è¢«è°ƒç”¨
```bash
# æŸ¥çœ‹æ—¥å¿—ä¸­æ˜¯å¦æœ‰ reward è®¡ç®—ç›¸å…³è¾“å‡º
grep -i "reward" outputs/training.log
```

### é—®é¢˜3: Ray åˆå§‹åŒ–å¤±è´¥

**è§£å†³ï¼š**
```bash
# åœæ­¢æ‰€æœ‰ Ray è¿›ç¨‹
ray stop --force

# é‡æ–°è¿è¡Œè®­ç»ƒ
./run_first_training.sh
```

### é—®é¢˜4: è®­ç»ƒé€Ÿåº¦å¾ˆæ…¢

**æ£€æŸ¥ï¼š**
1. vLLM æ˜¯å¦æ­£ç¡®ä½¿ç”¨ GPU
2. batch size æ˜¯å¦å¤ªå°
3. æ•°æ®åŠ è½½æ˜¯å¦æˆä¸ºç“¶é¢ˆ

```bash
# æŸ¥çœ‹ GPU ä½¿ç”¨ç‡
watch -n 1 nvidia-smi
```

---

## 8. ä¸‹ä¸€æ­¥

å®Œæˆç¬¬ä¸€æ¬¡è®­ç»ƒåï¼Œä½ åº”è¯¥ï¼š

âœ… ç†è§£äº† verl çš„åŸºæœ¬å·¥ä½œæµç¨‹
âœ… ä¼šä½¿ç”¨ TensorBoard ç›‘æ§è®­ç»ƒ
âœ… èƒ½å¤Ÿè¯„ä¼°è®­ç»ƒåçš„æ¨¡å‹

**æ¥ä¸‹æ¥å­¦ä¹ ï¼š**
1. ğŸ“Š **æ•°æ®å‡†å¤‡**ï¼šå­¦ä¹ å¦‚ä½•å‡†å¤‡è‡ªå·±çš„æ•°æ®é›†
2. ğŸ§  **RL ç®—æ³•**ï¼šæ·±å…¥ç†è§£ GRPOã€PPO ç­‰ç®—æ³•
3. ğŸ¯ **Reward è®¾è®¡**ï¼šå®ç°è‡ªå®šä¹‰ reward å‡½æ•°

---

## ğŸ“š ç›¸å…³èµ„æº

**ä»£ç ä½ç½®é€ŸæŸ¥ï¼š**
- è®­ç»ƒå…¥å£ï¼š`verl/trainer/main_ppo.py`
- è®­ç»ƒä¸»å¾ªç¯ï¼š`verl/trainer/ppo/ray_trainer.py`
- GRPO ç®—æ³•ï¼š`verl/trainer/ppo/core_algos.py` (ç¬¬ 200-300 è¡Œ)
- æ•°æ®é¢„å¤„ç†ï¼š`examples/data_preprocess/gsm8k.py`

**å®˜æ–¹æ–‡æ¡£ï¼š**
- Quickstart: https://verl.readthedocs.io/en/latest/start/quickstart.html
- GRPO ç®—æ³•: https://verl.readthedocs.io/en/latest/algo/grpo.html

**ç¤ºä¾‹è„šæœ¬ï¼š**
- `examples/grpo_trainer/run_qwen2-7b.sh`
- `examples/grpo_trainer/run_qwen2_5-3b_gsm8k_grpo_lora.sh`
