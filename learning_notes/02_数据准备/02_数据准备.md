# 02 - æ•°æ®å‡†å¤‡ï¼šä¸º RL è®­ç»ƒå‡†å¤‡é«˜è´¨é‡æ•°æ®

> ç›®æ ‡ï¼šç†è§£ verl çš„æ•°æ®æ ¼å¼ï¼Œå­¦ä¼šå‡†å¤‡å•è½®ã€å¤šè½®å’Œå¤šæ¨¡æ€æ•°æ®

---

## ğŸ“‹ æœ¬ç« æ¦‚è§ˆ

æ•°æ®æ˜¯ RL è®­ç»ƒçš„åŸºç¡€ã€‚æœ¬ç« å°†æ•™ä½ ï¼š
- âœ… ç†è§£ verl çš„ Parquet æ•°æ®æ ¼å¼
- âœ… å‡†å¤‡å•è½®å¯¹è¯æ•°æ®
- âœ… å‡†å¤‡å¤šè½®å¯¹è¯æ•°æ®ï¼ˆAgent è®­ç»ƒï¼‰
- âœ… å‡†å¤‡å¤šæ¨¡æ€æ•°æ®ï¼ˆVLM è®­ç»ƒï¼‰
- âœ… å®ç°æ•°æ®è´¨é‡æ£€æŸ¥å·¥å…·
- âœ… åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†

---

## 1. verl æ•°æ®æ ¼å¼è¯¦è§£

### 1.1 ä¸ºä»€ä¹ˆä½¿ç”¨ Parquet æ ¼å¼ï¼Ÿ

**Parquet çš„ä¼˜åŠ¿ï¼š**
- åˆ—å¼å­˜å‚¨ï¼ŒæŸ¥è¯¢æ•ˆç‡é«˜
- å†…ç½®å‹ç¼©ï¼ŒèŠ‚çœç£ç›˜ç©ºé—´
- æ”¯æŒå¤æ‚æ•°æ®ç»“æ„ï¼ˆåµŒå¥—å­—æ®µï¼‰
- pandas åŸç”Ÿæ”¯æŒï¼Œæ˜“äºå¤„ç†

### 1.2 æ ‡å‡†æ•°æ®æ ¼å¼

verl è¦æ±‚æ•°æ®åŒ…å«ä»¥ä¸‹å­—æ®µï¼š

```python
{
    # ========== å¿…éœ€å­—æ®µ ==========
    "data_source": "gsm8k",           # æ•°æ®æ¥æºæ ‡è¯†ï¼Œç”¨äº reward å‡½æ•°è·¯ç”±
    "prompt": [...],                  # ç”¨æˆ·è¾“å…¥ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰

    # ========== å¯é€‰ä½†æ¨è ==========
    "ability": "math",                # ä»»åŠ¡èƒ½åŠ›ç±»åˆ«
    "reward_model": {                 # Reward è®¡ç®—æ‰€éœ€ä¿¡æ¯
        "style": "rule",              # rule æˆ– model
        "ground_truth": "42",         # æ ‡å‡†ç­”æ¡ˆ
    },
    "extra_info": {                   # é¢å¤–å…ƒæ•°æ®
        "split": "train",
        "index": 0,
    }
}
```

### 1.3 Prompt å­—æ®µçš„ä¸åŒæ ¼å¼

#### æ ¼å¼ 1ï¼šå•è½®å¯¹è¯ï¼ˆå­—ç¬¦ä¸²ï¼‰

```python
{
    "prompt": "What is 2 + 2?",
    # ...
}
```

#### æ ¼å¼ 2ï¼šå•è½®å¯¹è¯ï¼ˆChat æ ¼å¼ï¼Œæ¨èï¼‰

```python
{
    "prompt": [
        {"role": "user", "content": "What is 2 + 2?"}
    ],
    # ...
}
```

#### æ ¼å¼ 3ï¼šå¤šè½®å¯¹è¯

```python
{
    "prompt": [
        {"role": "user", "content": "What is 2 + 2?"},
        {"role": "assistant", "content": "4"},
        {"role": "user", "content": "What about 3 + 3?"}
    ],
    # ...
}
```

#### æ ¼å¼ 4ï¼šå¤šæ¨¡æ€ï¼ˆVLMï¼‰

```python
{
    "prompt": [
        {"type": "image", "image": "/path/to/image.jpg"},
        {"type": "text", "text": "What's in this image?"}
    ],
    # ...
}
```

### 1.4 Reward Model å­—æ®µè¯¦è§£

**ä»£ç ä½ç½®ï¼š** `verl/trainer/ppo/reward.py`

```python
# reward_model å­—æ®µçš„ä½œç”¨ï¼š
reward_model = {
    "style": "rule",             # "rule" æˆ– "model"
    "ground_truth": "42",        # æ ‡å‡†ç­”æ¡ˆï¼ˆç”¨äº rule-based rewardï¼‰
    "solution": "å®Œæ•´è§£é¢˜è¿‡ç¨‹",   # å¯é€‰ï¼šå®Œæ•´ç­”æ¡ˆ
    "test_cases": [...],         # å¯é€‰ï¼šä»£ç é¢˜çš„æµ‹è¯•ç”¨ä¾‹
    # å…¶ä»–è‡ªå®šä¹‰å­—æ®µ...
}
```

**å·¥ä½œæµç¨‹ï¼š**
```
1. æ¨¡å‹ç”Ÿæˆå“åº”
   â†“
2. RewardManager è¯»å– data_source
   â†“
3. æ ¹æ® data_source è°ƒç”¨å¯¹åº”çš„ reward å‡½æ•°
   â†“
4. Reward å‡½æ•°ä½¿ç”¨ ground_truth è®¡ç®—åˆ†æ•°
```

**ä»£ç ä½ç½®åˆ†æï¼š**
```
verl/trainer/ppo/reward.py
â”œâ”€â”€ RewardManager ç±» (ç¬¬ 50-100 è¡Œ)
â”‚   â””â”€â”€ __call__() æ–¹æ³•ï¼šè®¡ç®— reward
â”œâ”€â”€ ç¬¬ 120-150 è¡Œï¼šæ ¹æ® data_source è·¯ç”±åˆ°ä¸åŒ reward å‡½æ•°
â””â”€â”€ ç¬¬ 200+ è¡Œï¼šè‡ªå®šä¹‰ reward å‡½æ•°æ”¯æŒ
```

---

## 2. å‡†å¤‡å•è½®å¯¹è¯æ•°æ®

### 2.1 GSM8K æ•°æ®é›†ç¤ºä¾‹

è®©æˆ‘ä»¬æ·±å…¥åˆ†æ GSM8K æ•°æ®é¢„å¤„ç†è„šæœ¬ï¼š

**æ–‡ä»¶ä½ç½®ï¼š** `examples/data_preprocess/gsm8k.py`

```python
# ç¬¬ 27-32 è¡Œï¼šæå–ç­”æ¡ˆçš„å‡½æ•°
def extract_solution(solution_str):
    """ä»åŸå§‹ç­”æ¡ˆä¸­æå–æ•°å­—ç­”æ¡ˆ

    è¾“å…¥: "Let's solve it step by step...\\n#### 1035"
    è¾“å‡º: "1035"
    """
    solution = re.search("#### (\\-?[0-9\\.\\,]+)", solution_str)
    assert solution is not None
    final_solution = solution.group(0)
    final_solution = final_solution.split("#### ")[1].replace(",", "")
    return final_solution
```

```python
# ç¬¬ 60-87 è¡Œï¼šæ•°æ®å¤„ç†å‡½æ•°
def make_map_fn(split):
    def process_fn(example, idx):
        # 1. è·å–åŸå§‹é—®é¢˜
        question_raw = example.pop("question")

        # 2. æ·»åŠ æŒ‡ä»¤ï¼ˆå¼•å¯¼æ¨¡å‹è¾“å‡ºæ ¼å¼ï¼‰
        instruction = 'Let\'s think step by step and output the final answer after "####".'
        question = question_raw + " " + instruction

        # 3. æå–ç­”æ¡ˆ
        answer_raw = example.pop("answer")
        solution = extract_solution(answer_raw)

        # 4. æ„å»º verl æ ¼å¼
        data = {
            "data_source": "openai/gsm8k",
            "prompt": [
                {"role": "user", "content": question}
            ],
            "ability": "math",
            "reward_model": {
                "style": "rule",
                "ground_truth": solution  # åªä¿ç•™æ•°å­—ç­”æ¡ˆ
            },
            "extra_info": {
                "split": split,
                "index": idx,
                "answer": answer_raw,      # ä¿å­˜å®Œæ•´ç­”æ¡ˆä¾›è°ƒè¯•
                "question": question_raw   # ä¿å­˜åŸå§‹é—®é¢˜
            }
        }
        return data

    return process_fn
```

### 2.2 åˆ›å»ºè‡ªå·±çš„å•è½®æ•°æ®é›†

**ç¤ºä¾‹ï¼šä»£ç ç”Ÿæˆä»»åŠ¡**

```python
# prepare_code_data.py
import pandas as pd
import datasets
import json

def make_code_map_fn(split):
    def process_fn(example, idx):
        # 1. æ„å»ºæç¤º
        problem_description = example['description']
        prompt_text = f"""Write a Python function to solve this problem:

{problem_description}

Requirements:
- Function name: {example['function_name']}
- Return type: {example['return_type']}
"""

        # 2. æå–æµ‹è¯•ç”¨ä¾‹
        test_cases = example['test_cases']

        # 3. æ„å»ºæ•°æ®
        data = {
            "data_source": "code_generation",
            "prompt": [
                {"role": "user", "content": prompt_text}
            ],
            "ability": "coding",
            "reward_model": {
                "style": "rule",  # æˆ– "sandbox" å¦‚æœä½¿ç”¨ä»£ç æ‰§è¡Œ
                "ground_truth": example['solution'],
                "test_cases": test_cases,
                "timeout": 5.0,
            },
            "extra_info": {
                "split": split,
                "index": idx,
                "difficulty": example.get('difficulty', 'medium'),
            }
        }
        return data

    return process_fn

# åŠ è½½æ•°æ®
dataset = datasets.load_dataset("your_dataset_name")
train_dataset = dataset['train'].map(
    function=make_code_map_fn('train'),
    with_indices=True
)

# ä¿å­˜ä¸º Parquet
train_dataset.to_parquet("~/data/code_gen/train.parquet")
```

### 2.3 é€šç”¨æ•°æ®å‡†å¤‡æ¨¡æ¿

```python
# template_single_turn.py
import pandas as pd
import argparse

def prepare_single_turn_data(
    input_file,
    output_file,
    data_source_name,
    question_field,
    answer_field
):
    """é€šç”¨çš„å•è½®æ•°æ®å‡†å¤‡æ¨¡æ¿

    Args:
        input_file: åŸå§‹æ•°æ®æ–‡ä»¶ï¼ˆCSV/JSON/Parquetï¼‰
        output_file: è¾“å‡ºçš„ Parquet æ–‡ä»¶
        data_source_name: æ•°æ®é›†åç§°
        question_field: é—®é¢˜å­—æ®µå
        answer_field: ç­”æ¡ˆå­—æ®µå
    """
    # 1. è¯»å–æ•°æ®
    if input_file.endswith('.csv'):
        df = pd.read_csv(input_file)
    elif input_file.endswith('.json'):
        df = pd.read_json(input_file)
    elif input_file.endswith('.parquet'):
        df = pd.read_parquet(input_file)
    else:
        raise ValueError(f"Unsupported file format: {input_file}")

    # 2. è½¬æ¢ä¸º verl æ ¼å¼
    data_list = []
    for idx, row in df.iterrows():
        data = {
            "data_source": data_source_name,
            "prompt": [
                {"role": "user", "content": row[question_field]}
            ],
            "ability": "general",  # æ ¹æ®ä»»åŠ¡ä¿®æ”¹
            "reward_model": {
                "style": "rule",
                "ground_truth": row[answer_field]
            },
            "extra_info": {
                "index": idx
            }
        }
        data_list.append(data)

    # 3. ä¿å­˜
    output_df = pd.DataFrame(data_list)
    output_df.to_parquet(output_file)
    print(f"âœ“ å·²ä¿å­˜ {len(output_df)} æ¡æ•°æ®åˆ° {output_file}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", required=True)
    parser.add_argument("--output", required=True)
    parser.add_argument("--data_source", required=True)
    parser.add_argument("--question_field", default="question")
    parser.add_argument("--answer_field", default="answer")

    args = parser.parse_args()

    prepare_single_turn_data(
        input_file=args.input,
        output_file=args.output,
        data_source_name=args.data_source,
        question_field=args.question_field,
        answer_field=args.answer_field
    )
```

**ä½¿ç”¨æ–¹æ³•ï¼š**
```bash
python template_single_turn.py \
    --input my_data.csv \
    --output my_data.parquet \
    --data_source my_task \
    --question_field prompt \
    --answer_field answer
```

---

## 3. å‡†å¤‡å¤šè½®å¯¹è¯æ•°æ®ï¼ˆAgent è®­ç»ƒï¼‰

### 3.1 å¤šè½®å¯¹è¯æ ¼å¼

å¤šè½®å¯¹è¯æ•°æ®çš„ `prompt` å­—æ®µæ˜¯ä¸€ä¸ªæ¶ˆæ¯åˆ—è¡¨ï¼š

```python
{
    "data_source": "multiturn_qa",
    "prompt": [
        # ç¬¬1è½®
        {"role": "user", "content": "Can you help me with math?"},
        {"role": "assistant", "content": "Of course! What do you need help with?"},

        # ç¬¬2è½®
        {"role": "user", "content": "What is 25 * 4?"},
        {"role": "assistant", "content": "Let me calculate... 25 * 4 = 100"},

        # ç¬¬3è½®ï¼ˆæ¨¡å‹éœ€è¦ç”Ÿæˆè¿™ä¸€è½®çš„å›å¤ï¼‰
        {"role": "user", "content": "Great! Now what about 100 / 5?"}
    ],
    "reward_model": {
        "ground_truth": "20"  # æœ€ç»ˆç­”æ¡ˆ
    }
}
```

### 3.2 GSM8K å¤šè½®æ•°æ®ç¤ºä¾‹

**æ–‡ä»¶ä½ç½®ï¼š** `examples/data_preprocess/gsm8k_multiturn_sft.py`

```python
# ç¬¬ 40-80 è¡Œï¼šæ„å»ºå¤šè½®å¯¹è¯
def make_map_fn_sft(split):
    def process_fn(example, idx):
        question = example['question']
        answer = example['answer']

        # å°†è§£é¢˜è¿‡ç¨‹åˆ†æˆå¤šä¸ªæ­¥éª¤
        steps = split_into_steps(answer)

        # æ„å»ºå¤šè½®å¯¹è¯
        messages = []

        # ç”¨æˆ·æé—®
        messages.append({
            "role": "user",
            "content": question
        })

        # åŠ©æ‰‹é€æ­¥è§£ç­”
        for step in steps:
            messages.append({
                "role": "assistant",
                "content": step
            })

        data = {
            "data_source": "openai/gsm8k",
            "prompt": messages,
            "ability": "math",
            "reward_model": {
                "ground_truth": extract_solution(answer)
            }
        }
        return data

    return process_fn
```

### 3.3 å·¥å…·è°ƒç”¨æ•°æ®æ ¼å¼

**æ–‡ä»¶ä½ç½®ï¼š** `examples/data_preprocess/gsm8k_multiturn_w_tool.py`

```python
# å·¥å…·è°ƒç”¨æ ¼å¼çš„å¤šè½®å¯¹è¯
{
    "data_source": "gsm8k",
    "prompt": [
        {
            "role": "user",
            "content": "Calculate 23 * 45 + 67"
        }
    ],
    "tool_config": {
        "available_tools": ["calculator"],
        "tool_format": "react",  # æˆ– "function_calling"
        "max_tool_calls": 5
    },
    "reward_model": {
        "ground_truth": "1102"
    }
}
```

**Agent è®­ç»ƒæ—¶çš„æ•°æ®æµï¼š**
```
1. ç”¨æˆ·æé—®
   â†“
2. Agent å†³å®šè°ƒç”¨å·¥å…·
   â†“
3. å·¥å…·è¿”å›ç»“æœï¼ˆæ·»åŠ åˆ° messagesï¼‰
   â†“
4. Agent ç»§ç»­ç”Ÿæˆï¼ˆå¯èƒ½å†æ¬¡è°ƒç”¨å·¥å…·ï¼‰
   â†“
5. Agent è¾“å‡ºæœ€ç»ˆç­”æ¡ˆ
   â†“
6. è®¡ç®— Reward
```

---

## 4. å‡†å¤‡å¤šæ¨¡æ€æ•°æ®ï¼ˆVLMï¼‰

### 4.1 å¤šæ¨¡æ€æ•°æ®æ ¼å¼

**æ–‡ä»¶ä½ç½®ï¼š** `examples/data_preprocess/` (VLM ç›¸å…³)

```python
{
    "data_source": "vqa_v2",
    "prompt": [
        {
            "type": "image",
            "image": "/path/to/image.jpg",  # æˆ– base64 ç¼–ç 
            "resolution": [224, 224]         # å¯é€‰
        },
        {
            "type": "text",
            "text": "What objects are in this image?"
        }
    ],
    "ability": "visual_qa",
    "reward_model": {
        "ground_truth": "a cat, a table, a lamp"
    }
}
```

### 4.2 è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ•°æ®å‡†å¤‡

```python
# prepare_vqa_data.py
from PIL import Image
import base64
import io

def image_to_base64(image_path):
    """å°†å›¾ç‰‡è½¬æ¢ä¸º base64 ç¼–ç """
    with Image.open(image_path) as img:
        # å¯é€‰ï¼šè°ƒæ•´å¤§å°
        img = img.resize((224, 224))

        # è½¬æ¢ä¸º base64
        buffer = io.BytesIO()
        img.save(buffer, format='JPEG')
        img_bytes = buffer.getvalue()
        img_base64 = base64.b64encode(img_bytes).decode('utf-8')

    return img_base64

def prepare_vqa_data(vqa_dataset):
    data_list = []

    for item in vqa_dataset:
        # 1. å¤„ç†å›¾ç‰‡
        image_path = item['image_path']
        image_base64 = image_to_base64(image_path)

        # 2. æ„å»º prompt
        question = item['question']
        answer = item['answer']

        data = {
            "data_source": "vqa_v2",
            "prompt": [
                {
                    "type": "image",
                    "image": image_base64,  # æˆ–ç›´æ¥ç”¨è·¯å¾„
                    "image_path": image_path,
                },
                {
                    "type": "text",
                    "text": question
                }
            ],
            "ability": "visual_qa",
            "reward_model": {
                "ground_truth": answer,
                "answer_type": item.get('answer_type', 'other')
            }
        }
        data_list.append(data)

    return data_list
```

---

## 5. æ•°æ®è´¨é‡æ£€æŸ¥

### 5.1 å¿…éœ€çš„æ£€æŸ¥é¡¹

åˆ›å»ºæ•°æ®è´¨é‡æ£€æŸ¥å·¥å…·æ˜¯ç¡®ä¿è®­ç»ƒæˆåŠŸçš„å…³é”®ï¼š

```python
# data_quality_check.py
import pandas as pd
from pathlib import Path

class DataQualityChecker:
    """æ•°æ®è´¨é‡æ£€æŸ¥å·¥å…·"""

    def __init__(self, data_path):
        self.data_path = Path(data_path)
        self.df = None
        self.errors = []
        self.warnings = []

    def load_data(self):
        """åŠ è½½æ•°æ®"""
        try:
            self.df = pd.read_parquet(self.data_path)
            print(f"âœ“ æˆåŠŸåŠ è½½ {len(self.df)} æ¡æ•°æ®")
            return True
        except Exception as e:
            self.errors.append(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return False

    def check_required_fields(self):
        """æ£€æŸ¥å¿…éœ€å­—æ®µ"""
        required_fields = ['data_source', 'prompt']

        for field in required_fields:
            if field not in self.df.columns:
                self.errors.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
            else:
                # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºå€¼
                null_count = self.df[field].isnull().sum()
                if null_count > 0:
                    self.errors.append(
                        f"å­—æ®µ '{field}' æœ‰ {null_count} ä¸ªç©ºå€¼"
                    )

    def check_prompt_format(self):
        """æ£€æŸ¥ prompt æ ¼å¼"""
        for idx, row in self.df.head(100).iterrows():  # æŠ½æ ·æ£€æŸ¥
            prompt = row['prompt']

            # æ£€æŸ¥æ ¼å¼
            if isinstance(prompt, str):
                # å­—ç¬¦ä¸²æ ¼å¼ï¼ˆç®€å•ï¼‰
                if len(prompt) == 0:
                    self.warnings.append(f"æ ·æœ¬ {idx}: prompt ä¸ºç©ºå­—ç¬¦ä¸²")

            elif isinstance(prompt, list):
                # åˆ—è¡¨æ ¼å¼ï¼ˆChatï¼‰
                if len(prompt) == 0:
                    self.errors.append(f"æ ·æœ¬ {idx}: prompt åˆ—è¡¨ä¸ºç©º")

                for msg in prompt:
                    if not isinstance(msg, dict):
                        self.errors.append(
                            f"æ ·æœ¬ {idx}: prompt ä¸­çš„æ¶ˆæ¯ä¸æ˜¯å­—å…¸æ ¼å¼"
                        )
                    elif 'role' not in msg or 'content' not in msg:
                        self.errors.append(
                            f"æ ·æœ¬ {idx}: æ¶ˆæ¯ç¼ºå°‘ role æˆ– content"
                        )
            else:
                self.errors.append(
                    f"æ ·æœ¬ {idx}: prompt æ ¼å¼é”™è¯¯ï¼ˆåº”ä¸º str æˆ– listï¼‰"
                )

    def check_reward_model(self):
        """æ£€æŸ¥ reward_model å­—æ®µ"""
        if 'reward_model' not in self.df.columns:
            self.warnings.append("ç¼ºå°‘ reward_model å­—æ®µï¼ˆè®­ç»ƒæ—¶éœ€è¦ï¼‰")
            return

        for idx, row in self.df.head(100).iterrows():
            reward_info = row.get('reward_model')

            if reward_info is None:
                self.warnings.append(f"æ ·æœ¬ {idx}: reward_model ä¸ºç©º")
            elif isinstance(reward_info, dict):
                if 'ground_truth' not in reward_info:
                    self.warnings.append(
                        f"æ ·æœ¬ {idx}: reward_model ç¼ºå°‘ ground_truth"
                    )
            else:
                self.errors.append(
                    f"æ ·æœ¬ {idx}: reward_model æ ¼å¼é”™è¯¯ï¼ˆåº”ä¸º dictï¼‰"
                )

    def check_statistics(self):
        """ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*60)
        print("æ•°æ®ç»Ÿè®¡")
        print("="*60)

        # æ ·æœ¬æ•°
        print(f"æ€»æ ·æœ¬æ•°: {len(self.df)}")

        # data_source åˆ†å¸ƒ
        if 'data_source' in self.df.columns:
            print(f"\ndata_source åˆ†å¸ƒ:")
            print(self.df['data_source'].value_counts())

        # prompt é•¿åº¦ç»Ÿè®¡
        if 'prompt' in self.df.columns:
            prompt_lengths = []
            for prompt in self.df['prompt']:
                if isinstance(prompt, str):
                    prompt_lengths.append(len(prompt))
                elif isinstance(prompt, list):
                    # è®¡ç®—æ‰€æœ‰æ¶ˆæ¯çš„æ€»é•¿åº¦
                    total_len = sum(
                        len(msg.get('content', ''))
                        for msg in prompt
                        if isinstance(msg, dict)
                    )
                    prompt_lengths.append(total_len)

            if prompt_lengths:
                import numpy as np
                print(f"\nPrompt é•¿åº¦ç»Ÿè®¡:")
                print(f"  æœ€å°: {np.min(prompt_lengths)}")
                print(f"  æœ€å¤§: {np.max(prompt_lengths)}")
                print(f"  å¹³å‡: {np.mean(prompt_lengths):.0f}")
                print(f"  ä¸­ä½æ•°: {np.median(prompt_lengths):.0f}")

    def run_all_checks(self):
        """è¿è¡Œæ‰€æœ‰æ£€æŸ¥"""
        print("="*60)
        print(f"æ£€æŸ¥æ•°æ®æ–‡ä»¶: {self.data_path}")
        print("="*60)
        print()

        if not self.load_data():
            return False

        self.check_required_fields()
        self.check_prompt_format()
        self.check_reward_model()
        self.check_statistics()

        # è¾“å‡ºç»“æœ
        print("\n" + "="*60)
        print("æ£€æŸ¥ç»“æœ")
        print("="*60)

        if self.errors:
            print(f"\nâŒ å‘ç° {len(self.errors)} ä¸ªé”™è¯¯:")
            for error in self.errors:
                print(f"  - {error}")

        if self.warnings:
            print(f"\nâš  å‘ç° {len(self.warnings)} ä¸ªè­¦å‘Š:")
            for warning in self.warnings:
                print(f"  - {warning}")

        if not self.errors and not self.warnings:
            print("\nâœ“ æ•°æ®æ£€æŸ¥å…¨éƒ¨é€šè¿‡ï¼")
            return True
        elif not self.errors:
            print("\nâš  æ•°æ®æ£€æŸ¥é€šè¿‡ï¼Œä½†æœ‰è­¦å‘Š")
            return True
        else:
            print("\nâŒ æ•°æ®æ£€æŸ¥å¤±è´¥")
            return False


if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("ç”¨æ³•: python data_quality_check.py <data_path>")
        sys.exit(1)

    checker = DataQualityChecker(sys.argv[1])
    success = checker.run_all_checks()

    sys.exit(0 if success else 1)
```

---

## 6. æ•°æ®å‡†å¤‡æœ€ä½³å®è·µ

### 6.1 æ•°æ®é‡å»ºè®®

| ä»»åŠ¡ç±»å‹ | æœ€å°æ ·æœ¬æ•° | æ¨èæ ·æœ¬æ•° | è¯´æ˜ |
|---------|-----------|-----------|------|
| æ•°å­¦é¢˜ï¼ˆGSM8Kï¼‰ | 1,000 | 5,000+ | éœ€è¦å……åˆ†è¦†ç›–é¢˜å‹ |
| ä»£ç ç”Ÿæˆ | 2,000 | 10,000+ | éœ€è¦å¤šæ ·çš„é—®é¢˜ |
| RLHF | 10,000 | 50,000+ | éœ€è¦å¤§é‡åå¥½æ•°æ® |
| Agent è®­ç»ƒ | 500 | 2,000+ | å·¥å…·è°ƒç”¨åœºæ™¯æœ‰é™ |

### 6.2 æ•°æ®æ¸…æ´—æŠ€å·§

```python
def clean_data(df):
    """æ•°æ®æ¸…æ´—ç¤ºä¾‹"""

    # 1. ç§»é™¤é‡å¤æ•°æ®
    df = df.drop_duplicates(subset=['prompt'])

    # 2. è¿‡æ»¤è¿‡é•¿/è¿‡çŸ­çš„æ ·æœ¬
    def filter_length(row):
        prompt = row['prompt']
        if isinstance(prompt, str):
            length = len(prompt)
        else:
            length = sum(len(msg.get('content', '')) for msg in prompt)
        return 10 < length < 2000

    df = df[df.apply(filter_length, axis=1)]

    # 3. ç§»é™¤ç­”æ¡ˆä¸ºç©ºçš„æ ·æœ¬
    df = df[df['reward_model'].apply(
        lambda x: x is not None and x.get('ground_truth') not in [None, '', 'N/A']
    )]

    return df
```

### 6.3 æ•°æ®å¢å¼º

```python
def augment_data(df):
    """æ•°æ®å¢å¼ºç¤ºä¾‹"""
    augmented_data = []

    for _, row in df.iterrows():
        # åŸå§‹æ•°æ®
        augmented_data.append(row.to_dict())

        # å˜ä½“1ï¼šæ”¹å†™é—®é¢˜
        if row['data_source'] == 'math':
            # ä¾‹å¦‚ï¼šå°† "What is X?" æ”¹ä¸º "Calculate X"
            original_prompt = row['prompt'][0]['content']
            augmented_prompt = rewrite_question(original_prompt)

            augmented_row = row.to_dict()
            augmented_row['prompt'] = [
                {"role": "user", "content": augmented_prompt}
            ]
            augmented_data.append(augmented_row)

    return pd.DataFrame(augmented_data)
```

---

## 7. å®æˆ˜ç»ƒä¹ 

### ç»ƒä¹  1ï¼šå‡†å¤‡è‡ªå®šä¹‰å•è½®æ•°æ®

åˆ›å»ºä¸€ä¸ªåŒ…å« 100 ä¸ªæ ·æœ¬çš„æ•°å­¦é¢˜æ•°æ®é›†ï¼š

```bash
# 1. ç”Ÿæˆæ•°æ®
python create_custom_math_data.py --num_samples 100 --output custom_math.parquet

# 2. æ£€æŸ¥æ•°æ®
python data_quality_check.py custom_math.parquet

# 3. è®­ç»ƒæµ‹è¯•
python3 -m verl.trainer.main_ppo \
    data.train_files="['custom_math.parquet']" \
    ...
```

### ç»ƒä¹  2ï¼šå‡†å¤‡å¤šè½®å¯¹è¯æ•°æ®

å°†å•è½®å¯¹è¯æ•°æ®è½¬æ¢ä¸ºå¤šè½®æ ¼å¼ï¼š

```bash
python convert_to_multiturn.py \
    --input single_turn.parquet \
    --output multiturn.parquet \
    --num_turns 3
```

### ç»ƒä¹  3ï¼šæ•°æ®åˆ†æ

åˆ†æç°æœ‰æ•°æ®é›†çš„ç‰¹å¾ï¼š

```bash
python analyze_dataset.py \
    --data ~/data/gsm8k/train.parquet \
    --output analysis_report.html
```

---

## 8. ä¸‹ä¸€æ­¥

å®Œæˆæœ¬ç« åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š
- âœ… ç†è§£ verl çš„æ•°æ®æ ¼å¼
- âœ… å‡†å¤‡å„ç§ç±»å‹çš„è®­ç»ƒæ•°æ®
- âœ… å®ç°æ•°æ®è´¨é‡æ£€æŸ¥
- âœ… è¿›è¡Œæ•°æ®æ¸…æ´—å’Œå¢å¼º

**æ¥ä¸‹æ¥å­¦ä¹ ï¼š**
- **03 - RL ç®—æ³•**ï¼šæ·±å…¥ç†è§£ GRPOã€PPO ç­‰ç®—æ³•
- **04 - Reward è®¾è®¡**ï¼šå®ç°è‡ªå®šä¹‰ Reward å‡½æ•°

---

## ğŸ“š ç›¸å…³èµ„æº

**å®˜æ–¹æ–‡æ¡£ï¼š**
- [Prepare Data](https://verl.readthedocs.io/en/latest/preparation/prepare_data.html)
- [Reward Function](https://verl.readthedocs.io/en/latest/preparation/reward_function.html)

**ä»£ç ä½ç½®ï¼š**
- æ•°æ®é¢„å¤„ç†ç¤ºä¾‹ï¼š`examples/data_preprocess/`
  - `gsm8k.py` - GSM8K å•è½®
  - `gsm8k_multiturn_sft.py` - GSM8K å¤šè½®
  - `gsm8k_multiturn_w_tool.py` - GSM8K å·¥å…·è°ƒç”¨
- RewardManagerï¼š`verl/trainer/ppo/reward.py`
- æ•°æ®åŠ è½½ï¼š`verl/utils/dataset/rl_dataset.py`

**å‚è€ƒæ–‡æ¡£ï¼š**
- æœ¬ç« æ‰€æœ‰ä»£ç åŸºäºå®˜æ–¹æ–‡æ¡£ï¼š`docs/preparation/prepare_data.rst`
