# Reward ç³»ç»Ÿè¯¦è§£

> æ·±å…¥ç†è§£ verl çš„ Reward è®¡ç®—æµç¨‹ï¼Œå­¦ä¼šè®¾è®¡å’Œè°ƒè¯• Reward å‡½æ•°

---

## ğŸ“‹ æœ¬æ–‡å†…å®¹

1. Reward ç³»ç»Ÿæ¦‚è§ˆ
2. RewardManager å·¥ä½œæµç¨‹
   - 2.3 Reward Token æ”¾ç½®æœºåˆ¶ä¸ Advantage å¹¿æ’­ â­
3. GSM8K Reward è¯¦ç»†è§£æ
4. è‡ªå®šä¹‰ Reward å‡½æ•°
5. Reward è°ƒè¯•æŠ€å·§
6. å¸¸è§é—®é¢˜è§£å†³

---

## 1. Reward ç³»ç»Ÿæ¦‚è§ˆ

### 1.1 Reward åœ¨ RL è®­ç»ƒä¸­çš„ä½œç”¨

```
è®­ç»ƒæµç¨‹ï¼š
1. Rollout ç”Ÿæˆå“åº”
   â†“
2. Reward å‡½æ•°è¯„åˆ†  â† æˆ‘ä»¬åœ¨è¿™é‡Œï¼
   â†“
3. Advantage è®¡ç®—
   â†“
4. Policy æ›´æ–°
```

**Reward çš„é‡è¦æ€§ï¼š**
- âœ… **å®šä¹‰ç›®æ ‡**ï¼šå‘Šè¯‰æ¨¡å‹ä»€ä¹ˆæ˜¯"å¥½"çš„è¾“å‡º
- âœ… **å¼•å¯¼å­¦ä¹ **ï¼šé«˜ reward â†’ å¼ºåŒ–ï¼Œä½ reward â†’ æŠ‘åˆ¶
- âœ… **å½±å“æ•ˆæœ**ï¼šReward è®¾è®¡ç›´æ¥å†³å®šæœ€ç»ˆæ¨¡å‹è¡Œä¸º

### 1.2 Reward ç±»å‹

verl æ”¯æŒä¸‰ç§ Reward è®¡ç®—æ–¹å¼ï¼š

| ç±»å‹ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ | ç¤ºä¾‹ |
|------|------|---------|------|
| **Rule-based** | åŸºäºè§„åˆ™çš„æ‰“åˆ† | æœ‰æ ‡å‡†ç­”æ¡ˆçš„ä»»åŠ¡ | GSM8K, ä»£ç é¢˜ |
| **Model-based** | ä½¿ç”¨ Reward Model | RLHF, ä¸»è§‚ä»»åŠ¡ | å¯¹è¯è´¨é‡ |
| **Sandbox** | æ‰§è¡Œä»£ç è·å–ç»“æœ | ä»£ç ç”Ÿæˆ | APPS, HumanEval |

---

## 2. RewardManager å·¥ä½œæµç¨‹

### 2.1 RewardManager æ¶æ„

**æ ¸å¿ƒæ–‡ä»¶ï¼š**
- `verl/trainer/ppo/reward.py` - ä¸»å…¥å£
- `verl/workers/reward_manager/` - RewardManager å®ç°
- `verl/utils/reward_score/` - å†…ç½® reward å‡½æ•°

**ç±»å›¾ï¼š**
```
AbstractRewardManager (æŠ½è±¡åŸºç±»)
    â†‘
    â”œâ”€â”€ NaiveRewardManager        # ç®€å•çš„ rule-based
    â”œâ”€â”€ RateLimitedRewardManager  # æ”¯æŒ rate limitï¼ˆAPI è°ƒç”¨ï¼‰
    â””â”€â”€ RewardLoopManager         # å¼‚æ­¥ reward è®¡ç®—
```

### 2.2 RewardManager åˆå§‹åŒ–

**ä»£ç ä½ç½®ï¼š** `verl/trainer/ppo/reward.py: ç¬¬ 99-175 è¡Œ`

```python
# verl/trainer/ppo/reward.py

def load_reward_manager(config, tokenizer, num_examine, **reward_kwargs):
    """åŠ è½½ RewardManager

    ä¸»è¦æ­¥éª¤ï¼š
    1. åŠ è½½è‡ªå®šä¹‰ reward å‡½æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
    2. é€‰æ‹© RewardManager ç±»å‹
    3. å®ä¾‹åŒ– RewardManager
    """

    # ========== æ­¥éª¤ 1: è·å–è‡ªå®šä¹‰ reward å‡½æ•° ==========
    compute_score = get_custom_reward_fn(config)
    # å¦‚æœ config.custom_reward_function.path å­˜åœ¨ï¼Œåˆ™åŠ è½½

    # ========== æ­¥éª¤ 2: å¦‚æœæ²¡æœ‰è‡ªå®šä¹‰å‡½æ•°ï¼Œä½¿ç”¨é»˜è®¤ ==========
    if compute_score is None:
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ Sandboxï¼ˆä»£ç æ‰§è¡Œï¼‰
        sandbox_config = config.reward_model.get("sandbox_fusion")
        if sandbox_config and sandbox_config.get("url"):
            # ä½¿ç”¨ Sandbox æ‰§è¡Œä»£ç 
            compute_score = partial(
                default_compute_score,
                sandbox_fusion_url=sandbox_url,
                ...
            )
        else:
            # ä½¿ç”¨é»˜è®¤çš„ rule-based reward
            compute_score = default_compute_score

    # ========== æ­¥éª¤ 3: å®ä¾‹åŒ– RewardManager ==========
    reward_manager = NaiveRewardManager(
        tokenizer=tokenizer,
        num_examine=num_examine,
        compute_score=compute_score,
    )

    return reward_manager
```

### 2.3 Reward è®¡ç®—æµç¨‹

**ä»£ç ä½ç½®ï¼š** `verl/workers/reward_manager/naive.py`

```python
# verl/workers/reward_manager/naive.pyï¼ˆç®€åŒ–ç‰ˆï¼‰

class NaiveRewardManager:
    def __init__(self, tokenizer, compute_score, ...):
        self.tokenizer = tokenizer
        self.compute_score = compute_score

    def __call__(self, data: DataProto) -> torch.Tensor:
        """è®¡ç®— reward

        è¾“å…¥ï¼š
            data: DataProtoï¼ŒåŒ…å«ï¼š
                - responses: ç”Ÿæˆçš„ token IDs
                - data_source: æ•°æ®æ¥æºï¼ˆç”¨äºè·¯ç”±ï¼‰
                - ground_truth: æ ‡å‡†ç­”æ¡ˆ

        è¾“å‡ºï¼š
            reward_tensor: [batch_size, seq_len] çš„ reward
        """

        batch_size = len(data)
        rewards = []

        # ========== éå†æ¯ä¸ªæ ·æœ¬ ==========
        for i in range(batch_size):
            # æ­¥éª¤ 1: Decode å“åº”
            response_ids = data.batch['responses'][i]
            response_text = self.tokenizer.decode(
                response_ids,
                skip_special_tokens=True
            )

            # æ­¥éª¤ 2: è·å–å…ƒæ•°æ®
            data_source = data.non_tensor_batch['data_source'][i]
            ground_truth = data.non_tensor_batch['ground_truth'][i]

            # æ­¥éª¤ 3: è°ƒç”¨ reward å‡½æ•°
            score = self.compute_score(
                data_source=data_source,
                solution_str=response_text,
                ground_truth=ground_truth,
            )

            # æ­¥éª¤ 4: å°† reward æ”¾åˆ°æœ€åä¸€ä¸ª token
            seq_len = len(response_ids)
            reward_seq = torch.zeros(seq_len)
            reward_seq[-1] = score  # åªæœ‰æœ€åä¸€ä¸ª token æœ‰ reward

            rewards.append(reward_seq)

        # ========== è¿”å› tensor ==========
        reward_tensor = torch.stack(rewards)  # [batch_size, seq_len]
        return reward_tensor
```

**å…³é”®ç‚¹ï¼šReward é€šå¸¸åªåœ¨æœ€åä¸€ä¸ª tokenï¼**
```
Response: "Let me think... 25 * 4 = 100"
Tokens:   [T1, T2, T3, ..., T_n]
Rewards:  [0,  0,  0,  ..., 1.0]  â† åªæœ‰æœ€åä¸€ä¸ªæœ‰ reward
```

### 2.3 Reward Token æ”¾ç½®æœºåˆ¶ä¸ Advantage å¹¿æ’­ â­

> **æ ¸å¿ƒé—®é¢˜**ï¼šä¸ºä»€ä¹ˆ Reward åªåœ¨æœ€åä¸€ä¸ª tokenï¼Ÿåé¢ä¼šå¹¿æ’­åˆ°æ¯ä¸ª token å—ï¼Ÿ

#### 2.3.1 ä¸ºä»€ä¹ˆåªåœ¨æœ€åä¸€ä¸ª Tokenï¼Ÿ

**è®¾è®¡ç†å¿µï¼šOutcome Supervisionï¼ˆç»“æœç›‘ç£ï¼‰**

**ä»£ç ä½ç½®ï¼š** `verl/trainer/ppo/core_algos.py: ç¬¬ 265 è¡Œ`

```python
# verl/trainer/ppo/core_algos.py
def compute_gae_advantage_return(...):
    """
    NOTE(sgm): this implementation only consider outcome supervision,
    where the reward is a scalar.
    """
```

**ä¸‰ä¸ªæ ¸å¿ƒåŸå› ï¼š**

1. **å•ä¸€æ ‡é‡ Reward**
   - æ¯ä¸ªå®Œæ•´çš„ response å¾—åˆ°ä¸€ä¸ªè¯„åˆ†ï¼ˆå¦‚ GSM8K çš„ æ­£ç¡®/é”™è¯¯ï¼‰
   - è¿™æ˜¯ä¸€ä¸ª **æ ‡é‡å€¼**ï¼ˆscalarï¼‰ï¼Œä¸æ˜¯ token çº§åˆ«çš„å¯†é›†ä¿¡å·
   - ä¾‹å¦‚ï¼š`"Let's solve... The answer is 100"` â†’ reward = 1.0ï¼ˆæ•´ä½“æ­£ç¡®ï¼‰

2. **æ‰€æœ‰ RewardManager å®ç°éƒ½é‡‡ç”¨æ­¤è®¾è®¡**

   **ä»£ç ä½ç½®ï¼š**
   - `verl/workers/reward_manager/naive.py: ç¬¬ 100 è¡Œ`
   - `verl/workers/reward_manager/dapo.py: ç¬¬ 127 è¡Œ`
   - `verl/workers/reward_manager/batch.py: ç¬¬ 110 è¡Œ`

   ```python
   # verl/workers/reward_manager/naive.py
   def __call__(self, data: DataProto) -> torch.Tensor:
       for i in range(batch_size):
           # è®¡ç®—æ•´ä¸ª response çš„ rewardï¼ˆæ ‡é‡ï¼‰
           reward = self.compute_score(
               data_source=data_source,
               solution_str=response_text,
               ground_truth=ground_truth,
           )

           # åªåœ¨æœ€åä¸€ä¸ª token ä½ç½®èµ‹å€¼
           valid_response_length = compute_response_length(...)
           reward_tensor[i, valid_response_length - 1] = reward  # â† åªæœ‰è¿™é‡Œï¼
           # å…¶ä»–ä½ç½®éƒ½æ˜¯ 0
   ```

3. **è®¡ç®—æ•ˆç‡**
   - åªéœ€è®¡ç®—ä¸€æ¬¡æ ‡é‡ reward
   - ä¸éœ€è¦é€ token åˆ†é…ä¿¡ç”¨ï¼ˆCredit Assignment ç”± Advantage è®¡ç®—è´Ÿè´£ï¼‰

**Reward Tensor çš„å½¢çŠ¶ï¼š**
```python
reward_tensor.shape = (batch_size, sequence_length)

# å®é™…å†…å®¹ç¤ºä¾‹ï¼ˆbatch_size=2, seq_len=10ï¼‰ï¼š
reward_tensor = [
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],  # ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼Œreward=1.0
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],  # ç¬¬äºŒä¸ªæ ·æœ¬ï¼Œreward=0.0
]
```

---

#### 2.3.2 Reward å¦‚ä½•å½±å“æ‰€æœ‰ Tokenï¼Ÿä¸¤ç§æœºåˆ¶

è™½ç„¶ Reward åªæ”¾åœ¨æœ€åä¸€ä¸ª tokenï¼Œä½†é€šè¿‡ **Advantage è®¡ç®—**ï¼Œè¿™ä¸ªä¿¡å·ä¼šå½±å“åˆ°æ‰€æœ‰ tokenã€‚ä¸åŒç®—æ³•é‡‡ç”¨ä¸åŒç­–ç•¥ï¼š

---

##### **æ–¹å¼ä¸€ï¼šGAEï¼ˆé€’å½’åå‘ä¼ æ’­ï¼‰**

**ä»£ç ä½ç½®ï¼š** `verl/trainer/ppo/core_algos.py: ç¬¬ 214-262 è¡Œ`

**æ ¸å¿ƒæ€æƒ³ï¼š** Reward é€šè¿‡ TD-error å’Œé€’å½’åå‘ä¼ æ’­è‡ªç„¶å½±å“æ‰€æœ‰ tokenã€‚

```python
@register_adv_est(AdvantageEstimator.GAE)
def compute_gae_advantage_return(
    token_level_rewards: torch.Tensor,  # shape: (bs, response_length)
    values: torch.Tensor,               # shape: (bs, response_length)
    response_mask: torch.Tensor,        # shape: (bs, response_length)
    gamma: float,  # æŠ˜æ‰£å› å­ï¼Œé€šå¸¸ 0.99
    lam: float,    # GAE lambdaï¼Œé€šå¸¸ 0.95
):
    """
    Args:
        token_level_rewards: ç¨€ç– rewardï¼Œåªæœ‰æœ€åä½ç½®éé›¶
            ç¤ºä¾‹ï¼š[0, 0, 0, ..., 0, 1.0]
        values: æ¯ä¸ª token ä½ç½®çš„ value ä¼°è®¡ï¼ˆç”± Critic é¢„æµ‹ï¼‰
        response_mask: EOS åçš„ padding ä½ç½®ä¸º 0
    """

    batch_size = values.size(0)
    gen_len = values.size(1)

    advantages = torch.zeros_like(values)
    lastgaelam = 0
    nextvalues = 0  # æœ€åä¸€ä¸ª token åé¢æ²¡æœ‰ value

    # ========== ä»åå¾€å‰é€’å½’è®¡ç®— ==========
    for t in reversed(range(gen_len)):
        # æ­¥éª¤ 1: è®¡ç®— TD-errorï¼ˆÎ´_tï¼‰
        # Î´_t = r_t + Î³ * V(s_{t+1}) - V(s_t)
        delta = token_level_rewards[:, t] + gamma * nextvalues - values[:, t]
        #       â†‘ è¿™é‡Œä½¿ç”¨äº† token_level_rewards
        #       åœ¨ t = gen_len-1ï¼ˆæœ€åä½ç½®ï¼‰æ—¶ï¼Œtoken_level_rewards[:, t] = rewardï¼ˆéé›¶ï¼‰
        #       åœ¨å…¶ä»–ä½ç½®ï¼Œtoken_level_rewards[:, t] = 0

        # æ­¥éª¤ 2: é€’å½’è®¡ç®— Advantageï¼ˆGAE å…¬å¼ï¼‰
        # A_t = Î´_t + Î³Î» * A_{t+1}
        lastgaelam_ = delta + gamma * lam * lastgaelam

        # æ­¥éª¤ 3: åº”ç”¨ maskï¼ˆå¤„ç† paddingï¼‰
        nextvalues = values[:, t] * response_mask[:, t] + \
                     (1 - response_mask[:, t]) * nextvalues
        lastgaelam = lastgaelam_ * response_mask[:, t] + \
                     (1 - response_mask[:, t]) * lastgaelam

        advantages[:, t] = lastgaelam

    returns = advantages + values
    return advantages, returns
```

**å…³é”®ç†è§£ï¼š**

1. **æœ€åä¸€ä¸ª tokenï¼ˆt=n-1ï¼‰**ï¼š
   ```python
   delta_n = reward + 0 - V(s_n)  # nextvalues=0
   A_n = delta_n                    # lastgaelam=0
   ```

2. **å€’æ•°ç¬¬äºŒä¸ª tokenï¼ˆt=n-2ï¼‰**ï¼š
   ```python
   delta_{n-1} = 0 + Î³ * V(s_n) - V(s_{n-1})
   A_{n-1} = delta_{n-1} + Î³Î» * A_n  # â† A_n åŒ…å«äº† reward çš„ä¿¡æ¯ï¼
   ```

3. **ç»§ç»­å¾€å‰ï¼ˆt=n-3, n-4, ...ï¼‰**ï¼š
   ```python
   A_t = Î´_t + Î³Î» * A_{t+1}
   ```
   æ¯ä¸ªä½ç½®çš„ Advantage éƒ½ä¾èµ–åé¢ä½ç½®çš„ Advantageï¼Œä»è€Œå½¢æˆåå‘ä¼ æ’­é“¾ã€‚

**ç¤ºä¾‹ï¼šè¿½è¸ªä¸€ä¸ªåºåˆ—**

```
åºåˆ—ï¼š["Let", "me", "think", "...", "100"]  (5 ä¸ª tokens)
Rewards:    [0,     0,     0,      0,    1.0]
Values:     [0.1,  0.2,   0.3,    0.5,   0.8]  (Critic é¢„æµ‹)

Î³ = 0.99, Î» = 0.95

ä»åå¾€å‰è®¡ç®—ï¼š
t=4 (æœ€å):  Î´_4 = 1.0 + 0.99*0 - 0.8 = 0.2
             A_4 = 0.2

t=3:         Î´_3 = 0 + 0.99*0.8 - 0.5 = 0.292
             A_3 = 0.292 + 0.99*0.95*0.2 = 0.480

t=2:         Î´_2 = 0 + 0.99*0.5 - 0.3 = 0.195
             A_2 = 0.195 + 0.99*0.95*0.480 = 0.646

t=1:         Î´_1 = 0 + 0.99*0.3 - 0.2 = 0.097
             A_1 = 0.097 + 0.99*0.95*0.646 = 0.703

t=0:         Î´_0 = 0 + 0.99*0.2 - 0.1 = 0.098
             A_0 = 0.098 + 0.99*0.95*0.703 = 0.756

æœ€ç»ˆ Advantages: [0.756, 0.703, 0.646, 0.480, 0.200]
                  â†‘ æ‰€æœ‰ token éƒ½å¾—åˆ°äº† advantage å€¼ï¼
                  â†‘ è¶Šé å‰çš„ tokenï¼Œadvantage è¡°å‡è¶Šå¤šï¼ˆç”± Î³Î» æ§åˆ¶ï¼‰
```

**ç»“è®ºï¼š** GAE ä¸éœ€è¦æ˜¾å¼å¹¿æ’­ï¼ŒReward é€šè¿‡é€’å½’åå‘ä¼ æ’­è‡ªç„¶å½±å“æ‰€æœ‰ tokenã€‚

---

##### **æ–¹å¼äºŒï¼šGRPOï¼ˆæ˜¾å¼å¹¿æ’­ï¼‰**

**ä»£ç ä½ç½®ï¼š** `verl/trainer/ppo/core_algos.py: ç¬¬ 267-330 è¡Œ`

**æ ¸å¿ƒæ€æƒ³ï¼š** æå–æ ‡é‡ reward â†’ å½’ä¸€åŒ– â†’ **æ˜¾å¼å¤åˆ¶åˆ°æ‰€æœ‰ token**ã€‚

```python
@register_adv_est(AdvantageEstimator.GRPO)
def compute_grpo_outcome_advantage(
    token_level_rewards: torch.Tensor,  # shape: (bs, response_length)
    response_mask: torch.Tensor,        # shape: (bs, response_length)
    index: np.ndarray,                  # æ¯ä¸ªæ ·æœ¬çš„ prompt_idï¼ˆç”¨äºåˆ†ç»„ï¼‰
    epsilon: float = 1e-6,
    norm_adv_by_std_in_grpo: bool = True,
):
    """
    GRPO ç®—æ³•æ­¥éª¤ï¼š
    1. æå–æ ‡é‡ rewardï¼ˆsum across tokensï¼‰
    2. æŒ‰ prompt åˆ†ç»„ï¼Œè®¡ç®— group mean/std
    3. å½’ä¸€åŒ–ï¼š(reward - mean) / std
    4. **å¹¿æ’­ï¼šå¤åˆ¶åˆ°æ‰€æœ‰ token ä½ç½®**
    """

    batch_size, response_length = token_level_rewards.shape

    # ========== æ­¥éª¤ 1: æå–æ ‡é‡ reward ==========
    # ç”±äºåªæœ‰æœ€åä¸€ä¸ª token æœ‰ rewardï¼Œsum æ“ä½œå®é™…ä¸Šæå–äº†è¿™ä¸ªå€¼
    scores = token_level_rewards.sum(dim=-1)  # shape: (batch_size,)
    # ç¤ºä¾‹ï¼š[[0, 0, 1.0], [0, 0, 0.5]] â†’ [1.0, 0.5]

    # ========== æ­¥éª¤ 2: æŒ‰ Group å½’ä¸€åŒ– ==========
    # å°†åŒä¸€ä¸ª prompt çš„å¤šä¸ª response åˆ†ç»„
    id2score = defaultdict(list)
    for i in range(batch_size):
        id2score[index[i]].append(scores[i])

    # è®¡ç®—æ¯ä¸ª group çš„ mean å’Œ std
    id2mean = {}
    id2std = {}
    for idx in id2score:
        if len(id2score[idx]) > 1:
            scores_tensor = torch.stack(id2score[idx])
            id2mean[idx] = torch.mean(scores_tensor)
            id2std[idx] = torch.std(scores_tensor)

    # å½’ä¸€åŒ–ï¼š(score - group_mean) / group_std
    for i in range(batch_size):
        if norm_adv_by_std_in_grpo:
            scores[i] = (scores[i] - id2mean[index[i]]) / (id2std[index[i]] + epsilon)
        else:
            scores[i] = scores[i] - id2mean[index[i]]

    # æ­¤æ—¶ scores.shape = (batch_size,)
    # ç¤ºä¾‹ï¼š[0.5, -0.3, 1.2, -0.8]

    # ========== æ­¥éª¤ 3: **å¹¿æ’­åˆ°æ‰€æœ‰ token** ==========
    # è¿™æ˜¯å…³é”®çš„å¹¿æ’­æ“ä½œï¼
    scores = scores.unsqueeze(-1) * response_mask
    #        â†‘ shape: (bs,) â†’ (bs, 1)
    #                          Ã— (bs, response_length) â†’ (bs, response_length)

    # ç¤ºä¾‹ï¼š
    # scores = [0.5, -0.3]  (batch_size=2)
    # response_mask = [[1, 1, 1, 0, 0],  # å‰ 3 ä¸ª token æœ‰æ•ˆï¼Œå 2 ä¸ªæ˜¯ padding
    #                   [1, 1, 1, 1, 1]]  # æ‰€æœ‰ 5 ä¸ª token æœ‰æ•ˆ
    #
    # scores.unsqueeze(-1) = [[0.5], [-0.3]]
    #
    # å¹¿æ’­ç»“æœï¼š
    # scores = [[0.5, 0.5, 0.5, 0.0, 0.0],   # æœ‰æ•ˆ token éƒ½æ˜¯ 0.5ï¼Œpadding æ˜¯ 0
    #           [-0.3, -0.3, -0.3, -0.3, -0.3]]  # æ‰€æœ‰ token éƒ½æ˜¯ -0.3

    return scores, scores  # (advantages, returns)
```

**å…³é”®ç†è§£ï¼š**

1. **`unsqueeze(-1)` çš„ä½œç”¨**ï¼š
   ```python
   # Before: (batch_size,)
   scores = torch.tensor([0.5, -0.3])

   # After: (batch_size, 1)
   scores = torch.tensor([[0.5], [-0.3]])
   ```

2. **Broadcasting æœºåˆ¶**ï¼š
   ```python
   # PyTorch è‡ªåŠ¨å°† (batch_size, 1) å¹¿æ’­åˆ° (batch_size, response_length)
   (bs, 1) Ã— (bs, response_length) â†’ (bs, response_length)

   # æ¯ä¸ªæ ‡é‡å€¼å¤åˆ¶åˆ°æ•´è¡Œ
   [[0.5]] Ã— [[1, 1, 1, 0, 0]] = [[0.5, 0.5, 0.5, 0.0, 0.0]]
   ```

3. **response_mask çš„ä½œç”¨**ï¼š
   - ç¡®ä¿ padding ä½ç½®çš„ advantage ä¸º 0
   - åªæœ‰æœ‰æ•ˆ token ä½ç½®æœ‰ advantage å€¼

**ç¤ºä¾‹ï¼šå®Œæ•´æµç¨‹**

```
è¾“å…¥ï¼š
prompt = "What is 25*4?"
responses (åŒä¸€ä¸ª prompt çš„ 4 ä¸ª response):
  - response_0: "The answer is 100"  â†’ reward = 1.0
  - response_1: "Let me think... 100" â†’ reward = 1.0
  - response_2: "It's 99"             â†’ reward = 0.0
  - response_3: "I don't know"        â†’ reward = 0.0

æ­¥éª¤ 1: æå–æ ‡é‡ reward
scores = [1.0, 1.0, 0.0, 0.0]

æ­¥éª¤ 2: Group å½’ä¸€åŒ–
group_mean = 0.5
group_std = 0.5
normalized_scores = [(1.0-0.5)/0.5, (1.0-0.5)/0.5, (0.0-0.5)/0.5, (0.0-0.5)/0.5]
                  = [1.0, 1.0, -1.0, -1.0]

æ­¥éª¤ 3: å¹¿æ’­
å‡è®¾ response_0 æœ‰ 5 ä¸ª tokensï¼ˆæ—  paddingï¼‰ï¼š
advantages[0] = [1.0, 1.0, 1.0, 1.0, 1.0]
                 â†‘ æ¯ä¸ª token éƒ½å¾—åˆ°ç›¸åŒçš„ advantageï¼

å‡è®¾ response_2 æœ‰ 3 ä¸ªæœ‰æ•ˆ tokens + 2 ä¸ª paddingï¼š
response_mask[2] = [1, 1, 1, 0, 0]
advantages[2] = [-1.0, -1.0, -1.0, 0.0, 0.0]
                 â†‘ æœ‰æ•ˆ token éƒ½æ˜¯ -1.0ï¼Œpadding æ˜¯ 0
```

**ç»“è®ºï¼š** GRPO é€šè¿‡æ˜¾å¼å¹¿æ’­ï¼Œå°†æ ‡é‡ advantage å¤åˆ¶åˆ°æ‰€æœ‰æœ‰æ•ˆ token ä½ç½®ã€‚

---

#### 2.3.3 å…¶ä»–ç®—æ³•ä¹Ÿä½¿ç”¨å¹¿æ’­

**æ‰€æœ‰åŸºäº outcome supervision çš„ç®—æ³•éƒ½é‡‡ç”¨ç±»ä¼¼çš„å¹¿æ’­æœºåˆ¶ï¼š**

**ä»£ç ä½ç½®ï¼š** `verl/trainer/ppo/core_algos.py`

| ç®—æ³• | å‡½æ•°å | å¹¿æ’­ä»£ç è¡Œ | å¹¿æ’­æ–¹å¼ |
|------|--------|-----------|---------|
| **GRPO** | `compute_grpo_outcome_advantage` | 328 | `scores.unsqueeze(-1) * response_mask` |
| **GRPO_VECTORIZED** | `compute_grpo_vectorized_outcome_advantage` | 356 | `scalars.unsqueeze(-1) * response_mask` |
| **REINFORCE++** | `compute_reinforce_plus_plus` | 418 | `scores.unsqueeze(-1) * response_mask` |
| **RLOO** | `compute_rloo_outcome_advantage` | 470 | `scores.unsqueeze(-1) * response_mask` |
| **OPO** | `compute_opo_outcome_advantage` | 523 | `scores.unsqueeze(-1) * response_mask` |
| **ReMax** | `compute_remax_outcome_advantage` | 577 | `scalars.unsqueeze(-1) * response_mask` |

**å”¯ä¸€çš„ä¾‹å¤–æ˜¯ GAE**ï¼Œå®ƒé€šè¿‡é€’å½’åå‘ä¼ æ’­è‡ªç„¶å¤„ç†ã€‚

---

#### 2.3.4 response_mask çš„å…³é”®ä½œç”¨

**å®šä¹‰ï¼š** `response_mask` æ ‡è¯†å“ªäº› token æ˜¯æœ‰æ•ˆçš„ï¼ˆ1ï¼‰ï¼Œå“ªäº›æ˜¯ paddingï¼ˆ0ï¼‰ã€‚

**ä»£ç ä½ç½®ï¼š** `verl/trainer/ppo/ray_trainer.py`

```python
def compute_response_mask(data: DataProto):
    """
    è®¡ç®— response éƒ¨åˆ†çš„ attention mask

    Returns:
        torch.Tensor: shape (batch_size, response_length)
            - 1.0: æœ‰æ•ˆ tokenï¼ˆåŒ…æ‹¬ EOSï¼‰
            - 0.0: padding token
    """
    responses = data.batch["responses"]
    response_length = responses.size(1)
    attention_mask = data.batch["attention_mask"]

    # æå– response éƒ¨åˆ†çš„ mask
    return attention_mask[:, -response_length:]
```

**ä¸ºä»€ä¹ˆéœ€è¦ maskï¼Ÿ**

1. **å˜é•¿åºåˆ—**ï¼šä¸åŒ response é•¿åº¦ä¸åŒï¼Œéœ€è¦ padding å¯¹é½
2. **é˜²æ­¢æ±¡æŸ“**ï¼špadding ä½ç½®ä¸åº”å‚ä¸æ¢¯åº¦è®¡ç®—
3. **å¹¿æ’­è¿‡æ»¤**ï¼šç¡®ä¿åªæœ‰æœ‰æ•ˆ token å¾—åˆ° advantage

**ç¤ºä¾‹ï¼š**
```python
# Batch ä¸­çš„ä¸¤ä¸ª responseï¼ˆé•¿åº¦ä¸åŒï¼‰
response_0 = "The answer is 100"        # 5 ä¸ª tokens
response_1 = "100"                      # 1 ä¸ª token

# Padding åï¼ˆmax_length=5ï¼‰
padded_responses = [
    [token_1, token_2, token_3, token_4, token_5],  # response_0ï¼ˆæ—  paddingï¼‰
    [token_1, <pad>,  <pad>,  <pad>,  <pad>],       # response_1ï¼ˆ4 ä¸ª paddingï¼‰
]

response_mask = [
    [1, 1, 1, 1, 1],  # æ‰€æœ‰ä½ç½®æœ‰æ•ˆ
    [1, 0, 0, 0, 0],  # åªæœ‰ç¬¬ä¸€ä¸ªä½ç½®æœ‰æ•ˆ
]

# å¹¿æ’­ advantageï¼ˆå‡è®¾ normalized_score = 0.5ï¼‰
advantages = [
    [0.5, 0.5, 0.5, 0.5, 0.5],  # response_0ï¼šæ‰€æœ‰ token éƒ½æœ‰ advantage
    [0.5, 0.0, 0.0, 0.0, 0.0],  # response_1ï¼šåªæœ‰ç¬¬ä¸€ä¸ª token æœ‰ advantage
]
```

---

#### 2.3.5 å®Œæ•´æ•°æ®æµå›¾ç¤º

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. REWARD COMPUTATION (RewardManager)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Input: Response sequences
   â†“
   responses = ["Let me think... 25 * 4 = 100", "The answer is 99"]
   â†“
   token_level_rewards = [
       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],  â† reward åªåœ¨æœ€å
       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
   ]
   shape: (batch_size, response_length)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2A. GAE ADVANTAGE CALCULATION (é€’å½’åå‘ä¼ æ’­)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Input: token_level_rewards, values, response_mask
   â†“
   for t in reversed(range(response_length)):
       delta = token_level_rewards[:, t] + gamma * nextvalues - values[:, t]
       advantages[:, t] = delta + gamma * lam * advantages[:, t+1]
   â†“
   Output: advantages = [
       [0.85, 0.78, 0.69, 0.58, 0.45, 0.30, 0.20],  â† è‡ªç„¶è¡°å‡
       [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
   ]

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2B. GRPO ADVANTAGE CALCULATION (æ˜¾å¼å¹¿æ’­)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Input: token_level_rewards, response_mask, index
   â†“
   æ­¥éª¤ 1: æå–æ ‡é‡
   scores = token_level_rewards.sum(dim=-1)  # [1.0, 0.0]
   â†“
   æ­¥éª¤ 2: Group å½’ä¸€åŒ–
   normalized_scores = [1.0, -1.0]  # å‡è®¾ group_mean=0.5, group_std=0.5
   â†“
   æ­¥éª¤ 3: å¹¿æ’­
   advantages = normalized_scores.unsqueeze(-1) * response_mask
   â†“
   Output: advantages = [
       [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],  â† æ‰€æœ‰ token ç›¸åŒ
       [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0],
   ]

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. LOSS COMPUTATION (Actor Training)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Input: advantages, old_log_probs, new_log_probs
   â†“
   ratio = exp(new_log_probs - old_log_probs)  # importance sampling
   clipped_ratio = clip(ratio, 1-Îµ, 1+Îµ)
   â†“
   loss = -min(ratio * advantages, clipped_ratio * advantages)
   â†“
   # æ¯ä¸ª token ä½ç½®éƒ½æœ‰ lossï¼Œæ±‚ mean
   final_loss = loss.sum() / response_mask.sum()
   â†“
   Backpropagation â†’ Update Actor Model
```

---

#### 2.3.6 æ€»ç»“å¯¹æ¯”

| ç»´åº¦ | GAE | GRPO |
|------|-----|------|
| **Reward æ”¾ç½®** | æœ€åä¸€ä¸ª token | æœ€åä¸€ä¸ª token |
| **æå–æ–¹å¼** | ç›´æ¥ä½¿ç”¨ `token_level_rewards[:, t]` | `token_level_rewards.sum(dim=-1)` æå–æ ‡é‡ |
| **å¹¿æ’­æ–¹å¼** | é€’å½’åå‘ä¼ æ’­ï¼ˆéšå¼ï¼‰ | `unsqueeze(-1) * response_mask`ï¼ˆæ˜¾å¼ï¼‰|
| **Advantage åˆ†å¸ƒ** | éå‡åŒ€ï¼ˆè¶Šé å‰è¶Šå°ï¼‰ | å‡åŒ€ï¼ˆæ‰€æœ‰ token ç›¸åŒï¼‰|
| **ä¾èµ– Value** | æ˜¯ï¼ˆéœ€è¦ Criticï¼‰ | å¦ï¼ˆåªéœ€è¦ Group ç»Ÿè®¡ï¼‰ |
| **è®¡ç®—å¤æ‚åº¦** | O(response_length) é€’å½’ | O(batch_size) å½’ä¸€åŒ– + O(1) å¹¿æ’­ |

**æ ¸å¿ƒè¦ç‚¹ï¼š**
1. âœ… **Reward åªåœ¨æœ€åä¸€ä¸ª token**ï¼šè¿™æ˜¯ outcome supervision çš„è®¾è®¡
2. âœ… **GAEï¼šé€’å½’åå‘ä¼ æ’­**ï¼šé€šè¿‡ TD-error é“¾è‡ªç„¶å½±å“æ‰€æœ‰ token
3. âœ… **GRPOï¼šæ˜¾å¼å¹¿æ’­**ï¼šå°†æ ‡é‡ advantage å¤åˆ¶åˆ°æ‰€æœ‰ token
4. âœ… **response_maskï¼šè¿‡æ»¤ padding**ï¼šç¡®ä¿åªæœ‰æœ‰æ•ˆ token å‚ä¸è®¡ç®—

---

## 3. GSM8K Reward è¯¦ç»†è§£æ

### 3.1 GSM8K Reward å‡½æ•°

**æ–‡ä»¶ä½ç½®ï¼š** `verl/utils/reward_score/gsm8k.py`

```python
# verl/utils/reward_score/gsm8k.py

def extract_solution(solution_str, method="strict"):
    """ä»å“åº”ä¸­æå–ç­”æ¡ˆ

    method="strict": è¦æ±‚æ ¼å¼ "#### ç­”æ¡ˆ"
    method="flexible": æå–æœ€åä¸€ä¸ªæ•°å­—

    ç¤ºä¾‹ï¼š
    è¾“å…¥: "Let's solve step by step...\\n#### 100"
    è¾“å‡º: "100"
    """

    # ä¼˜åŒ–ï¼šåªæ£€æŸ¥æœ€å 300 å­—ç¬¦
    if len(solution_str) > 300:
        solution_str = solution_str[-300:]

    if method == "strict":
        # åŒ¹é… "#### æ•°å­—" æ ¼å¼
        solutions = re.findall("#### (\\-?[0-9\\.\\,]+)", solution_str)
        if len(solutions) == 0:
            return None
        else:
            # å–æœ€åä¸€ä¸ªåŒ¹é…
            final_answer = solutions[-1].replace(",", "").replace("$", "")
            return final_answer

    elif method == "flexible":
        # æå–æ‰€æœ‰æ•°å­—ï¼Œå–æœ€åä¸€ä¸ª
        numbers = re.findall("(\\-?[0-9\\.\\,]+)", solution_str)
        if len(numbers) == 0:
            return None
        # ä»åå¾€å‰æ‰¾ç¬¬ä¸€ä¸ªæœ‰æ•ˆæ•°å­—
        for num in reversed(numbers):
            if num not in ["", "."]:
                return num


def compute_score(solution_str, ground_truth,
                 method="strict",
                 format_score=0.0,
                 score=1.0):
    """GSM8K æ‰“åˆ†å‡½æ•°

    Args:
        solution_str: æ¨¡å‹ç”Ÿæˆçš„å“åº”
        ground_truth: æ ‡å‡†ç­”æ¡ˆ
        method: "strict" æˆ– "flexible"
        format_score: æ ¼å¼æ­£ç¡®ä½†ç­”æ¡ˆé”™è¯¯çš„åˆ†æ•°ï¼ˆé»˜è®¤ 0ï¼‰
        score: ç­”æ¡ˆæ­£ç¡®çš„åˆ†æ•°ï¼ˆé»˜è®¤ 1.0ï¼‰

    Returns:
        float: reward åˆ†æ•°
    """

    # æ­¥éª¤ 1: æå–ç­”æ¡ˆ
    answer = extract_solution(solution_str, method=method)

    # æ­¥éª¤ 2: æ‰“åˆ†
    if answer is None:
        # æ²¡æœ‰ç­”æ¡ˆ â†’ 0 åˆ†
        return 0
    elif answer == ground_truth:
        # ç­”æ¡ˆæ­£ç¡® â†’ æ»¡åˆ†
        return score
    else:
        # æ ¼å¼æ­£ç¡®ä½†ç­”æ¡ˆé”™è¯¯ â†’ format_scoreï¼ˆé€šå¸¸æ˜¯ 0ï¼‰
        return format_score
```

### 3.2 GSM8K Reward ç¤ºä¾‹

**ä¾‹å­ 1ï¼šå®Œç¾å“åº”**
```python
# è¾“å…¥
solution_str = """
Let me solve this step by step:
1. We need to calculate 25 * 4
2. 25 * 4 = 100

#### 100
"""
ground_truth = "100"

# å¤„ç†è¿‡ç¨‹
answer = extract_solution(solution_str, "strict")
# â†’ answer = "100"

score = compute_score(solution_str, ground_truth)
# â†’ score = 1.0 âœ“
```

**ä¾‹å­ 2ï¼šç­”æ¡ˆé”™è¯¯**
```python
# è¾“å…¥
solution_str = """
Let me calculate:
25 * 4 = 90

#### 90
"""
ground_truth = "100"

# å¤„ç†è¿‡ç¨‹
answer = extract_solution(solution_str, "strict")
# â†’ answer = "90"

score = compute_score(solution_str, ground_truth)
# â†’ score = 0.0 âœ—ï¼ˆç­”æ¡ˆé”™è¯¯ï¼‰
```

**ä¾‹å­ 3ï¼šæ ¼å¼é”™è¯¯**
```python
# è¾“å…¥
solution_str = """
The answer is 100
"""
ground_truth = "100"

# å¤„ç†è¿‡ç¨‹
answer = extract_solution(solution_str, "strict")
# â†’ answer = Noneï¼ˆæ²¡æœ‰ "####"ï¼‰

score = compute_score(solution_str, ground_truth)
# â†’ score = 0.0 âœ—ï¼ˆæ ¼å¼é”™è¯¯ï¼‰
```

**ä¾‹å­ 4ï¼šflexible æ¨¡å¼**
```python
# è¾“å…¥
solution_str = """
The final answer is 100
"""
ground_truth = "100"

# å¤„ç†è¿‡ç¨‹ï¼ˆflexible æ¨¡å¼ï¼‰
answer = extract_solution(solution_str, "flexible")
# â†’ answer = "100"ï¼ˆæå–æœ€åä¸€ä¸ªæ•°å­—ï¼‰

score = compute_score(solution_str, ground_truth, method="flexible")
# â†’ score = 1.0 âœ“
```

---

## 4. è‡ªå®šä¹‰ Reward å‡½æ•°

### 4.1 Reward å‡½æ•°ç­¾å

```python
def my_reward_function(
    data_source: str,      # æ•°æ®æ¥æºï¼ˆå¦‚ "gsm8k"ï¼‰
    solution_str: str,     # æ¨¡å‹ç”Ÿæˆçš„å“åº”
    ground_truth: Any,     # æ ‡å‡†ç­”æ¡ˆ
    extra_info: dict = None  # é¢å¤–ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
) -> float:
    """è‡ªå®šä¹‰ reward å‡½æ•°

    è¿”å›ï¼š
        float: reward åˆ†æ•°ï¼ˆé€šå¸¸ 0-1 ä¹‹é—´ï¼‰
    """
    pass
```

### 4.2 å®ä¾‹ 1ï¼šä»£ç ç”Ÿæˆ Reward

```python
# my_code_reward.py

import subprocess
import tempfile
import os

def compute_score(data_source, solution_str, ground_truth, extra_info=None):
    """ä»£ç ç”Ÿæˆä»»åŠ¡çš„ Reward

    è¯„ä¼°ï¼š
    1. ä»£ç æ˜¯å¦èƒ½è¿è¡Œ
    2. æ˜¯å¦é€šè¿‡æµ‹è¯•ç”¨ä¾‹
    """

    # æ­¥éª¤ 1: æå–ä»£ç 
    code = extract_code_block(solution_str)
    if code is None:
        return 0.0

    # æ­¥éª¤ 2: è·å–æµ‹è¯•ç”¨ä¾‹
    test_cases = extra_info.get('test_cases', [])
    if not test_cases:
        return 0.0

    # æ­¥éª¤ 3: æ‰§è¡Œæµ‹è¯•
    passed = 0
    for test_case in test_cases:
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(
                mode='w',
                suffix='.py',
                delete=False
            ) as f:
                f.write(code)
                f.write(f"\\n{test_case}")
                temp_file = f.name

            # æ‰§è¡Œä»£ç 
            result = subprocess.run(
                ['python', temp_file],
                capture_output=True,
                timeout=5,
                text=True
            )

            # æ£€æŸ¥ç»“æœ
            if result.returncode == 0:
                passed += 1

            # æ¸…ç†
            os.unlink(temp_file)

        except Exception as e:
            # è¿è¡Œå¤±è´¥
            pass

    # æ­¥éª¤ 4: è®¡ç®—åˆ†æ•°
    score = passed / len(test_cases)
    return score


def extract_code_block(text):
    """ä»å“åº”ä¸­æå–ä»£ç å—"""
    import re

    # åŒ¹é… ```python ... ``` æˆ– ```...```
    patterns = [
        r"```python\\n(.+?)```",
        r"```\\n(.+?)```"
    ]

    for pattern in patterns:
        match = re.search(pattern, text, re.DOTALL)
        if match:
            return match.group(1)

    return None
```

### 4.3 å®ä¾‹ 2ï¼šå¤šç›®æ ‡ Reward

```python
# multi_objective_reward.py

def compute_score(data_source, solution_str, ground_truth, extra_info=None):
    """å¤šç›®æ ‡ Rewardï¼šæ­£ç¡®æ€§ + ç®€æ´æ€§

    Reward = 0.7 * correctness + 0.3 * conciseness
    """

    # ç›®æ ‡ 1: æ­£ç¡®æ€§
    answer = extract_answer(solution_str)
    if answer == ground_truth:
        correctness = 1.0
    else:
        correctness = 0.0

    # ç›®æ ‡ 2: ç®€æ´æ€§ï¼ˆæƒ©ç½šè¿‡é•¿çš„å“åº”ï¼‰
    response_length = len(solution_str)
    if response_length < 100:
        conciseness = 1.0
    elif response_length < 300:
        conciseness = 0.5
    else:
        conciseness = 0.0

    # ç»„åˆ
    final_score = 0.7 * correctness + 0.3 * conciseness
    return final_score
```

### 4.4 å®ä¾‹ 3ï¼šReward Shaping

```python
# reward_shaping.py

def compute_score(data_source, solution_str, ground_truth, extra_info=None):
    """Reward Shapingï¼šæä¾›ä¸­é—´å¥–åŠ±

    ä¸åªæ˜¯æœ€ç»ˆç­”æ¡ˆï¼Œä¸­é—´æ­¥éª¤ä¹Ÿç»™ reward
    """

    # æœ€ç»ˆç­”æ¡ˆ reward
    answer = extract_answer(solution_str)
    if answer == ground_truth:
        final_reward = 1.0
    else:
        final_reward = 0.0

    # ä¸­é—´æ­¥éª¤ reward
    intermediate_reward = 0.0

    # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®æ­¥éª¤
    if "step by step" in solution_str.lower():
        intermediate_reward += 0.1

    if "let me think" in solution_str.lower():
        intermediate_reward += 0.05

    # æ£€æŸ¥æ˜¯å¦åˆ—å‡ºè®¡ç®—æ­¥éª¤
    if "=" in solution_str:
        intermediate_reward += 0.1

    # æ€» reward
    total_reward = final_reward + intermediate_reward
    return min(total_reward, 1.0)  # é™åˆ¶åœ¨ [0, 1]
```

### 4.5 ä½¿ç”¨è‡ªå®šä¹‰ Reward

```bash
# æ–¹æ³• 1ï¼šé€šè¿‡é…ç½®æ–‡ä»¶
python3 -m verl.trainer.main_ppo \
    custom_reward_function.path=/path/to/my_code_reward.py \
    custom_reward_function.name=compute_score

# æ–¹æ³• 2ï¼šå¦‚æœå‡½æ•°åå°±æ˜¯ compute_scoreï¼Œå¯ä»¥çœç•¥ name
python3 -m verl.trainer.main_ppo \
    custom_reward_function.path=/path/to/my_reward.py
```

---

## 5. Reward è°ƒè¯•æŠ€å·§

### 5.1 æ‰“å° Reward è¯¦æƒ…

```python
# åœ¨ reward å‡½æ•°ä¸­æ·»åŠ è°ƒè¯•è¾“å‡º

def compute_score(data_source, solution_str, ground_truth, extra_info=None):
    # æå–ç­”æ¡ˆ
    answer = extract_answer(solution_str)

    # è®¡ç®—åˆ†æ•°
    if answer == ground_truth:
        score = 1.0
    else:
        score = 0.0

    # è°ƒè¯•è¾“å‡º
    if extra_info and extra_info.get('debug', False):
        print(f"[DEBUG Reward]")
        print(f"  Solution (å‰100å­—ç¬¦): {solution_str[:100]}")
        print(f"  æå–ç­”æ¡ˆ: {answer}")
        print(f"  æ ‡å‡†ç­”æ¡ˆ: {ground_truth}")
        print(f"  åˆ†æ•°: {score}")

    return score
```

### 5.2 ç»Ÿè®¡ Reward åˆ†å¸ƒ

```python
# åœ¨ RewardManager ä¸­æ”¶é›†ç»Ÿè®¡

class DebugRewardManager(NaiveRewardManager):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.reward_history = []

    def __call__(self, data):
        rewards = super().__call__(data)

        # æ”¶é›†ç»Ÿè®¡
        self.reward_history.append(rewards.mean().item())

        # æ¯ 100 ä¸ª batch æ‰“å°ç»Ÿè®¡
        if len(self.reward_history) % 100 == 0:
            import numpy as np
            print(f"[Reward Stats] æœ€è¿‘ 100 ä¸ª batch:")
            print(f"  å¹³å‡: {np.mean(self.reward_history[-100:]):.3f}")
            print(f"  æœ€å¤§: {np.max(self.reward_history[-100:]):.3f}")
            print(f"  æœ€å°: {np.min(self.reward_history[-100:]):.3f}")

        return rewards
```

### 5.3 ä¿å­˜å¤±è´¥æ ·æœ¬

```python
# ä¿å­˜ reward=0 çš„æ ·æœ¬ç”¨äºåˆ†æ

def compute_score(data_source, solution_str, ground_truth, extra_info=None):
    answer = extract_answer(solution_str)
    score = 1.0 if answer == ground_truth else 0.0

    # ä¿å­˜å¤±è´¥æ ·æœ¬
    if score == 0.0:
        import json
        with open('failed_samples.jsonl', 'a') as f:
            sample = {
                'solution': solution_str,
                'extracted_answer': answer,
                'ground_truth': ground_truth,
            }
            f.write(json.dumps(sample, ensure_ascii=False) + '\\n')

    return score
```

### 5.4 å¯è§†åŒ– Reward

```python
# visualization.py
import pandas as pd
import matplotlib.pyplot as plt

# è¯»å–è®­ç»ƒæ—¥å¿—
df = pd.read_csv('training_log.csv')

# ç»˜åˆ¶ reward æ›²çº¿
plt.figure(figsize=(10, 6))
plt.plot(df['step'], df['reward_mean'])
plt.xlabel('Training Step')
plt.ylabel('Mean Reward')
plt.title('Reward Progression')
plt.savefig('reward_curve.png')
```

---

## 6. å¸¸è§é—®é¢˜è§£å†³

### Q1: Reward ä¸€ç›´æ˜¯ 0

**å¯èƒ½åŸå› ï¼š**

1. **æ ¼å¼ä¸åŒ¹é…**
```python
# æ£€æŸ¥ï¼šæ¨¡å‹è¾“å‡ºæ ¼å¼
print(f"Response: {solution_str}")
# æ˜¯å¦ç¬¦åˆ reward å‡½æ•°çš„é¢„æœŸæ ¼å¼ï¼Ÿ
```

2. **ground_truth å­—æ®µç¼ºå¤±**
```python
# æ£€æŸ¥æ•°æ®
import pandas as pd
df = pd.read_parquet('train.parquet')
print(df.iloc[0]['reward_model'])
# åº”è¯¥æœ‰ 'ground_truth' å­—æ®µ
```

3. **Reward å‡½æ•°æŠ¥é”™ä½†è¢«å¿½ç•¥**
```python
# æ·»åŠ  try-except æ•è·
def compute_score(...):
    try:
        # ä½ çš„é€»è¾‘
        ...
    except Exception as e:
        print(f"âŒ Reward è®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 0.0
```

### Q2: Reward ä¸ç¨³å®š

**ç°è±¡ï¼š** Reward åœ¨ç›¸ä¼¼çš„å“åº”ä¸Šç»™å‡ºä¸åŒåˆ†æ•°

**è§£å†³æ–¹æ³•ï¼š**
```python
# æ·»åŠ ç­”æ¡ˆæ ‡å‡†åŒ–
def normalize_answer(answer):
    """æ ‡å‡†åŒ–ç­”æ¡ˆæ ¼å¼"""
    answer = answer.strip()
    answer = answer.lower()
    answer = answer.replace(',', '')
    answer = answer.replace('$', '')
    return answer

def compute_score(data_source, solution_str, ground_truth, ...):
    answer = extract_answer(solution_str)
    answer = normalize_answer(answer)
    ground_truth = normalize_answer(ground_truth)

    if answer == ground_truth:
        return 1.0
    else:
        return 0.0
```

### Q3: Reward è®¡ç®—å¤ªæ…¢

**è§£å†³æ–¹æ³•ï¼š**

1. **å¹¶è¡ŒåŒ–**
```python
# ä½¿ç”¨ multiprocessing
from multiprocessing import Pool

class ParallelRewardManager(NaiveRewardManager):
    def __init__(self, *args, num_workers=4, **kwargs):
        super().__init__(*args, **kwargs)
        self.pool = Pool(num_workers)

    def __call__(self, data):
        # å¹¶è¡Œè®¡ç®— reward
        rewards = self.pool.map(self.compute_single, data)
        return torch.tensor(rewards)
```

2. **ä¼˜åŒ–æ­£åˆ™è¡¨è¾¾å¼**
```python
# åªæ£€æŸ¥å­—ç¬¦ä¸²æœ«å°¾
def extract_solution(solution_str):
    # åªå–æœ€å 300 å­—ç¬¦
    if len(solution_str) > 300:
        solution_str = solution_str[-300:]

    # æå–ç­”æ¡ˆ
    ...
```

### Q4: å¦‚ä½•è®¾è®¡å¥½çš„ Rewardï¼Ÿ

**åŸåˆ™ï¼š**

1. **Sparse vs Dense**
```python
# Sparseï¼ˆç¨€ç–ï¼‰ï¼šåªæœ‰æœ€ç»ˆç­”æ¡ˆæœ‰ reward
# ä¼˜ç‚¹ï¼šç®€å•
# ç¼ºç‚¹ï¼šå­¦ä¹ æ…¢

# Denseï¼ˆå¯†é›†ï¼‰ï¼šä¸­é—´æ­¥éª¤ä¹Ÿæœ‰ reward
# ä¼˜ç‚¹ï¼šå­¦ä¹ å¿«
# ç¼ºç‚¹ï¼šå¯èƒ½å¼•å¯¼é”™è¯¯è¡Œä¸ºï¼ˆreward hackingï¼‰
```

2. **é¿å… Reward Hacking**
```python
# é”™è¯¯ç¤ºä¾‹ï¼šåªæ ¹æ®é•¿åº¦ç»™ reward
def bad_reward(solution_str, ...):
    # âŒ æ¨¡å‹ä¼šå­¦ä¼šè¾“å‡ºå¾ˆé•¿çš„æ— æ„ä¹‰æ–‡æœ¬
    return len(solution_str) / 1000

# æ­£ç¡®ç¤ºä¾‹ï¼šç»“åˆå¤šä¸ªæŒ‡æ ‡
def good_reward(solution_str, ground_truth, ...):
    correctness = check_answer(solution_str, ground_truth)
    length_penalty = max(0, 1 - len(solution_str) / 500)
    return correctness * (1 + 0.1 * length_penalty)
```

3. **å½’ä¸€åŒ– Reward**
```python
# å°† reward å½’ä¸€åŒ–åˆ° [0, 1] èŒƒå›´
def compute_score(...):
    raw_score = calculate_raw_score(...)

    # å½’ä¸€åŒ–
    normalized_score = min(max(raw_score, 0.0), 1.0)

    return normalized_score
```

---

## 7. æ€»ç»“

**Reward ç³»ç»Ÿçš„æ ¸å¿ƒæµç¨‹ï¼š**
```
1. åŠ è½½ RewardManagerï¼ˆè®­ç»ƒå¼€å§‹æ—¶ï¼‰
   â†“
2. ç”Ÿæˆå“åº”åï¼Œè°ƒç”¨ reward_manager(data)
   â†“
3. å¯¹æ¯ä¸ªæ ·æœ¬ï¼š
   - Decode å“åº”
   - è°ƒç”¨ compute_score
   - å°†åˆ†æ•°æ”¾åˆ°æœ€åä¸€ä¸ª token
   â†“
4. è¿”å› reward tensor
```

**è®¾è®¡ Reward çš„å…³é”®ç‚¹ï¼š**
- âœ… æ˜ç¡®ç›®æ ‡ï¼ˆä»€ä¹ˆæ˜¯"å¥½"çš„è¾“å‡ºï¼‰
- âœ… è€ƒè™‘ä¸­é—´æ­¥éª¤ï¼ˆDense rewardï¼‰
- âœ… é¿å… Reward Hacking
- âœ… å½’ä¸€åŒ–åˆ†æ•°
- âœ… å……åˆ†æµ‹è¯•å’Œè°ƒè¯•

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [Reward Function å®˜æ–¹æ–‡æ¡£](https://verl.readthedocs.io/en/latest/preparation/reward_function.html)
- [æ•°æ®æµè¯¦è§£](./æ•°æ®æµè¯¦è§£.md)
- [RayPPOTrainer è¯¦è§£](../01_å¿«é€Ÿä¸Šæ‰‹/ray_trainer_è¯¦è§£.md)
