# 02 - æ•°æ®å‡†å¤‡

> ç¬¬äºŒéƒ¨åˆ†ï¼šæ·±å…¥ç†è§£æ•°æ®æ ¼å¼å’Œ Reward ç³»ç»Ÿ

---

## ğŸ“š æœ¬ç« å†…å®¹

### ğŸ“– å­¦ä¹ ç¬”è®°

#### **02_æ•°æ®å‡†å¤‡.md** - æ•°æ®æ ¼å¼è¯¦è§£ï¼ˆ10000+ å­—ï¼‰
- verl çš„ Parquet æ•°æ®æ ¼å¼
- 4 ç§ prompt æ ¼å¼è¯¦è§£
- å•è½®å¯¹è¯æ•°æ®å‡†å¤‡
- å¤šè½®å¯¹è¯æ•°æ®å‡†å¤‡ï¼ˆAgentï¼‰
- å¤šæ¨¡æ€æ•°æ®å‡†å¤‡ï¼ˆVLMï¼‰
- æ•°æ®è´¨é‡è¦æ±‚å’Œæœ€ä½³å®è·µ

#### **reward_ç³»ç»Ÿè¯¦è§£.md** - Reward ç³»ç»Ÿæ·±åº¦è§£æï¼ˆæ–°ï¼ï¼‰
- RewardManager æ¶æ„å’Œè°ƒç”¨æµç¨‹
- reward_model é…ç½®è¯¦è§£
- GSM8K Reward æºç åˆ†æ
  - strict å’Œ flexible ä¸¤ç§æå–æ–¹æ³•
  - æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…é€»è¾‘
  - å®Œæ•´çš„è®¡ç®—æµç¨‹è¿½è¸ª
- è‡ªå®šä¹‰ Reward å‡½æ•°å®ç°
  - Rule-based Reward ç¤ºä¾‹
  - Model-based Reward ç¤ºä¾‹
  - ä»£ç ç”Ÿæˆ Reward
  - å¤šç›®æ ‡ Reward
- Reward è°ƒè¯•æŠ€å·§
- å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### ğŸ› ï¸ æ ¸å¿ƒè„šæœ¬

- **data_quality_check.py** - æ•°æ®æ ¼å¼å’Œè´¨é‡æ£€æŸ¥

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ­¥éª¤ 1ï¼šç†è§£æ•°æ®æ ¼å¼

```bash
# æŸ¥çœ‹ GSM8K æ•°æ®ç¤ºä¾‹
python -c "
import pandas as pd
df = pd.read_parquet('~/data/gsm8k/train.parquet')
print(df.head(1))
print(df.columns.tolist())
"
```

### æ­¥éª¤ 2ï¼šæ£€æŸ¥æ•°æ®è´¨é‡

```bash
python data_quality_check.py ~/data/gsm8k/train.parquet
```

### æ­¥éª¤ 3ï¼šå‡†å¤‡è‡ªå·±çš„æ•°æ®

å‚è€ƒ `02_æ•°æ®å‡†å¤‡.md` ç¬¬ 2-4 èŠ‚çš„è¯¦ç»†ç¤ºä¾‹ã€‚

---

## ğŸ“– æ¨èå­¦ä¹ è·¯å¾„

### ç¬¬ 1 å¤©ï¼šæ•°æ®æ ¼å¼ç†è§£

1. **é˜…è¯»** `02_æ•°æ®å‡†å¤‡.md`ï¼ˆ2 å°æ—¶ï¼‰
   - ç†è§£ Parquet æ ¼å¼
   - æŒæ¡ 4 ç§ prompt æ ¼å¼
   - å­¦ä¹ æ•°æ®å‡†å¤‡æµç¨‹

2. **å®è·µ** æŸ¥çœ‹ GSM8K æ•°æ®
   ```bash
   # æŸ¥çœ‹æ•°æ®ç»“æ„
   python -c "
   import pandas as pd
   import json
   df = pd.read_parquet('~/data/gsm8k/train.parquet')
   sample = df.iloc[0].to_dict()
   print(json.dumps(sample, indent=2, ensure_ascii=False))
   "
   ```

3. **ç†è§£** reward_model å­—æ®µ
   - æŸ¥çœ‹ GSM8K çš„ reward_model é…ç½®
   - ç†è§£ä¸åŒ Reward å‡½æ•°çš„ä½œç”¨

### ç¬¬ 2 å¤©ï¼šReward ç³»ç»Ÿæ·±å…¥

1. **é˜…è¯»** `reward_ç³»ç»Ÿè¯¦è§£.md`ï¼ˆ2 å°æ—¶ï¼‰
   - ç†è§£ RewardManager æ¶æ„
   - æŒæ¡ GSM8K Reward å®ç°
   - å­¦ä¹ è‡ªå®šä¹‰ Reward æ–¹æ³•

2. **å®è·µ** è¿½è¸ª Reward è®¡ç®—
   ```python
   # åœ¨ verl/trainer/ppo/reward.py çš„ RewardManager.__call__ ä¸­æ·»åŠ 
   print(f"[Debug] Computing reward for batch_size={len(batch)}")
   print(f"  reward_model config: {batch['reward_model'][0]}")

   # åœ¨ verl/utils/reward_score/gsm8k.py çš„ compute_score ä¸­æ·»åŠ 
   print(f"[Debug] solution: {solution_str}")
   print(f"  ground_truth: {ground_truth}")
   print(f"  extracted: {answer}")
   print(f"  score: {result}")
   ```

### ç¬¬ 3 å¤©ï¼šå®è·µå’Œè°ƒè¯•

1. **å‡†å¤‡** è‡ªå·±çš„æ•°æ®é›†
   - é€‰æ‹©ä¸€ä¸ªä»»åŠ¡ï¼ˆå¦‚ä»£ç ç”Ÿæˆã€æ•°å­¦é—®é¢˜ï¼‰
   - æŒ‰ç…§ Parquet æ ¼å¼å‡†å¤‡æ•°æ®
   - é…ç½® reward_model

2. **å®ç°** è‡ªå®šä¹‰ Reward
   - å‚è€ƒ `reward_ç³»ç»Ÿè¯¦è§£.md` ä¸­çš„ç¤ºä¾‹
   - åœ¨ `verl/utils/reward_score/` ä¸‹åˆ›å»ºæ–°æ–‡ä»¶
   - æµ‹è¯• Reward å‡½æ•°

---

## ğŸ“‹ å­¦ä¹ æ£€æŸ¥æ¸…å•

### æ•°æ®æ ¼å¼ç†è§£ âœ“
- [ ] ç†è§£ Parquet æ ¼å¼å’Œå¿…éœ€å­—æ®µ
- [ ] æŒæ¡ 4 ç§ prompt æ ¼å¼ï¼ˆString, StringList, Chat, ChatListï¼‰
- [ ] ç†è§£ data_source å’Œ reward_model çš„ä½œç”¨
- [ ] èƒ½å¤Ÿæ£€æŸ¥æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®

### Reward ç³»ç»ŸæŒæ¡ âœ“
- [ ] ç†è§£ RewardManager çš„è°ƒç”¨æµç¨‹
- [ ] æŒæ¡ reward_model é…ç½®æ–¹æ³•
- [ ] ç†è§£ GSM8K Reward çš„å®ç°åŸç†
- [ ] èƒ½å¤Ÿé˜…è¯»å’Œç†è§£ Reward å‡½æ•°æºç 
- [ ] çŸ¥é“å¦‚ä½•è°ƒè¯• Reward è®¡ç®—

### æ•°æ®å‡†å¤‡å®è·µ âœ“
- [ ] å‡†å¤‡è¿‡å•è½®å¯¹è¯æ•°æ®
- [ ] ç†è§£å¤šè½®å¯¹è¯æ•°æ®æ ¼å¼
- [ ] ï¼ˆå¯é€‰ï¼‰å‡†å¤‡è¿‡å¤šæ¨¡æ€æ•°æ®
- [ ] èƒ½å¤Ÿå®ç°è‡ªå®šä¹‰ Reward å‡½æ•°

---

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬ç« åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

âœ… æ·±å…¥ç†è§£ verl çš„ Parquet æ•°æ®æ ¼å¼
âœ… æŒæ¡ 4 ç§ prompt æ ¼å¼çš„ä½¿ç”¨åœºæ™¯
âœ… ç†è§£ RewardManager çš„å·¥ä½œåŸç†
âœ… é˜…è¯»å’Œç†è§£ Reward å‡½æ•°æºç 
âœ… å‡†å¤‡å„ç§ç±»å‹çš„è®­ç»ƒæ•°æ®
âœ… å®ç°è‡ªå®šä¹‰ Reward å‡½æ•°
âœ… è°ƒè¯• Reward è®¡ç®—é—®é¢˜

---

## ğŸ’¡ é‡ç‚¹å†…å®¹

### æ•°æ®æ ¼å¼çš„ 3 ä¸ªå¿…éœ€å­—æ®µ

```python
{
    "data_source": "gsm8k",              # æ•°æ®æ¥æºæ ‡è¯†
    "prompt": "What is 2+2?",            # è¾“å…¥ï¼ˆ4 ç§æ ¼å¼ä¹‹ä¸€ï¼‰
    "reward_model": {                    # Reward è®¡ç®—é…ç½®
        "style": "rule",
        "module": "verl.utils.reward_score.gsm8k",
        "ground_truth": "4"
    }
}
```

### GSM8K Reward çš„ä¸¤ç§æå–æ–¹æ³•

**strict æ–¹æ³•**ï¼ˆæ¨èï¼‰ï¼š
```python
# åŒ¹é… "#### number" æ ¼å¼
solutions = re.findall(r"#### (\-?[0-9\.\,]+)", solution_str)
# ç¤ºä¾‹ï¼š
# "#### 42" â†’ "42"
# "#### -3.14" â†’ "-3.14"
```

**flexible æ–¹æ³•**ï¼š
```python
# æå–æœ€åä¸€ä¸ªæ•°å­—
numbers = re.findall(r"(\-?[0-9\.\,]+)", solution_str)
# ç¤ºä¾‹ï¼š
# "The answer is 42." â†’ "42"
# "We get 3.14 meters" â†’ "3.14"
```

### Reward é…ç½®çš„ 3 ç§ style

```yaml
# 1. rule-based
reward_model:
  style: "rule"
  module: "verl.utils.reward_score.gsm8k"

# 2. model-based
reward_model:
  style: "model"
  path: "path/to/reward_model"

# 3. sandboxï¼ˆä»£ç æ‰§è¡Œï¼‰
reward_model:
  style: "sandbox"
  language: "python"
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: prompt åº”è¯¥ç”¨ä»€ä¹ˆæ ¼å¼ï¼Ÿ

**æ¨èä½¿ç”¨ Chat æ ¼å¼**ï¼ˆåˆ—è¡¨å½¢å¼ï¼‰ï¼š
```python
"prompt": [{"role": "user", "content": "è§£è¿™é“é¢˜..."}]
```

å…¶ä»–æ ¼å¼è§ `02_æ•°æ®å‡†å¤‡.md` ç¬¬ 1.3 èŠ‚ã€‚

### Q2: reward_model çš„ 3 ä¸ªå­—æ®µéƒ½å¿…éœ€å—ï¼Ÿ

**å¿…éœ€å­—æ®µ**ï¼š
- `style`: "rule" | "model" | "sandbox"

**æ ¹æ® style ä¸åŒ**ï¼š
- rule â†’ éœ€è¦ `module` æŒ‡å®šè®¡ç®—å‡½æ•°
- model â†’ éœ€è¦ `path` æŒ‡å®šæ¨¡å‹è·¯å¾„
- sandbox â†’ éœ€è¦ `language` æŒ‡å®šæ‰§è¡Œè¯­è¨€

### Q3: GSM8K çš„ Reward æ€ä¹ˆè®¡ç®—ï¼Ÿ

æŸ¥çœ‹ `reward_ç³»ç»Ÿè¯¦è§£.md` ç¬¬ 2 èŠ‚ï¼Œæ ¸å¿ƒé€»è¾‘ï¼š
1. ç”¨æ­£åˆ™æå–ç­”æ¡ˆï¼ˆ`#### number`ï¼‰
2. ä¸ ground_truth æ¯”è¾ƒ
3. ç›¸ç­‰è¿”å› 1.0ï¼Œå¦åˆ™è¿”å› 0.0

### Q4: å¦‚ä½•è°ƒè¯• Reward è®¡ç®—é”™è¯¯ï¼Ÿ

**æ–¹æ³• 1ï¼šæŸ¥çœ‹ Reward æ—¥å¿—**
```python
# verl/trainer/ppo/reward.py æ·»åŠ 
print(f"[Debug] Reward: {rewards}")
```

**æ–¹æ³• 2ï¼šå•ç‹¬æµ‹è¯• Reward å‡½æ•°**
```python
from verl.utils.reward_score.gsm8k import compute_score
score = compute_score("#### 42", "42")
print(score)  # åº”è¯¥æ˜¯ 1.0
```

è¯¦è§ `reward_ç³»ç»Ÿè¯¦è§£.md` ç¬¬ 5 èŠ‚ã€‚

### Q5: å¦‚ä½•å‡†å¤‡å¤šè½®å¯¹è¯æ•°æ®ï¼Ÿ

**ä½¿ç”¨ ChatList æ ¼å¼**ï¼š
```python
"prompt": [
    [
        {"role": "user", "content": "ç¬¬ä¸€è½®é—®é¢˜"},
        {"role": "assistant", "content": "ç¬¬ä¸€è½®å›ç­”"},
        {"role": "user", "content": "ç¬¬äºŒè½®é—®é¢˜"}
    ]
]
```

è¯¦è§ `02_æ•°æ®å‡†å¤‡.md` ç¬¬ 3 èŠ‚ã€‚

---

## ğŸ”— ç›¸å…³èµ„æº

### æœ¬åœ°æ–‡ä»¶
- æ•°æ®æ ¼å¼è¯¦è§£: `02_æ•°æ®å‡†å¤‡.md`
- Reward ç³»ç»Ÿè¯¦è§£: `reward_ç³»ç»Ÿè¯¦è§£.md`
- é¡¹ç›®æ¦‚è§ˆ: `../../CLAUDE.md`
- å®Œæ•´å­¦ä¹ è·¯çº¿: `../../LEARNING_GUIDE.md`
- ç¬¬ä¸€éƒ¨åˆ†: `../01_å¿«é€Ÿä¸Šæ‰‹/`

### å®˜æ–¹æ–‡æ¡£
- [Prepare Data](https://verl.readthedocs.io/en/latest/preparation/prepare_data.html)
- [Reward Function](https://verl.readthedocs.io/en/latest/preparation/reward_function.html)

### ä»£ç ä½ç½®
- æ•°æ®é¢„å¤„ç†ç¤ºä¾‹: `examples/data_preprocess/`
- RewardManager: `verl/trainer/ppo/reward.py`
- Reward å‡½æ•°åº“: `verl/utils/reward_score/`
- æ•°æ®åŠ è½½å™¨: `verl/utils/dataset/rl_dataset.py`

---

## â­ï¸ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬ç« åï¼Œç»§ç»­å­¦ä¹ ï¼š
- **03 - RL ç®—æ³•**: æ·±å…¥ç†è§£ GRPOã€PPOã€RLOO ç­‰ç®—æ³•å®ç°
- **04 - Reward è®¾è®¡**: æ›´å¤šè‡ªå®šä¹‰ Reward ç¤ºä¾‹å’Œæœ€ä½³å®è·µ
- **05 - Agent RL**: å·¥å…·è°ƒç”¨å’Œå¤šè½®å¯¹è¯è®­ç»ƒ

---

*åˆ›å»ºæ—¶é—´: 2026-01-25*
*é¢„è®¡å®Œæˆæ—¶é—´: 2-3 å¤©*
