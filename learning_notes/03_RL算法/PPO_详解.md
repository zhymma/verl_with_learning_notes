# PPO ç®—æ³•è¯¦è§£

> Proximal Policy Optimization - Actor-Critic å¼ºåŒ–å­¦ä¹ ç®—æ³•

---

## ğŸ“– ç›®å½•

1. [PPO æ ¸å¿ƒæ€æƒ³](#1-ppo-æ ¸å¿ƒæ€æƒ³)
2. [GAE ä¼˜åŠ¿ä¼°è®¡æºç è§£æ](#2-gae-ä¼˜åŠ¿ä¼°è®¡æºç è§£æ)
3. [PPO Clipped Objective æºç è§£æ](#3-ppo-clipped-objective-æºç è§£æ)
   - 3.6 [æ·±åº¦è§£æï¼šold_log_prob vs new_log_prob ä¸ Importance Sampling](#36-æ·±åº¦è§£æold_log_prob-vs-new_log_prob-ä¸-importance-sampling-) â­
4. [å®Œæ•´è®­ç»ƒæµç¨‹](#4-å®Œæ•´è®­ç»ƒæµç¨‹)
5. [é…ç½®å‚æ•°è¯¦è§£](#5-é…ç½®å‚æ•°è¯¦è§£)
6. [KL æ•£åº¦æ§åˆ¶](#6-kl-æ•£åº¦æ§åˆ¶)
   - 6.4 [æ·±åº¦è§£æï¼šref_log_probs è®¡ç®—ä¸ KL åŒé‡æƒ©ç½šæœºåˆ¶](#64-æ·±åº¦è§£æref_log_probs-è®¡ç®—ä¸-kl-åŒé‡æƒ©ç½šæœºåˆ¶-) â­
7. [è°ƒè¯•æŠ€å·§](#7-è°ƒè¯•æŠ€å·§)
8. [å¸¸è§é—®é¢˜](#8-å¸¸è§é—®é¢˜)

---

## 1. PPO æ ¸å¿ƒæ€æƒ³

### 1.1 ä»€ä¹ˆæ˜¯ PPOï¼Ÿ

**PPO (Proximal Policy Optimization)** æ˜¯ OpenAI åœ¨ 2017 å¹´æå‡ºçš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œå¹³è¡¡äº†ç®€å•æ€§ã€ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚

**æ ¸å¿ƒç‰¹ç‚¹ï¼š**
- âœ… **Actor-Critic æ¶æ„**ï¼šåŒæ—¶è®­ç»ƒç­–ç•¥ç½‘ç»œå’Œä»·å€¼ç½‘ç»œ
- âœ… **Clipped Surrogate Objective**ï¼šé™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦
- âœ… **GAE ä¼˜åŠ¿ä¼°è®¡**ï¼šå¹³è¡¡ bias å’Œ variance
- âœ… **è®­ç»ƒç¨³å®šæ€§é«˜**ï¼šé¿å…ç¾éš¾æ€§çš„ç­–ç•¥å´©æºƒ

### 1.2 PPO vs GRPO

| ç‰¹æ€§ | PPO | GRPO |
|------|-----|------|
| **Critic æ¨¡å‹** | âœ… éœ€è¦ï¼ˆä»·å€¼å‡½æ•°ï¼‰ | âŒ ä¸éœ€è¦ |
| **Baseline** | Critic çš„ V(s) | ç»„å†…æ ·æœ¬å‡å€¼ |
| **ä¼˜åŠ¿ä¼°è®¡** | GAEï¼ˆæ—¶åºå·®åˆ†ï¼‰ | ç›¸å¯¹äºç»„å‡å€¼ |
| **GPU æ˜¾å­˜** | æ›´å¤šï¼ˆActor + Criticï¼‰ | æ›´å°‘ï¼ˆåªæœ‰ Actorï¼‰ |
| **è®­ç»ƒç¨³å®šæ€§** | æ›´ç¨³å®š | ä¾èµ– rollout.n |
| **é€‚ç”¨åœºæ™¯** | è¿‡ç¨‹å¯¼å‘ä»»åŠ¡ã€é•¿åºåˆ— | ç»“æœå¯¼å‘ä»»åŠ¡ |

### 1.3 PPO ä¸‰å¤§ç»„ä»¶

```
1. Critic æ¨¡å‹
   â””â”€ ä¼°è®¡çŠ¶æ€ä»·å€¼ V(s)
   â””â”€ æä¾› baseline å‡å°‘æ–¹å·®

2. GAE ä¼˜åŠ¿ä¼°è®¡
   â””â”€ è®¡ç®— A(s,a) = Q(s,a) - V(s)
   â””â”€ ä½¿ç”¨ Î» å¹³è¡¡ bias å’Œ variance

3. Clipped Objective
   â””â”€ é™åˆ¶ç­–ç•¥æ¯”ç‡åœ¨ [1-Îµ, 1+Îµ]
   â””â”€ é˜²æ­¢è¿‡å¤§çš„ç­–ç•¥æ›´æ–°
```

**å…³é”®é…ç½®ï¼š**
- `algorithm.adv_estimator=gae`
- `critic.model.path="Qwen/Qwen2.5-7B-Instruct"`
- `algorithm.gamma=0.99`ï¼ˆæŠ˜æ‰£å› å­ï¼‰
- `algorithm.lam=0.95`ï¼ˆGAE lambdaï¼‰

---

## 2. GAE ä¼˜åŠ¿ä¼°è®¡æºç è§£æ

### 2.1 å‡½æ•°ç­¾å

```python
# ä½ç½®: verl/trainer/ppo/core_algos.py:214-262

@register_adv_est(AdvantageEstimator.GAE)
def compute_gae_advantage_return(
    token_level_rewards: torch.Tensor,  # (bs, response_length)
    values: torch.Tensor,               # (bs, response_length) - Critic è¾“å‡º
    response_mask: torch.Tensor,        # (bs, response_length)
    gamma: torch.Tensor,                # æŠ˜æ‰£å› å­ï¼Œå¦‚ 0.99
    lam: torch.Tensor,                  # GAE lambdaï¼Œå¦‚ 0.95
):
```

**å‚æ•°è¯´æ˜ï¼š**
- `token_level_rewards`: æ¯ä¸ª token çš„å¥–åŠ±
- `values`: Critic æ¨¡å‹é¢„æµ‹çš„ä»·å€¼å‡½æ•° V(s)
- `gamma`: æŠ˜æ‰£å› å­ï¼ˆæœªæ¥å¥–åŠ±çš„æƒé‡ï¼‰
- `lam`: GAE çš„ Î» å‚æ•°ï¼ˆbias-variance tradeoffï¼‰

### 2.2 GAE æ ¸å¿ƒå…¬å¼

**TD-errorï¼ˆæ—¶åºå·®åˆ†è¯¯å·®ï¼‰ï¼š**
```
Î´_t = r_t + Î³ * V(s_{t+1}) - V(s_t)
```

**GAE ä¼˜åŠ¿å€¼ï¼š**
```
A_t = Î´_t + Î³Î» * A_{t+1}
    = Î´_t + Î³Î» * Î´_{t+1} + (Î³Î»)Â² * Î´_{t+2} + ...
```

**å‚æ•°è§£é‡Šï¼š**
- `Î³=1, Î»=1`: Monte Carloï¼ˆæ— åï¼Œé«˜æ–¹å·®ï¼‰
- `Î³>0, Î»=0`: 1-step TDï¼ˆä½æ–¹å·®ï¼Œæœ‰åï¼‰
- `Î³=0.99, Î»=0.95`: æŠ˜ä¸­é€‰æ‹©ï¼ˆé»˜è®¤ï¼‰

### 2.3 ç¬¬ 1 æ­¥ï¼šåˆå§‹åŒ–

```python
# ä»£ç ä½ç½®: verl/trainer/ppo/core_algos.py:243-247
with torch.no_grad():
    nextvalues = 0           # V(s_{t+1})
    lastgaelam = 0           # A_{t+1}
    advantages_reversed = [] # å­˜å‚¨å€’åºçš„ä¼˜åŠ¿å€¼
    gen_len = token_level_rewards.shape[-1]
```

**ä½œç”¨ï¼š**
ä»åºåˆ—æœ«å°¾å¼€å§‹ï¼Œåå‘è®¡ç®—ä¼˜åŠ¿å€¼ã€‚

### 2.4 ç¬¬ 2 æ­¥ï¼šé€†åºå¾ªç¯è®¡ç®—

```python
# ä»£ç ä½ç½®: verl/trainer/ppo/core_algos.py:249-257
for t in reversed(range(gen_len)):
    # TD-error: Î´_t = r_t + Î³ * V_{t+1} - V_t
    delta = token_level_rewards[:, t] + gamma * nextvalues - values[:, t]

    # GAE: A_t = Î´_t + Î³Î» * A_{t+1}
    lastgaelam_ = delta + gamma * lam * lastgaelam

    # åªåœ¨å“åº” token ä¸Šæ›´æ–°ï¼ˆè·³è¿‡ promptï¼‰
    nextvalues = values[:, t] * response_mask[:, t] + (1 - response_mask[:, t]) * nextvalues
    lastgaelam = lastgaelam_ * response_mask[:, t] + (1 - response_mask[:, t]) * lastgaelam

    advantages_reversed.append(lastgaelam)
```

**å…³é”®ç‚¹ï¼š**
- `delta`: TD-errorï¼Œè¡¡é‡ Critic çš„é¢„æµ‹è¯¯å·®
- `lastgaelam_`: æ–°çš„ä¼˜åŠ¿å€¼
- `response_mask`: ç¡®ä¿åªåœ¨å“åº”éƒ¨åˆ†è®¡ç®—ä¼˜åŠ¿

### 2.5 å®Œæ•´è®¡ç®—ç¤ºä¾‹

**å‡è®¾ï¼š**
```python
# åºåˆ—é•¿åº¦ T=4ï¼ˆ2 ä¸ª prompt token + 2 ä¸ª response tokenï¼‰
token_level_rewards = [0, 0, 0, 1.0]  # åªæœ‰æœ€åä¸€ä¸ª token æœ‰å¥–åŠ±
values = [0.2, 0.3, 0.4, 0.5]        # Critic é¢„æµ‹
response_mask = [0, 0, 1, 1]         # å 2 ä¸ªæ˜¯å“åº”
gamma = 0.99
lam = 0.95
```

**é€†åºè®¡ç®—ï¼š**

**t=3ï¼ˆæœ€åä¸€ä¸ª tokenï¼‰ï¼š**
```python
delta_3 = 1.0 + 0.99 * 0 - 0.5 = 0.5
A_3 = 0.5 + 0.99 * 0.95 * 0 = 0.5
```

**t=2ï¼š**
```python
delta_2 = 0 + 0.99 * 0.5 - 0.4 = 0.095
A_2 = 0.095 + 0.99 * 0.95 * 0.5 = 0.565
```

**t=1ï¼ˆprompt tokenï¼Œè·³è¿‡ï¼‰ï¼š**
```python
# ç”±äº response_mask[1]=0ï¼ŒA_1 ä¸æ›´æ–°
A_1 = 0
```

**t=0ï¼ˆprompt tokenï¼Œè·³è¿‡ï¼‰ï¼š**
```python
A_0 = 0
```

**æœ€ç»ˆä¼˜åŠ¿å€¼ï¼š**
```python
advantages = [0, 0, 0.565, 0.5]
```

### 2.6 ç¬¬ 3 æ­¥ï¼šå½’ä¸€åŒ–å’Œè¿”å›

```python
# ä»£ç ä½ç½®: verl/trainer/ppo/core_algos.py:258-262
advantages = torch.stack(advantages_reversed[::-1], dim=1)

# è®¡ç®— returnsï¼ˆç”¨äºè®­ç»ƒ Criticï¼‰
returns = advantages + values

# ç™½åŒ–ï¼ˆWhiteningï¼‰ä¼˜åŠ¿å€¼
advantages = verl_F.masked_whiten(advantages, response_mask)

return advantages, returns
```

**ä½œç”¨ï¼š**
- `returns`: ç”¨äº Critic çš„ç›®æ ‡å€¼ï¼ˆMSE lossï¼‰
- `advantages`: å½’ä¸€åŒ–åç”¨äº Actor æ›´æ–°

**masked_whiten å®ç°ï¼š**
```python
def masked_whiten(values, mask):
    mean = masked_mean(values, mask)
    std = masked_std(values, mask)
    return (values - mean) / (std + 1e-8)
```

---

## 3. PPO Clipped Objective æºç è§£æ

### 3.1 å‡½æ•°ç­¾å

```python
# ä½ç½®: verl/trainer/ppo/core_algos.py:1095-1156

def compute_policy_loss_clip(
    old_log_prob: torch.Tensor,    # æ—§ç­–ç•¥çš„ log prob
    log_prob: torch.Tensor,        # æ–°ç­–ç•¥çš„ log prob
    advantages: torch.Tensor,      # GAE è®¡ç®—çš„ä¼˜åŠ¿å€¼
    response_mask: torch.Tensor,   # å“åº” mask
    cliprange: float,              # Îµï¼ˆclip ratioï¼‰ï¼Œå¦‚ 0.2
    clip_ratio_c: float = 3.0,     # Dual-clip çš„ä¸‹ç•Œ
    loss_agg_mode: str = "token-mean",
):
```

### 3.2 ç¬¬ 1 æ­¥ï¼šè®¡ç®—æ¦‚ç‡æ¯”

```python
# ä»£ç ä½ç½®: verl/trainer/ppo/core_algos.py:1128-1132
negative_approx_kl = log_prob - old_log_prob
negative_approx_kl = torch.clamp(negative_approx_kl, min=-20.0, max=20.0)
ratio = torch.exp(negative_approx_kl)
ppo_kl = verl_F.masked_mean(-negative_approx_kl, response_mask)
```

**ä½œç”¨ï¼š**
è®¡ç®—æ–°æ—§ç­–ç•¥çš„æ¦‚ç‡æ¯” `ratio = Ï€_new / Ï€_old`

**ç¤ºä¾‹ï¼š**
```python
old_log_prob = -2.0  # log(Ï€_old(a|s)) = -2.0 â†’ Ï€_old = 0.135
log_prob = -1.5      # log(Ï€_new(a|s)) = -1.5 â†’ Ï€_new = 0.223
ratio = exp(-1.5 - (-2.0)) = exp(0.5) = 1.65
```

**è§£é‡Šï¼š**
- `ratio > 1`: æ–°ç­–ç•¥å¢åŠ äº†è¯¥åŠ¨ä½œçš„æ¦‚ç‡
- `ratio < 1`: æ–°ç­–ç•¥é™ä½äº†è¯¥åŠ¨ä½œçš„æ¦‚ç‡

### 3.3 ç¬¬ 2 æ­¥ï¼šè®¡ç®— Clipped Loss

**æ ‡å‡† PPO Clipï¼š**
```python
# ä»£ç ä½ç½®: verl/trainer/ppo/core_algos.py:1134-1145
pg_losses1 = -advantages * ratio
pg_losses2 = -advantages * torch.clamp(ratio, 1 - cliprange, 1 + cliprange)
clip_pg_losses1 = torch.maximum(pg_losses1, pg_losses2)
pg_clipfrac = verl_F.masked_mean(torch.gt(pg_losses2, pg_losses1).float(), response_mask)
```

**å…¬å¼ï¼š**
```
L^CLIP = -min(ratio * A, clip(ratio, 1-Îµ, 1+Îµ) * A)
```

**å›¾ç¤ºï¼ˆcliprange=0.2ï¼‰ï¼š**
```
A > 0ï¼ˆå¥½çš„åŠ¨ä½œï¼‰:
  å¦‚æœ ratio > 1.2ï¼Œclip åˆ° 1.2ï¼ˆé™åˆ¶å¢å¼ºå¹…åº¦ï¼‰

A < 0ï¼ˆåçš„åŠ¨ä½œï¼‰:
  å¦‚æœ ratio < 0.8ï¼Œclip åˆ° 0.8ï¼ˆé™åˆ¶æƒ©ç½šå¹…åº¦ï¼‰
```

### 3.4 ç¬¬ 3 æ­¥ï¼šDual-clip PPOï¼ˆå¯é€‰ï¼‰

```python
# ä»£ç ä½ç½®: verl/trainer/ppo/core_algos.py:1147-1153
pg_losses3 = -advantages * clip_ratio_c
clip_pg_losses2 = torch.min(pg_losses3, clip_pg_losses1)
pg_clipfrac_lower = verl_F.masked_mean(
    torch.gt(clip_pg_losses1, pg_losses3) * (advantages < 0).float(), response_mask
)
pg_losses = torch.where(advantages < 0, clip_pg_losses2, clip_pg_losses1)
```

**ä½œç”¨ï¼š**
å½“ `A < 0` æ—¶ï¼Œè¿›ä¸€æ­¥é™åˆ¶ ratio çš„ä¸‹ç•Œä¸º `-clip_ratio_c`

**Dual-clip å…¬å¼ï¼š**
```
å½“ A < 0 æ—¶:
  L = -max(min(ratio * A, clip(ratio, 1-Îµ, 1+Îµ) * A), c * A)
```

### 3.5 å®Œæ•´ç¤ºä¾‹

**è¾“å…¥ï¼š**
```python
old_log_prob = -2.0
log_prob = -1.5
advantage = 0.5  # å¥½çš„åŠ¨ä½œ
cliprange = 0.2
```

**è®¡ç®—ï¼š**
```python
ratio = exp(-1.5 - (-2.0)) = 1.65

# Loss 1: ä¸ clip
loss1 = -0.5 * 1.65 = -0.825

# Loss 2: clip ratio åˆ° [0.8, 1.2]
clipped_ratio = min(max(1.65, 0.8), 1.2) = 1.2
loss2 = -0.5 * 1.2 = -0.6

# å– maxï¼ˆloss è¶Šå¤§ï¼Œæ¢¯åº¦è¶Šå°ï¼‰
final_loss = max(-0.825, -0.6) = -0.6
```

**è§£é‡Šï¼š**
- ç”±äº ratio=1.65 > 1.2ï¼Œè¢« clip åˆ° 1.2
- é™åˆ¶äº†ç­–ç•¥æ›´æ–°çš„å¹…åº¦ï¼Œé˜²æ­¢è¿‡åº¦ä¼˜åŒ–

### 3.6 æ·±åº¦è§£æï¼šold_log_prob vs new_log_prob ä¸ Importance Sampling â­

#### 3.6.1 æ ¸å¿ƒæ¦‚å¿µï¼šä¸¤ä¸ªä¸åŒçš„ log_prob

PPO çš„è®­ç»ƒä¸­æ¶‰åŠ **ä¸¤ä¸ªå…³é”®çš„ log æ¦‚ç‡**ï¼Œå®ƒä»¬åœ¨ä¸åŒæ—¶æœºè®¡ç®—ï¼ŒæœåŠ¡äºä¸åŒçš„ç›®çš„ï¼š

```
æ—¶é—´çº¿ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Rollout   â”‚ Compute old_log  â”‚ Mini-batch Training    â”‚
â”‚            â”‚                  â”‚ (é‡å¤å¤šæ¬¡)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ï€_rollout  â”‚  Ï€_old (å†»ç»“)    â”‚  Ï€_Î¸ (ä¸æ–­æ›´æ–°)        â”‚
â”‚ (vLLM)     â”‚  (FSDP)          â”‚  (FSDP)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†‘
                                new_log_prob
```

#### 3.6.2 old_log_probï¼ˆÏ€_oldï¼‰- è¿‘ç«¯é”šç‚¹

**ä½ç½®**ï¼š`verl/trainer/ppo/ray_trainer.py:1258-1281`

```python
def _compute_old_log_prob(self, batch: DataProto):
    """
    è®¡ç®— old_log_probï¼šè®­ç»ƒå¼€å§‹å‰çš„ç­–ç•¥å¿«ç…§
    è¿™æ˜¯ PPO çš„"è¿‘ç«¯é”šç‚¹"ï¼ˆproximal anchorï¼‰
    """

    # 1. è½¬æ¢æ•°æ®æ ¼å¼
    batch_td = batch.to_tensordict()
    batch_td = left_right_2_no_padding(batch_td)

    # 2. è®¾ç½®å…ƒæ•°æ®
    tu.assign_non_tensor(
        batch_td,
        calculate_entropy=True,
        compute_loss=False
    )

    # 3. ä½¿ç”¨å½“å‰ Actor é‡æ–°è®¡ç®— log_prob
    output = self.actor_rollout_wg.compute_log_prob(batch_td)

    # 4. æå–ç»“æœ
    old_log_probs = tu.get(output, "log_probs")
    entropy = tu.get(output, "entropy")

    return old_log_probs, entropy
```

**å…³é”®ç‰¹ç‚¹**ï¼š
- **è®¡ç®—æ—¶æœº**ï¼šæ¯ä¸ª batch å¼€å§‹æ—¶è®¡ç®— **ä¸€æ¬¡**
- **ç­–ç•¥ç‰ˆæœ¬**ï¼šå½“å‰ Actor çš„æƒé‡ï¼ˆè®­ç»ƒå‰çš„å¿«ç…§ï¼‰
- **ä½œç”¨**ï¼šåœ¨æ•´ä¸ª mini-batch è®­ç»ƒæœŸé—´ **ä¿æŒä¸å˜**
- **ç›®çš„**ï¼šä½œä¸º PPO çš„"è¿‘ç«¯é”šç‚¹"ï¼Œé™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦

#### 3.6.3 new_log_probï¼ˆÏ€_Î¸ï¼‰- ä¼˜åŒ–ç›®æ ‡

**ä½ç½®**ï¼š`verl/workers/utils/losses.py:97-174`

```python
def ppo_loss(config, model_output, data):
    """PPO Loss è®¡ç®—"""

    # 1. ä»å½“å‰æ¨¡å‹çš„å‰å‘ä¼ æ’­è·å– log_prob
    log_prob = model_output["log_probs"]  # â† new_log_prob (Ï€_Î¸)

    # 2. ä»æ•°æ®ä¸­è·å– old_log_prob
    old_log_prob = data["old_log_probs"]  # â† å†»ç»“çš„å‚è€ƒ

    # 3. è®¡ç®— importance sampling ratio
    negative_approx_kl = log_prob - old_log_prob
    ratio = torch.exp(negative_approx_kl)  # Ï€_Î¸ / Ï€_old

    # 4. PPO Clipping
    advantages = data["advantages"]
    surr1 = ratio * advantages
    surr2 = torch.clamp(ratio, 1-Îµ, 1+Îµ) * advantages
    policy_loss = -torch.min(surr1, surr2).mean()

    return policy_loss
```

**å…³é”®ç‰¹ç‚¹**ï¼š
- **è®¡ç®—æ—¶æœº**ï¼šæ¯ä¸ª mini-batch çš„æ¯æ¬¡å‰å‘ä¼ æ’­
- **ç­–ç•¥ç‰ˆæœ¬**ï¼šå½“å‰æ­£åœ¨ä¼˜åŒ–çš„ Actor æƒé‡ï¼ˆä¸æ–­æ›´æ–°ï¼‰
- **ä½œç”¨**ï¼šåœ¨ mini-batch è®­ç»ƒä¸­ **ä¸æ–­å˜åŒ–**
- **ç›®çš„**ï¼šè¿™æ˜¯æˆ‘ä»¬è¦ä¼˜åŒ–çš„ç›®æ ‡ç­–ç•¥

#### 3.6.4 Importance Sampling Ratio è®¡ç®—

**ä½ç½®**ï¼š`verl/trainer/ppo/core_algos.py:1210-1226`

```python
def vanilla_ppo_policy_loss(old_log_prob, log_prob, advantages, ...):
    """æ ‡å‡† PPO çš„ importance sampling ratio è®¡ç®—"""

    # 1. è®¡ç®— log ratioï¼ˆæ•°å€¼ç¨³å®šï¼‰
    negative_approx_kl = log_prob - old_log_prob
    negative_approx_kl = torch.clamp(negative_approx_kl, min=-20, max=20)

    # 2. è®¡ç®— importance sampling ratio
    ratio = torch.exp(negative_approx_kl)  # r = Ï€_Î¸ / Ï€_old

    # 3. PPO Clipping
    cliprange_low = config.clip_ratio       # 0.2
    cliprange_high = config.clip_ratio_high  # 0.2

    pg_losses1 = -advantages * ratio  # æœªè£å‰ª
    pg_losses2 = -advantages * torch.clamp(
        ratio, 1-cliprange_low, 1+cliprange_high
    )  # è£å‰ªåˆ° [0.8, 1.2]

    # 4. å–è¾ƒå¤§çš„ lossï¼ˆæ›´ä¿å®ˆï¼‰
    clip_pg_losses1 = torch.maximum(pg_losses1, pg_losses2)

    return policy_loss
```

**å…¬å¼è¯¦è§£**ï¼š

**Importance Sampling Ratio**ï¼š
$$r_t = \frac{\pi_\theta(a_t|s_t)}{\pi_{\text{old}}(a_t|s_t)} = \exp(\log \pi_\theta - \log \pi_{\text{old}})$$

**PPO Clipped Objective**ï¼š
$$L^{\text{CLIP}}(\theta) = \mathbb{E}[\min(r_t A_t, \text{clip}(r_t, 1-\epsilon, 1+\epsilon) A_t)]$$

å…¶ä¸­ï¼š
- $r_t$ = ratioï¼ˆé‡è¦æ€§é‡‡æ ·æ¯”ç‡ï¼‰
- $A_t$ = advantagesï¼ˆä¼˜åŠ¿å‡½æ•°ï¼‰
- $\epsilon$ = clip_ratioï¼ˆé»˜è®¤ 0.2ï¼‰

#### 3.6.5 ä¸ºä»€ä¹ˆéœ€è¦ä¸¤ä¸ª log_probï¼Ÿ

##### **åŸå›  1ï¼šä¿¡ä»»åŸŸæœºåˆ¶ï¼ˆTrust Regionï¼‰**

```python
# å¦‚æœåªæœ‰ new_log_probï¼Œæ²¡æœ‰ old_log_probï¼š
# âŒ æ— æ³•è®¡ç®— ratio = Ï€_Î¸ / Ï€_old
# âŒ æ— æ³•å®æ–½ clipping
# âŒ ç­–ç•¥å¯èƒ½å‰§çƒˆå˜åŒ–ï¼Œå¯¼è‡´å´©æºƒ

# æœ‰äº† old_log_prob å’Œ new_log_probï¼š
ratio = exp(new_log_prob - old_log_prob)
ratio_clipped = clamp(ratio, 0.8, 1.2)  # é™åˆ¶åœ¨ Â±20%

# âœ… ç­–ç•¥å˜åŒ–è¢«é™åˆ¶åœ¨ Â±20%
# âœ… è®­ç»ƒç¨³å®šï¼Œé¿å…ç¾éš¾æ€§é—å¿˜
```

##### **åŸå›  2ï¼šMini-batch è®­ç»ƒçš„ç¨³å®šæ€§**

```python
# ä¸€ä¸ª batch ä¼šè¿›è¡Œå¤šæ¬¡ mini-batch æ›´æ–°
for epoch in range(ppo_epochs):  # é€šå¸¸ 1-4 æ¬¡
    for mini_batch in dataloader:
        # new_log_prob æ¯æ¬¡éƒ½æ”¹å˜
        # ä½† old_log_prob ä¿æŒä¸å˜ï¼

        ratio = Ï€_Î¸ / Ï€_old  # å§‹ç»ˆç›¸å¯¹äºåŒä¸€ä¸ªå‚è€ƒç‚¹

        # è¿™ç¡®ä¿äº†ï¼š
        # 1. Advantages ä¸ä¼šå˜å¾—"è¿‡æ—¶"
        # 2. ä¼˜åŒ–è¿‡ç¨‹æœ‰æ˜ç¡®çš„é”šç‚¹
        # 3. æ¯æ¬¡æ›´æ–°éƒ½æ˜¯ç›¸å¯¹äºåŒä¸€åŸºå‡†
```

##### **åŸå›  3ï¼šé˜²æ­¢ Advantage å¤±æ•ˆ**

```python
# Advantages åŸºäº rollout æ—¶çš„ rewards è®¡ç®—
# å¦‚æœç­–ç•¥å˜åŒ–å¤ªå¤§ï¼Œadvantages å°±ä¸å†æœ‰æ•ˆ

# ç¤ºä¾‹ï¼š
old_policy: "The answer is 42"  â†’ A = 0.5
# å¦‚æœå…è®¸ç­–ç•¥å‰§çƒˆå˜åŒ–ï¼š
new_policy: "blah blah blah"    â†’ å®Œå…¨ä¸åŒçš„åˆ†å¸ƒï¼

# PPO é€šè¿‡ clipping é˜²æ­¢è¿™ç§æƒ…å†µï¼š
# ratio > 1.2 â†’ è£å‰ªåˆ° 1.2
# ratio < 0.8 â†’ è£å‰ªåˆ° 0.8
# ä¿è¯ç­–ç•¥å˜åŒ–åœ¨åˆç†èŒƒå›´å†…
```

#### 3.6.6 å®Œæ•´ç¤ºä¾‹ï¼šè¿½è¸ªä¸€ä¸ª Batch

```python
# å‡è®¾ï¼šbatch_size=4, seq_len=10, ppo_epochs=2

# ==================== Rollout ====================
prompts = ["What is 2+2?", "What is 3+3?", ...]
responses = vllm_generate(prompts)

# ==================== Compute old_log_prob ====================
old_log_prob = actor_model.compute_log_prob(responses)
# shape: [4, 10]
print(f"old_log_prob[0, :5]: {old_log_prob[0, :5]}")
# è¾“å‡º: [-2.3, -1.8, -0.9, -0.5, -0.2]

# ==================== Advantages ====================
advantages = compute_gae(rewards, values)

# ==================== Mini-batch Training ====================
# Epoch 1
for step in range(100):  # æ¨¡æ‹Ÿ 100 æ­¥ä¼˜åŒ–
    # å‰å‘ä¼ æ’­
    new_log_prob = actor_model(responses)  # â† æ¯æ¬¡éƒ½è®¡ç®—

    # step 1:  [-2.3, -1.8, -0.9, -0.5, -0.2]  (åˆå§‹æ¥è¿‘)
    # step 50: [-2.2, -1.7, -0.8, -0.4, -0.1]  (å¼€å§‹å˜åŒ–)
    # step 100:[-2.1, -1.6, -0.7, -0.3, 0.0]   (ç»§ç»­å˜åŒ–)

    # è®¡ç®— ratioï¼ˆå§‹ç»ˆç›¸å¯¹äº old_log_probï¼‰
    ratio = exp(new_log_prob - old_log_prob)

    # step 1:  [1.0, 1.0, 1.0, 1.0, 1.0]  (å‡ ä¹æ²¡å˜)
    # step 50: [1.1, 1.1, 1.1, 1.1, 1.2]  (å¼€å§‹åç¦»)
    # step 100:[1.2, 1.2, 1.2, 1.2, 1.2]  (æ¥è¿‘è¾¹ç•Œ)

    # Clipping
    ratio_clipped = clamp(ratio, 0.8, 1.2)
    # step 100:[1.2, 1.2, 1.2, 1.2, 1.2]  (è¢«è£å‰ª)

    # Loss
    loss = -min(ratio * A, ratio_clipped * A).mean()
    loss.backward()
    optimizer.step()

# å…³é”®ï¼šåœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œold_log_prob å§‹ç»ˆä¸å˜ï¼
```

#### 3.6.7 ä¸‰ç§ç­–ç•¥æ¨¡å¼

verl æ”¯æŒä¸‰ç§ç­–ç•¥é…ç½®ï¼š

**æ¨¡å¼ 1ï¼šDecoupledï¼ˆ3 ç­–ç•¥ï¼‰- é»˜è®¤**
```
Ï€_rollout (vLLM BF16)  â†’ ç”Ÿæˆå“åº”
    â†“
Ï€_old (FSDP FP32)      â†’ é‡æ–°è®¡ç®—ï¼Œä½œä¸ºé”šç‚¹
    â†“
Ï€_Î¸ (FSDP FP32)        â†’ ä¼˜åŒ–ç›®æ ‡

ratio = Ï€_Î¸ / Ï€_old
```
âœ… ç²¾ç¡®çš„ ratio è®¡ç®—
âœ… è®­ç»ƒæœ€ç¨³å®š

**æ¨¡å¼ 2ï¼šBypassï¼ˆ2 ç­–ç•¥ï¼‰**
```python
# ray_trainer.py:1527-1535
if self.bypass_mode:
    batch.batch["old_log_probs"] = batch.batch["rollout_log_probs"]
```
```
Ï€_rollout (vLLM BF16)  â†’ ç”Ÿæˆ + ä½œä¸º old_log_prob
    â†“
Ï€_Î¸ (FSDP FP32)        â†’ ä¼˜åŒ–ç›®æ ‡

ratio = Ï€_Î¸ / Ï€_rollout
```
âœ… èŠ‚çœè®¡ç®—ï¼ˆä¸éœ€è¦é‡ç®— old_log_probï¼‰
âš ï¸ å¯èƒ½æœ‰åˆ†å¸ƒå·®å¼‚ï¼ˆBF16 vs FP32ï¼‰

**æ¨¡å¼ 3ï¼šRollout Correctionï¼ˆä¿®æ­£çš„ 2 ç­–ç•¥ï¼‰**
```python
# è®¡ç®—ä¿®æ­£æƒé‡
log_ratio = old_log_prob - rollout_log_prob
rollout_is_weights = torch.exp(log_ratio)

# åœ¨ loss ä¸­åº”ç”¨
policy_loss *= rollout_is_weights
```
âœ… ä¿®æ­£ vLLM å’Œ FSDP çš„åˆ†å¸ƒå·®å¼‚

#### 3.6.8 å¯¹æ¯”è¡¨

| ç»´åº¦ | old_log_prob (Ï€_old) | new_log_prob (Ï€_Î¸) |
|------|---------------------|-------------------|
| **è®¡ç®—æ—¶æœº** | æ¯ä¸ª batch ä¸€æ¬¡ | æ¯æ¬¡å‰å‘ä¼ æ’­ |
| **ç­–ç•¥ç‰ˆæœ¬** | è®­ç»ƒå‰çš„ Actor å¿«ç…§ | å½“å‰æ­£åœ¨ä¼˜åŒ–çš„ Actor |
| **åœ¨è®­ç»ƒä¸­** | å†»ç»“ä¸å˜ | ä¸æ–­æ›´æ–° |
| **ç”¨é€”** | è®¡ç®— importance ratio | ä¼˜åŒ–ç›®æ ‡ |
| **ä»£ç ä½ç½®** | `ray_trainer.py:1258` | `losses.py:97` |
| **ä¾èµ–** | å½“å‰ Actor weights | å½“å‰ Actor weights (evolving) |

**æ ¸å¿ƒè¦ç‚¹**ï¼š
- old_log_prob æ˜¯è®­ç»ƒå¼€å§‹æ—¶çš„å¿«ç…§ï¼Œè®­ç»ƒæœŸé—´ä¿æŒä¸å˜
- new_log_prob æ˜¯æ¯æ¬¡å‰å‘ä¼ æ’­çš„è¾“å‡ºï¼Œä¸æ–­æ›´æ–°
- ratio = Ï€_Î¸ / Ï€_old é™åˆ¶ç­–ç•¥å˜åŒ–åœ¨ Â±20%ï¼ˆclip_ratio=0.2ï¼‰
- è¿™å°±æ˜¯ PPO ç¨³å®šè®­ç»ƒçš„æ ¸å¿ƒæœºåˆ¶ ğŸ¯

---

## 4. å®Œæ•´è®­ç»ƒæµç¨‹

### 4.1 PPO è®­ç»ƒçš„ 7 ä¸ªé˜¶æ®µ

```python
# åœ¨ RayPPOTrainer._train_step ä¸­

# é˜¶æ®µ 1: Rollout - Actor ç”Ÿæˆå“åº”
rollout_output = self.actor_rollout_wg.generate_sequences(batch)

# é˜¶æ®µ 2: Reward - è®¡ç®—å¥–åŠ±
rollout_output = self._compute_reward(rollout_output)

# é˜¶æ®µ 3: Ref - è®¡ç®—å‚è€ƒæ¨¡å‹ log probï¼ˆç”¨äº KL penaltyï¼‰
rollout_output = self.actor_rollout_wg.compute_ref_log_prob(rollout_output)

# é˜¶æ®µ 4: Value - Critic é¢„æµ‹ä»·å€¼å‡½æ•°ï¼ˆPPO ç‹¬æœ‰ï¼‰
rollout_output = self.critic_wg.compute_values(rollout_output)

# é˜¶æ®µ 5: Advantage - è®¡ç®— GAE ä¼˜åŠ¿å€¼ï¼ˆPPO ç‹¬æœ‰ï¼‰
advantages, returns = compute_gae_advantage_return(
    token_level_rewards=rollout_output.batch['token_level_rewards'],
    values=rollout_output.batch['values'],  # Critic è¾“å‡º
    response_mask=rollout_output.batch['response_mask'],
    gamma=self.config.algorithm.gamma,
    lam=self.config.algorithm.lam,
)

# é˜¶æ®µ 6: Actor Update - ä½¿ç”¨ PPO Clip æ›´æ–°ç­–ç•¥
actor_metrics = self.actor_rollout_wg.update_actor(rollout_output)

# é˜¶æ®µ 7: Critic Update - ä½¿ç”¨ MSE loss æ›´æ–°ä»·å€¼å‡½æ•°ï¼ˆPPO ç‹¬æœ‰ï¼‰
critic_metrics = self.critic_wg.update_critic(rollout_output)
```

### 4.2 GSM8K è®­ç»ƒç¤ºä¾‹è¿½è¸ª

**Prompt:**
```
"Janet's ducks lay 16 eggs per day..."
```

**ç”Ÿæˆ 1 ä¸ªå“åº”ï¼ˆPPO é€šå¸¸ n=1ï¼‰ï¼š**
```python
response = "Let's solve step by step... #### 12"
```

**é˜¶æ®µ 2: è®¡ç®— Reward**
```python
reward = 1.0  # ç­”æ¡ˆæ­£ç¡®
token_level_rewards = [0, 0, ..., 0, 1.0]  # åªæœ‰æœ€åä¸€ä¸ª token
```

**é˜¶æ®µ 4: Critic é¢„æµ‹**
```python
values = critic_model(input_ids)
# values = [0.1, 0.2, 0.3, ..., 0.8]
```

**é˜¶æ®µ 5: GAE ä¼˜åŠ¿å€¼**
```python
# ä»åå‘å‰è®¡ç®—
delta_T = 1.0 + 0 - 0.8 = 0.2
A_T = 0.2

delta_{T-1} = 0 + 0.99 * 0.8 - 0.7 = 0.092
A_{T-1} = 0.092 + 0.99 * 0.95 * 0.2 = 0.280

# ...
advantages = [0.45, 0.42, 0.38, ..., 0.28, 0.2]
```

**é˜¶æ®µ 6: Actor æ›´æ–°**
```python
# è®¡ç®— policy loss
loss = compute_policy_loss_clip(
    old_log_prob=old_log_probs,
    log_prob=new_log_probs,
    advantages=advantages,
    cliprange=0.2,
)

# åå‘ä¼ æ’­æ›´æ–° Actor
loss.backward()
optimizer.step()
```

**é˜¶æ®µ 7: Critic æ›´æ–°**
```python
# MSE loss
returns = advantages + values
critic_loss = (critic_values - returns) ** 2

# åå‘ä¼ æ’­æ›´æ–° Critic
critic_loss.backward()
critic_optimizer.step()
```

---

## 5. é…ç½®å‚æ•°è¯¦è§£

### 5.1 æ ¸å¿ƒé…ç½®

```yaml
# ç®—æ³•é€‰æ‹©
algorithm:
  adv_estimator: gae  # ä½¿ç”¨ GAE ä¼˜åŠ¿ä¼°è®¡
  gamma: 0.99         # æŠ˜æ‰£å› å­
  lam: 0.95           # GAE lambda

# Critic æ¨¡å‹ï¼ˆPPO å¿…éœ€ï¼ï¼‰
critic:
  model:
    path: "Qwen/Qwen2.5-7B-Instruct"
    enable_gradient_checkpointing: true

  # Critic è®­ç»ƒ
  ppo_epochs: 2
  ppo_mini_batch_size: 64
  ppo_micro_batch_size: 2
  optim:
    lr: 5e-6

# Actor é…ç½®
actor_rollout_ref:
  rollout:
    n: 1  # PPO é€šå¸¸ n=1ï¼ˆæœ‰ Critic ä½œä¸º baselineï¼‰

  actor:
    ppo_epochs: 2
    ppo_mini_batch_size: 64
    clip_ratio: 0.2      # PPO clip range Îµ
    clip_ratio_c: 3.0    # Dual-clip ä¸‹ç•Œ
```

### 5.2 å‚æ•°è¯¦è§£

#### `gamma`ï¼ˆæŠ˜æ‰£å› å­ï¼‰

**ä½œç”¨ï¼š**æœªæ¥å¥–åŠ±çš„æƒé‡

**æ¨èå€¼ï¼š**
- `gamma=0.99`: é»˜è®¤å€¼ï¼Œé‡è§†é•¿æœŸå¥–åŠ±
- `gamma=0.95`: æ›´å…³æ³¨è¿‘æœŸå¥–åŠ±
- `gamma=1.0`: æ‰€æœ‰å¥–åŠ±ç­‰æƒï¼ˆä¸æ¨èï¼‰

**å½±å“ï¼š**
```
TD-error: Î´_t = r_t + Î³ * V_{t+1} - V_t
Î³ è¶Šå¤§ï¼Œè¶Šé‡è§†æœªæ¥ä»·å€¼
```

#### `lam`ï¼ˆGAE lambdaï¼‰

**ä½œç”¨ï¼š**å¹³è¡¡ bias å’Œ variance

**æ¨èå€¼ï¼š**
- `lam=0.95`: é»˜è®¤å€¼ï¼ˆæŠ˜ä¸­ï¼‰
- `lam=1.0`: Monte Carloï¼ˆæ— åï¼Œé«˜æ–¹å·®ï¼‰
- `lam=0.0`: 1-step TDï¼ˆä½æ–¹å·®ï¼Œæœ‰åï¼‰

**å…¬å¼ï¼š**
```
A_t = Î´_t + Î³Î» * Î´_{t+1} + (Î³Î»)Â² * Î´_{t+2} + ...
```

#### `clip_ratio`ï¼ˆPPO clip rangeï¼‰

**ä½œç”¨ï¼š**é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦

**æ¨èå€¼ï¼š**
- `clip_ratio=0.2`: é»˜è®¤å€¼
- `clip_ratio=0.1`: æ›´ä¿å®ˆï¼ˆæ›´ç¨³å®šï¼‰
- `clip_ratio=0.3`: æ›´æ¿€è¿›ï¼ˆå¯èƒ½ä¸ç¨³å®šï¼‰

**å«ä¹‰ï¼š**
```
ratio âˆˆ [1-Îµ, 1+Îµ] = [0.8, 1.2]
```

#### `ppo_epochs`

**ä½œç”¨ï¼š**æ¯ä¸ª batch é‡å¤è®­ç»ƒå¤šå°‘æ¬¡

**æ¨èå€¼ï¼š**
- `ppo_epochs=2`: å¹³è¡¡æ•ˆç‡å’Œç¨³å®šæ€§
- `ppo_epochs=1`: æ›´å¿«ï¼Œå¯èƒ½æ¬ æ‹Ÿåˆ
- `ppo_epochs=4`: æ›´å……åˆ†è®­ç»ƒï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ

### 5.3 å®Œæ•´è®­ç»ƒå‘½ä»¤

```bash
python3 -m verl.trainer.main_ppo \
    # æ•°æ®
    data.train_files=$HOME/data/gsm8k/train.parquet \
    data.val_files=$HOME/data/gsm8k/test.parquet \
    data.train_batch_size=256 \
    \
    # Actor æ¨¡å‹
    actor_rollout_ref.model.path=Qwen/Qwen2.5-7B-Instruct \
    actor_rollout_ref.actor.ppo_epochs=2 \
    actor_rollout_ref.actor.ppo_mini_batch_size=64 \
    actor_rollout_ref.actor.clip_ratio=0.2 \
    actor_rollout_ref.actor.optim.lr=1e-6 \
    \
    # Critic æ¨¡å‹ï¼ˆPPO å¿…éœ€ï¼‰
    critic.model.path=Qwen/Qwen2.5-7B-Instruct \
    critic.ppo_epochs=2 \
    critic.ppo_mini_batch_size=64 \
    critic.optim.lr=5e-6 \
    \
    # ç®—æ³•ï¼ˆPPO æ ¸å¿ƒï¼‰
    algorithm.adv_estimator=gae \
    algorithm.gamma=0.99 \
    algorithm.lam=0.95 \
    \
    # è®­ç»ƒ
    trainer.total_epochs=3 \
    trainer.n_gpus_per_node=8
```

---

## 6. KL æ•£åº¦æ§åˆ¶

### 6.1 ä¸¤ç§ KL æ§åˆ¶æ–¹å¼

#### æ–¹å¼ 1: KL Reward Penaltyï¼ˆPPO ä¼ ç»Ÿï¼‰

```yaml
algorithm:
  use_kl_in_reward: true
  kl_penalty: "kl"  # k1, abs, mse, low_var_kl
  kl_ctrl:
    type: "fixed"  # or "adaptive"
    kl_coef: 0.001
```

**å®ç°ï¼š**
```python
reward_with_kl = reward - kl_coef * kl_divergence(new_policy, ref_policy)
```

#### æ–¹å¼ 2: KL Lossï¼ˆGRPO æ¨èï¼ŒPPO ä¹Ÿå¯ç”¨ï¼‰

```yaml
actor_rollout_ref:
  actor:
    use_kl_loss: true
    kl_loss_coef: 0.001
    kl_loss_type: "k1"
```

**å®ç°ï¼š**
```python
total_loss = policy_loss + kl_loss_coef * kl_divergence
```

### 6.2 KL æ•£åº¦çš„ 4 ç§è®¡ç®—æ–¹å¼

```python
# k1 (æ ‡å‡† KL)
kl = old_log_prob - log_prob

# abs
kl = abs(old_log_prob - log_prob)

# mse (k2)
kl = 0.5 * (old_log_prob - log_prob) ** 2

# low_var_kl (k3)
ratio = exp(log_prob - old_log_prob)
kl = (ratio - 1) - log(ratio)
```

**å‚è€ƒï¼š** [KL Approximations Blog](http://joschu.net/blog/kl-approx.html)

### 6.3 Adaptive KL Controller

```yaml
algorithm:
  kl_ctrl:
    type: "adaptive"
    kl_coef: 0.001     # åˆå§‹ç³»æ•°
    target_kl: 0.01    # ç›®æ ‡ KL
    horizon: 10000     # è°ƒæ•´çª—å£
```

**è‡ªé€‚åº”è°ƒæ•´ï¼š**
```python
if current_kl > target_kl:
    kl_coef *= (1 + proportional_error)  # å¢å¤§æƒ©ç½š
else:
    kl_coef *= (1 - proportional_error)  # å‡å°æƒ©ç½š
```

### 6.4 æ·±åº¦è§£æï¼šref_log_probs è®¡ç®—ä¸ KL åŒé‡æƒ©ç½šæœºåˆ¶ â­

#### 6.4.1 ref_log_probs å¦‚ä½•è®¡ç®—

**ä½ç½®**ï¼š`verl/trainer/ppo/ray_trainer.py:1231-1256`

```python
def _compute_ref_log_prob(self, batch: DataProto) -> DataProto:
    """è®¡ç®—å‚è€ƒç­–ç•¥çš„ log æ¦‚ç‡"""

    batch_td = batch.to_tensordict()
    batch_td = left_right_2_no_padding(batch_td)

    tu.assign_non_tensor(
        batch_td,
        calculate_entropy=False,
        compute_loss=False,
    )

    if self.ref_in_actor:
        # æ–¹å¼ 1ï¼šä½¿ç”¨ LoRA è®­ç»ƒæ—¶
        # ç¦ç”¨ LoRA adapterï¼Œå›åˆ° base model
        output = self.actor_rollout_wg.compute_log_prob(
            batch_td,
            no_lora_adapter=True  # å…³é”®ï¼šä¸ç”¨ LoRA
        )
    else:
        # æ–¹å¼ 2ï¼šä½¿ç”¨å•ç‹¬çš„å‚è€ƒç­–ç•¥ Worker
        output = self.ref_policy_wg.compute_ref_log_prob(batch_td)

    ref_log_prob = tu.get(output, "log_probs")
    return ref_log_prob
```

**ä¸¤ç§å®ç°æ–¹å¼å¯¹æ¯”**ï¼š

| æ–¹å¼ | ref_in_actor=True | ref_in_actor=False |
|------|-------------------|-------------------|
| **æ¨¡å‹ç»“æ„** | Actor = Base + LoRA | Actor ç‹¬ç«‹ + RefPolicy ç‹¬ç«‹ |
| **è®¡ç®—æ–¹å¼** | ç¦ç”¨ LoRAï¼Œç”¨ Base | ç”¨å•ç‹¬çš„ RefPolicy Worker |
| **æ˜¾å­˜å ç”¨** | æ›´å°‘ï¼ˆå…±äº« baseï¼‰ | æ›´å¤šï¼ˆä¸¤ä¸ªå®Œæ•´æ¨¡å‹ï¼‰ |
| **çµæ´»æ€§** | LoRA è®­ç»ƒä¸“ç”¨ | å‚è€ƒç­–ç•¥å¯ä»¥æ˜¯ä»»æ„æ¨¡å‹ |
| **é€‚ç”¨åœºæ™¯** | å¾®è°ƒã€å¯¹é½ | å®Œå…¨ç‹¬ç«‹çš„å‚è€ƒç­–ç•¥ |

#### 6.4.2 KL åŒé‡æƒ©ç½šï¼šuse_kl_in_reward vs use_kl_loss

verl å®ç°äº† **ä¸¤ç§å®Œå…¨ç‹¬ç«‹** çš„ KL åº”ç”¨æ–¹å¼ï¼Œå¯ä»¥å•ç‹¬æˆ–ç»„åˆä½¿ç”¨ï¼š

##### **æœºåˆ¶ Aï¼šKL in Rewardï¼ˆReward é˜¶æ®µï¼‰**

**ä½ç½®**ï¼š`verl/trainer/ppo/ray_trainer.py:127-166`

```python
def apply_kl_penalty(data, kl_ctrl, kl_penalty="kl"):
    """åœ¨ reward ä¸­å‡å» KL æ•£åº¦"""

    # 1. è®¡ç®— KL æ•£åº¦
    kld = core_algos.kl_penalty(
        logprob=data.batch["old_log_probs"],      # Ï€_old
        ref_logprob=data.batch["ref_log_prob"],   # Ï€_ref
        kl_penalty=kl_penalty
    )

    # 2. ä» reward ä¸­å‡å» KL
    beta = kl_ctrl.value  # è‡ªé€‚åº”ç³»æ•°
    token_level_rewards = token_level_scores - beta * kld

    # 3. æ›´æ–°è‡ªé€‚åº”æ§åˆ¶å™¨
    current_kl = masked_mean(kld, mask=response_mask)
    kl_ctrl.update(current_kl=current_kl, n_steps=batch_size)

    return token_level_rewards
```

**é…ç½®**ï¼š
```yaml
algorithm:
  use_kl_in_reward: true
  kl_penalty: "low_var_kl"  # k3 ä¼°è®¡å™¨
  kl_ctrl:
    type: "adaptive"
    kl_coef: 0.1
    target_kl: 0.1
```

##### **æœºåˆ¶ Bï¼šKL in Lossï¼ˆPolicy æ›´æ–°é˜¶æ®µï¼‰**

**ä½ç½®**ï¼š`verl/workers/utils/losses.py:96-174`

```python
def ppo_loss(config, model_output, data):
    """PPO Loss è®¡ç®—"""

    # 1. æ ‡å‡† PPO Loss
    log_prob = model_output["log_probs"]
    old_log_prob = data["old_log_probs"]

    ratio = torch.exp(log_prob - old_log_prob)
    surr1 = ratio * advantages
    surr2 = torch.clamp(ratio, 1-Îµ, 1+Îµ) * advantages
    policy_loss = -torch.min(surr1, surr2).mean()

    total_loss = policy_loss

    # 2. å¦‚æœå¯ç”¨ KL Loss
    if config.use_kl_loss:
        ref_log_prob = data["ref_log_prob"]

        # è®¡ç®— KL æ•£åº¦
        kld = kl_penalty(
            logprob=log_prob,              # Ï€_Î¸
            ref_logprob=ref_log_prob,      # Ï€_ref
            kl_penalty=config.kl_loss_type
        )

        kl_loss = agg_loss(kld, loss_mask=response_mask)
        total_loss += config.kl_loss_coef * kl_loss

    return total_loss
```

**é…ç½®**ï¼š
```yaml
actor_rollout_ref:
  actor:
    use_kl_loss: true
    kl_loss_coef: 0.001
    kl_loss_type: "low_var_kl"
```

##### **ä¸¤è€…å¯¹æ¯”è¡¨**

| ç»´åº¦ | KL in Reward | KL in Loss |
|------|-------------|-----------|
| **æ—¶æœº** | Reward è®¡ç®—åï¼ŒAdvantage å‰ | Actor æ›´æ–°æ—¶ï¼ŒLoss è®¡ç®—ä¸­ |
| **å½±å“** | Token-level rewards | Policy gradient |
| **å…¬å¼** | `r' = r - Î² * KL(Ï€_old â€– Ï€_ref)` | `L = L_ppo + Î» * KL(Ï€_Î¸ â€– Ï€_ref)` |
| **ç­–ç•¥** | Ï€_old, Ï€_ref | Ï€_Î¸, Ï€_ref |
| **ç³»æ•°** | Î² (è‡ªé€‚åº”ï¼ŒAdaptiveKLController) | Î» (å›ºå®š) |
| **é…ç½®** | `algorithm.use_kl_in_reward` | `actor.use_kl_loss` |
| **é»˜è®¤** | False | False |
| **ä¼°è®¡å™¨** | `"kl"` (k1) | `"low_var_kl"` (k3) |

##### **å®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆåŒ KLï¼‰**

```python
# Step 1: ç”Ÿæˆå“åº”
responses = actor_rollout_wg.generate_sequences(prompts)

# Step 2: è®¡ç®—åŸå§‹ Reward
raw_rewards = reward_model(responses)

# Step 3: è®¡ç®— ref_log_prob
ref_log_prob = _compute_ref_log_prob(batch)  # log Ï€_ref

# Step 4: è®¡ç®— old_log_prob
old_log_prob = actor_rollout_wg.compute_log_prob(batch)  # log Ï€_old

# ========== KL in Reward ==========
if use_kl_in_reward:
    kld = old_log_prob - ref_log_prob
    token_level_rewards = raw_rewards - beta * kld
    # beta è‡ªé€‚åº”è°ƒæ•´
else:
    token_level_rewards = raw_rewards

# Step 5: è®¡ç®— Advantage (åŸºäºè°ƒæ•´åçš„ rewards)
advantages = compute_gae(token_level_rewards, values)

# Step 6: Actor æ›´æ–°
for epoch in range(ppo_epochs):
    new_log_prob = actor_model(responses)  # log Ï€_Î¸

    # PPO Loss
    ratio = torch.exp(new_log_prob - old_log_prob)
    ppo_loss = -torch.min(
        ratio * advantages,
        torch.clamp(ratio, 0.8, 1.2) * advantages
    ).mean()

    total_loss = ppo_loss

    # ========== KL in Loss ==========
    if use_kl_loss:
        kld = new_log_prob - ref_log_prob
        kl_loss = kld.mean()
        total_loss += lambda_kl * kl_loss

    total_loss.backward()
```

##### **ä½¿ç”¨åœºæ™¯å»ºè®®**

**åœºæ™¯ 1ï¼šRLHF å¯¹é½ï¼ˆåªç”¨ KL in Rewardï¼‰**
```yaml
algorithm:
  use_kl_in_reward: true
  kl_ctrl:
    type: "adaptive"
    kl_coef: 0.1

actor:
  use_kl_loss: false
```
âœ… é€‚åˆï¼šå¯¹é½ä»»åŠ¡ï¼Œé˜²æ­¢åç¦» base model
âœ… ä¼˜ç‚¹ï¼šè‡ªé€‚åº”è°ƒæ•´ï¼Œè®­ç»ƒç¨³å®š

**åœºæ™¯ 2ï¼šå¼ºæ­£åˆ™åŒ–ï¼ˆåªç”¨ KL in Lossï¼‰**
```yaml
algorithm:
  use_kl_in_reward: false

actor:
  use_kl_loss: true
  kl_loss_coef: 0.001
```
âœ… é€‚åˆï¼šéœ€è¦ç›´æ¥çº¦æŸ policy
âœ… ä¼˜ç‚¹ï¼šç†è®ºæ¸…æ™°ï¼Œæ¢¯åº¦ç›´æ¥

**åœºæ™¯ 3ï¼šåŒé‡çº¦æŸï¼ˆä¸¤è€…éƒ½ç”¨ï¼‰**
```yaml
algorithm:
  use_kl_in_reward: true
  kl_ctrl:
    type: "fixed"
    kl_coef: 0.05

actor:
  use_kl_loss: true
  kl_loss_coef: 0.001
```
âœ… é€‚åˆï¼šå®‰å…¨æ€§è¦æ±‚æé«˜çš„åœºæ™¯
âš ï¸ æ³¨æ„ï¼šå¯èƒ½è¿‡åº¦ä¿å®ˆï¼Œpolicy æ›´æ–°æ…¢

---

## 7. è°ƒè¯•æŠ€å·§

### 7.1 æ·»åŠ  GAE è®¡ç®—æ—¥å¿—

```python
# åœ¨ verl/trainer/ppo/core_algos.py:250 æ·»åŠ 

for t in reversed(range(gen_len)):
    delta = token_level_rewards[:, t] + gamma * nextvalues - values[:, t]

    # æ·»åŠ è°ƒè¯•
    if t == gen_len - 1:  # æœ€åä¸€ä¸ª token
        print(f"[GAE Debug] t={t}")
        print(f"  reward: {token_level_rewards[:, t][:3]}")
        print(f"  value: {values[:, t][:3]}")
        print(f"  delta: {delta[:3]}")
```

### 7.2 æ£€æŸ¥ Critic é¢„æµ‹è´¨é‡

```python
# åœ¨ RayPPOTrainer._train_step é˜¶æ®µ 5 åæ·»åŠ 

print(f"\n[Critic Debug]")
print(f"  Values mean: {values.mean():.4f}, std: {values.std():.4f}")
print(f"  Rewards mean: {token_level_rewards.sum(-1).mean():.4f}")
print(f"  Critic MSE: {((values - returns) ** 2).mean():.4f}")
```

### 7.3 ç›‘æ§ Clipping æ¯”ä¾‹

```python
# åœ¨ compute_policy_loss_clip ä¸­å·²æœ‰
pg_clipfrac = verl_F.masked_mean(torch.gt(pg_losses2, pg_losses1).float(), response_mask)

# åœ¨ TensorBoard è®°å½•
metrics = {
    'ppo/clipfrac': pg_clipfrac,  # è¢« clip çš„æ¯”ä¾‹
    'ppo/kl': ppo_kl,              # KL æ•£åº¦
}
```

**ç†æƒ³å€¼ï¼š**
- `clipfrac < 0.2`: ç­–ç•¥æ›´æ–°åˆç†
- `clipfrac > 0.5`: ç­–ç•¥æ›´æ–°è¿‡æ¿€ï¼Œè€ƒè™‘é™ä½å­¦ä¹ ç‡

### 7.4 TensorBoard ç›‘æ§

å…³é”®æŒ‡æ ‡ï¼š
```python
metrics = {
    # å¥–åŠ±
    'reward/mean': rewards.mean(),

    # Advantage
    'advantage/mean': advantages.mean(),
    'advantage/std': advantages.std(),

    # Critic
    'critic/value_mean': values.mean(),
    'critic/loss': critic_loss,

    # Actor
    'actor/loss': policy_loss,
    'actor/clipfrac': clipfrac,

    # KL
    'kl/mean': kl_divergence.mean(),
}
```

---

## 8. å¸¸è§é—®é¢˜

### Q1: PPO éœ€è¦å¤šå°‘ GPU æ˜¾å­˜ï¼Ÿ

**è®¡ç®—å…¬å¼ï¼š**
```
æ€»æ˜¾å­˜ = Actor æ˜¾å­˜ + Critic æ˜¾å­˜ + Rollout æ˜¾å­˜
```

**ç¤ºä¾‹ï¼ˆQwen2.5-7Bï¼‰ï¼š**
- Actor: ~30GBï¼ˆFSDP è®­ç»ƒï¼‰
- Critic: ~30GBï¼ˆFSDP è®­ç»ƒï¼‰
- Rollout: ~20GBï¼ˆvLLM æ¨ç†ï¼‰
- **æ€»è®¡ï¼š~80GB**ï¼ˆéœ€è¦ 2 å¼  A100 80GBï¼‰

**ä¼˜åŒ–æ–¹æ³•ï¼š**
- ä½¿ç”¨ Gradient Checkpointing
- å‡å° micro batch size
- ä½¿ç”¨ mixed precision (FP16/BF16)

### Q2: Critic æŸå¤±ä¸ä¸‹é™æ€ä¹ˆåŠï¼Ÿ

**å¯èƒ½åŸå›  1ï¼š** å­¦ä¹ ç‡å¤ªå°
```yaml
critic.optim.lr: 1e-5  # ä» 5e-6 å¢å¤§
```

**å¯èƒ½åŸå›  2ï¼š** Reward åˆ†å¸ƒå˜åŒ–å¤§
```yaml
# å¢åŠ  Critic è®­ç»ƒ epochs
critic.ppo_epochs: 4
```

**å¯èƒ½åŸå›  3ï¼š** å€¼å‡½æ•°åˆå§‹åŒ–ä¸å¥½
```yaml
# ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹åˆå§‹åŒ–
critic.model.path: "path/to/pretrained/critic"
```

### Q3: PPO vs GRPO å¦‚ä½•é€‰æ‹©ï¼Ÿ

**é€‰æ‹© PPOï¼š**
- âœ… é•¿åºåˆ—ç”Ÿæˆï¼ˆå¦‚é•¿æ–‡æœ¬ã€å¯¹è¯ï¼‰
- âœ… éœ€è¦ç»†ç²’åº¦ä»·å€¼ä¼°è®¡
- âœ… è¿½æ±‚è®­ç»ƒç¨³å®šæ€§
- âœ… GPU èµ„æºå……è¶³

**é€‰æ‹© GRPOï¼š**
- âœ… ç»“æœå¯¼å‘ä»»åŠ¡ï¼ˆæ•°å­¦ã€ä»£ç ï¼‰
- âœ… å¿«é€Ÿå®éªŒ
- âœ… GPU èµ„æºæœ‰é™
- âœ… è®­ç»ƒé€Ÿåº¦ä¼˜å…ˆ

### Q4: GAE çš„ lambda æ€ä¹ˆè°ƒï¼Ÿ

**lambda å½±å“ï¼š**
- `Î»=0`: åªç”¨ 1-step TDï¼ˆä½æ–¹å·®ï¼Œé«˜åå·®ï¼‰
  - é€‚åˆï¼šCritic å¾ˆå‡†ç¡®
- `Î»=1`: ç”¨å®Œæ•´ MCï¼ˆé«˜æ–¹å·®ï¼Œæ— åï¼‰
  - é€‚åˆï¼šCritic ä¸å‡†ç¡®
- `Î»=0.95`: æŠ˜ä¸­ï¼ˆé»˜è®¤æ¨èï¼‰

**è°ƒå‚å»ºè®®ï¼š**
1. å…ˆç”¨ Î»=0.95
2. å¦‚æœè®­ç»ƒä¸ç¨³å®š â†’ é™ä½ Î»ï¼ˆå¦‚ 0.9ï¼‰
3. å¦‚æœ Critic loss å¾ˆä½ â†’ é™ä½ Î»ï¼ˆä¿¡ä»» Criticï¼‰
4. å¦‚æœ Critic loss å¾ˆé«˜ â†’ å¢å¤§ Î»ï¼ˆä¸ä¿¡ä»» Criticï¼‰

### Q5: Dual-clip PPO ä»€ä¹ˆæ—¶å€™ç”¨ï¼Ÿ

**æ ‡å‡†åœºæ™¯ï¼š**
- ä½¿ç”¨æ ‡å‡† PPOï¼ˆ`clip_ratio_c` å¾ˆå¤§ï¼Œå®é™…ä¸ç”Ÿæ•ˆï¼‰

**Dual-clip åœºæ™¯ï¼š**
- è´Ÿä¼˜åŠ¿æ—¶ï¼Œç­–ç•¥ä¸‹é™è¿‡å¿«
- è®¾ç½® `clip_ratio_c=3.0`ï¼ˆé»˜è®¤ï¼‰

**å…¬å¼ï¼š**
```
å½“ A < 0 æ—¶ï¼Œratio ä¸‹ç•Œ = -clip_ratio_c
é˜²æ­¢è¿‡åº¦æƒ©ç½šååŠ¨ä½œ
```

---

## ğŸ“š å‚è€ƒèµ„æº

### è®ºæ–‡
- [PPO è®ºæ–‡](https://arxiv.org/abs/1707.06347)
- [GAE è®ºæ–‡](https://arxiv.org/abs/1506.02438)
- [Dual-clip PPO](https://arxiv.org/pdf/1912.09729)

### ä»£ç ä½ç½®
- GAE å®ç°: `verl/trainer/ppo/core_algos.py:214-262`
- PPO Clip Loss: `verl/trainer/ppo/core_algos.py:1095-1156`
- Critic æ›´æ–°: `verl/workers/fsdp_workers.py` (CriticWorker)

### å®˜æ–¹æ–‡æ¡£
- [PPO æ–‡æ¡£](https://verl.readthedocs.io/en/latest/algo/ppo.html)
- [OpenAI Spinning Up - PPO](https://spinningup.openai.com/en/latest/algorithms/ppo.html)

### ç¤ºä¾‹è„šæœ¬
- `examples/ppo_trainer/run_gemma.sh`
- `examples/ppo_trainer/run_qwen2.5-0.5b.sh`

---

*æœ€åæ›´æ–°: 2026-01-26*
