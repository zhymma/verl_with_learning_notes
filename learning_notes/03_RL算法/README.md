# 03 - RL ç®—æ³•

> ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ·±å…¥ç†è§£ PPOã€GRPO ç­‰å¼ºåŒ–å­¦ä¹ ç®—æ³•

---

## ğŸ“š æœ¬ç« å†…å®¹

### ğŸ“– å­¦ä¹ ç¬”è®°

#### **03_RLç®—æ³•æ¦‚è§ˆ.md** - ç®—æ³•å¯¹æ¯”ä¸é€‰æ‹©æŒ‡å—
- verl æ”¯æŒçš„ RL ç®—æ³•æ€»è§ˆ
- GRPO vs PPO vs RLOO å¯¹æ¯”
- ç®—æ³•é€‰æ‹©å»ºè®®
- é…ç½®åˆ‡æ¢æ–¹æ³•

#### **GRPO_è¯¦è§£.md** - GRPO ç®—æ³•æ·±åº¦è§£æï¼ˆæ–°ï¼ï¼‰
- GRPO æ ¸å¿ƒæ€æƒ³ï¼ˆGroup Relative ä¼˜åŠ¿ä¼°è®¡ï¼‰
- æ— éœ€ Critic æ¨¡å‹çš„ä¼˜åŠ¿
- `compute_grpo_outcome_advantage` æºç åˆ†æ
  - åˆ†ç»„å’Œå‡å€¼è®¡ç®—
  - æ ‡å‡†å·®å½’ä¸€åŒ–
  - ä¼˜åŠ¿å€¼å¹¿æ’­åˆ° token ç»´åº¦
- DrGRPO å˜ä½“è¯¦è§£
- å®Œæ•´è®­ç»ƒæµç¨‹ç¤ºä¾‹
- é…ç½®å‚æ•°è¯¦è§£

#### **PPO_è¯¦è§£.md** - PPO ç®—æ³•æ·±åº¦è§£æï¼ˆæ–°ï¼ï¼‰
- PPO æ ¸å¿ƒæ€æƒ³ï¼ˆClipped Surrogate Objectiveï¼‰
- Actor-Critic æ¶æ„
- GAEï¼ˆGeneralized Advantage Estimationï¼‰æºç åˆ†æ
  - TD-error è®¡ç®—
  - ä¼˜åŠ¿å€¼é€’æ¨
  - Baseline å‡å»å€¼å‡½æ•°
- `compute_policy_loss` æºç åˆ†æ
  - Ratio è®¡ç®—
  - Clipping æœºåˆ¶
  - Dual-clip PPO
- KL æ•£åº¦æ§åˆ¶ï¼ˆKL reward vs KL lossï¼‰
- å®Œæ•´è®­ç»ƒæµç¨‹ç¤ºä¾‹
- é…ç½®å‚æ•°è¯¦è§£

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ­¥éª¤ 1ï¼šç†è§£ç®—æ³•åŒºåˆ«

```bash
# é˜…è¯»ç®—æ³•æ¦‚è§ˆ
cat 03_RLç®—æ³•æ¦‚è§ˆ.md
```

å…³é”®åŒºåˆ«ï¼š
- **GRPO**: æ—  Criticï¼ŒåŸºäºç»„ç›¸å¯¹å¥–åŠ±ï¼Œæ›´å¿«
- **PPO**: æœ‰ Criticï¼ŒGAE ä¼˜åŠ¿ä¼°è®¡ï¼Œæ›´ç¨³å®š

### æ­¥éª¤ 2ï¼šæŸ¥çœ‹ GRPO æºç 

```bash
# æ ¸å¿ƒç®—æ³•å®ç°
cat verl/trainer/ppo/core_algos.py:266-330
```

### æ­¥éª¤ 3ï¼šåˆ‡æ¢ç®—æ³•

```bash
# ä½¿ç”¨ GRPOï¼ˆæ¨èå…¥é—¨ï¼‰
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    actor_rollout_ref.rollout.n=4

# ä½¿ç”¨ PPOï¼ˆæ›´ç¨³å®šï¼‰
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=gae \
    critic.model.path=Qwen/Qwen2.5-7B-Instruct
```

---

## ğŸ“– æ¨èå­¦ä¹ è·¯å¾„

### ç¬¬ 1 å¤©ï¼šç®—æ³•æ¦‚è§ˆå’Œ GRPO

1. **é˜…è¯»** `03_RLç®—æ³•æ¦‚è§ˆ.md`ï¼ˆ1 å°æ—¶ï¼‰
   - ç†è§£ä¸åŒç®—æ³•çš„åº”ç”¨åœºæ™¯
   - æŒæ¡ç®—æ³•é€‰æ‹©æ ‡å‡†

2. **é˜…è¯»** `GRPO_è¯¦è§£.md`ï¼ˆ2-3 å°æ—¶ï¼‰
   - æ·±å…¥ç†è§£ GRPO åŸç†
   - é˜…è¯»æºç å®ç°
   - ç†è§£åˆ†ç»„æœºåˆ¶å’Œä¼˜åŠ¿è®¡ç®—

3. **å®è·µ** è¿è¡Œ GRPO è®­ç»ƒ
   ```bash
   # ä½¿ç”¨ GSM8K æ•°æ®
   python3 -m verl.trainer.main_ppo \
       data.train_files=$HOME/data/gsm8k/train.parquet \
       data.val_files=$HOME/data/gsm8k/test.parquet \
       data.train_batch_size=256 \
       actor_rollout_ref.model.path=Qwen/Qwen2.5-7B-Instruct \
       actor_rollout_ref.rollout.n=4 \
       algorithm.adv_estimator=grpo \
       actor_rollout_ref.actor.use_kl_loss=true
   ```

4. **è°ƒè¯•** æ·»åŠ ä¼˜åŠ¿è®¡ç®—æ—¥å¿—
   ```python
   # åœ¨ verl/trainer/ppo/core_algos.py:303 æ·»åŠ 
   print(f"[GRPO Debug] Scores: {scores}")
   print(f"  Group means: {list(id2mean.values())[:5]}")
   print(f"  Group stds: {list(id2std.values())[:5]}")
   print(f"  Normalized advantages: {scores[:5]}")
   ```

### ç¬¬ 2 å¤©ï¼šPPO ç®—æ³•

1. **é˜…è¯»** `PPO_è¯¦è§£.md`ï¼ˆ2-3 å°æ—¶ï¼‰
   - ç†è§£ Actor-Critic æ¶æ„
   - æŒæ¡ GAE ä¼˜åŠ¿ä¼°è®¡
   - ç†è§£ Clipping æœºåˆ¶

2. **å¯¹æ¯”** GAE vs GRPO æºç 
   ```bash
   # æŸ¥çœ‹ GAE å®ç°
   grep -A 30 "def compute_gae" verl/trainer/ppo/core_algos.py

   # æŸ¥çœ‹ GRPO å®ç°
   grep -A 30 "def compute_grpo" verl/trainer/ppo/core_algos.py
   ```

3. **å®è·µ** è¿è¡Œ PPO è®­ç»ƒ
   ```bash
   python3 -m verl.trainer.main_ppo \
       data.train_files=$HOME/data/gsm8k/train.parquet \
       algorithm.adv_estimator=gae \
       critic.model.path=Qwen/Qwen2.5-7B-Instruct \
       actor_rollout_ref.actor.ppo_epochs=2 \
       critic.ppo_epochs=2
   ```

### ç¬¬ 3 å¤©ï¼šç®—æ³•å¯¹æ¯”å®éªŒ

1. **å®éªŒ 1**ï¼šGRPO vs PPO åœ¨ GSM8K ä¸Šçš„æ•ˆæœ
   - è¿è¡Œç›¸åŒé…ç½®çš„ GRPO å’Œ PPO
   - å¯¹æ¯” reward/mean æ›²çº¿
   - å¯¹æ¯”è®­ç»ƒé€Ÿåº¦

2. **å®éªŒ 2**ï¼šä¸åŒ rollout.n çš„å½±å“
   ```bash
   # GRPO with n=2
   python3 -m verl.trainer.main_ppo \
       algorithm.adv_estimator=grpo \
       actor_rollout_ref.rollout.n=2

   # GRPO with n=4
   python3 -m verl.trainer.main_ppo \
       algorithm.adv_estimator=grpo \
       actor_rollout_ref.rollout.n=4
   ```

3. **å®éªŒ 3**ï¼šKL æ§åˆ¶ç­–ç•¥å¯¹æ¯”
   ```bash
   # KL loss (GRPO æ¨è)
   python3 -m verl.trainer.main_ppo \
       actor_rollout_ref.actor.use_kl_loss=true \
       actor_rollout_ref.actor.kl_loss_coef=0.001

   # KL reward penalty
   python3 -m verl.trainer.main_ppo \
       algorithm.use_kl_in_reward=true \
       algorithm.kl_ctrl.kl_coef=0.001
   ```

---

## ğŸ“‹ å­¦ä¹ æ£€æŸ¥æ¸…å•

### ç®—æ³•ç†è§£ âœ“
- [ ] ç†è§£ PPOã€GRPOã€RLOO çš„æ ¸å¿ƒåŒºåˆ«
- [ ] æŒæ¡ GRPO çš„åˆ†ç»„æœºåˆ¶
- [ ] æŒæ¡ PPO çš„ GAE ä¼˜åŠ¿ä¼°è®¡
- [ ] ç†è§£ Clipping åœ¨ PPO ä¸­çš„ä½œç”¨
- [ ] çŸ¥é“ä½•æ—¶é€‰æ‹© GRPO vs PPO

### æºç é˜…è¯» âœ“
- [ ] é˜…è¯» `compute_grpo_outcome_advantage` å®ç°
- [ ] é˜…è¯» `compute_gae` å®ç°
- [ ] ç†è§£ `compute_policy_loss` ä¸­çš„ clipping
- [ ] ç†è§£ KL æ•£åº¦çš„ä¸¤ç§æ§åˆ¶æ–¹å¼

### å®è·µèƒ½åŠ› âœ“
- [ ] è¿è¡Œè¿‡ GRPO è®­ç»ƒ
- [ ] è¿è¡Œè¿‡ PPO è®­ç»ƒ
- [ ] èƒ½å¤Ÿåˆ‡æ¢ä¸åŒçš„ä¼˜åŠ¿ä¼°è®¡å™¨
- [ ] èƒ½å¤Ÿè°ƒæ•´ rollout.n å’Œ batch size
- [ ] èƒ½å¤Ÿé…ç½® KL æ§åˆ¶ç­–ç•¥

---

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬ç« åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

âœ… æ·±å…¥ç†è§£ PPO å’Œ GRPO ç®—æ³•åŸç†
âœ… é˜…è¯»å’Œç†è§£ core_algos.py æºç 
âœ… æ ¹æ®ä»»åŠ¡ç‰¹ç‚¹é€‰æ‹©åˆé€‚çš„ç®—æ³•
âœ… ç†Ÿç»ƒé…ç½®ç®—æ³•å‚æ•°
âœ… è°ƒè¯•ä¼˜åŠ¿è®¡ç®—å’Œç­–ç•¥æ›´æ–°
âœ… è¿›è¡Œç®—æ³•å¯¹æ¯”å®éªŒ

---

## ğŸ’¡ é‡ç‚¹å†…å®¹

### GRPO ä¼˜åŠ¿è®¡ç®—å…¬å¼

å¯¹äºæ¯ä¸ªç»„ gï¼ˆåŒä¸€ä¸ª prompt ç”Ÿæˆçš„ n ä¸ªå“åº”ï¼‰ï¼š

```python
# 1. è®¡ç®—æ¯ä¸ªå“åº”çš„æ€»å¥–åŠ±
score_i = sum(token_rewards[i])

# 2. è®¡ç®—ç»„å†…å‡å€¼å’Œæ ‡å‡†å·®
mean_g = mean(score_1, score_2, ..., score_n)
std_g = std(score_1, score_2, ..., score_n)

# 3. å½’ä¸€åŒ–ä¼˜åŠ¿å€¼
advantage_i = (score_i - mean_g) / (std_g + epsilon)

# 4. å¹¿æ’­åˆ° token ç»´åº¦
advantages[i, :] = advantage_i * response_mask[i, :]
```

### GAE ä¼˜åŠ¿è®¡ç®—å…¬å¼

é€æ­¥ä»åå‘å‰è®¡ç®—ï¼š

```python
for t in reversed(range(T)):
    # TD-error
    delta_t = reward[t] + gamma * V[t+1] - V[t]

    # GAE é€’æ¨
    A[t] = delta_t + gamma * lambda * A[t+1]

# å½’ä¸€åŒ–
A = (A - mean(A)) / (std(A) + epsilon)
```

### PPO Clipped Objective

```python
# è®¡ç®—æ¦‚ç‡æ¯”
ratio = exp(new_log_prob - old_log_prob)

# Clipped surrogate
loss1 = ratio * advantage
loss2 = clip(ratio, 1-epsilon, 1+epsilon) * advantage
loss = -min(loss1, loss2)
```

### ç®—æ³•é€‰æ‹©å»ºè®®

| åœºæ™¯ | æ¨èç®—æ³• | åŸå›  |
|------|---------|------|
| **æ•°å­¦æ¨ç†ï¼ˆGSM8Kï¼‰** | GRPO | ç»“æœå¯¼å‘ï¼Œæ— éœ€è¿‡ç¨‹ç›‘ç£ |
| **ä»£ç ç”Ÿæˆ** | GRPO | å¯æ‰§è¡Œæ€§æ˜¯äºŒå…ƒç»“æœ |
| **é•¿æ–‡æœ¬ç”Ÿæˆ** | PPO | Critic æä¾›æ›´ç»†ç²’åº¦çš„ä»·å€¼ä¼°è®¡ |
| **å¯¹è¯è´¨é‡** | PPO | éœ€è¦ç»†è‡´çš„ä»·å€¼å‡½æ•° |
| **å¿«é€Ÿå®éªŒ** | GRPO | è®­ç»ƒé€Ÿåº¦å¿«ï¼Œæ— éœ€ Critic |
| **è¿½æ±‚ç¨³å®šæ€§** | PPO | GAE æ–¹å·®æ›´å° |

---

## â“ å¸¸è§é—®é¢˜

### Q1: GRPO å’Œ PPO å“ªä¸ªæ›´å¥½ï¼Ÿ

**å–å†³äºä»»åŠ¡**ï¼š
- **ç»“æœå¯¼å‘ä»»åŠ¡**ï¼ˆå¦‚æ•°å­¦é¢˜ã€ä»£ç ç”Ÿæˆï¼‰ï¼šGRPO æ›´ç®€å•é«˜æ•ˆ
- **è¿‡ç¨‹å¯¼å‘ä»»åŠ¡**ï¼ˆå¦‚é•¿æ–‡æœ¬ã€å¯¹è¯ï¼‰ï¼šPPO æ›´ç¨³å®š

**èµ„æºè€ƒè™‘**ï¼š
- GRPO ä¸éœ€è¦ Critic æ¨¡å‹ï¼ŒèŠ‚çœ GPU æ˜¾å­˜å’Œè®­ç»ƒæ—¶é—´
- PPO éœ€è¦åŒæ—¶è®­ç»ƒ Actor å’Œ Critic

### Q2: rollout.n è®¾ç½®å¤šå°‘åˆé€‚ï¼Ÿ

**GRPO æ¨è**ï¼šn â‰¥ 4
- å¤ªå°ï¼ˆn=1, 2ï¼‰ï¼šç»„å†…æ–¹å·®ä¸å‡†ç¡®
- å¤ªå¤§ï¼ˆn>8ï¼‰ï¼šè®¡ç®—å¼€é”€å¤§ï¼Œæ”¶ç›Šé€’å‡

**PPO**ï¼šn=1 å³å¯
- PPO ä½¿ç”¨ Critic æä¾› baselineï¼Œä¸éœ€è¦å¤šä¸ªæ ·æœ¬

### Q3: ä¸ºä»€ä¹ˆ GRPO è¦ç”¨ use_kl_loss=trueï¼Ÿ

GRPO ä¸åœ¨ reward ä¸­åŠ  KL penaltyï¼Œè€Œæ˜¯ï¼š
```python
loss = policy_loss + kl_loss_coef * kl_divergence
```

è¿™æ ·å¯ä»¥ï¼š
- ç›´æ¥åœ¨æ¢¯åº¦ä¸­æ§åˆ¶ KL
- é¿å… reward shaping çš„å½±å“

### Q4: GAE ä¸­çš„ lambda æ€ä¹ˆè®¾ç½®ï¼Ÿ

**lambda** æ§åˆ¶ bias-variance tradeoffï¼š
- `lambda=0`: åªç”¨ 1-step TDï¼ˆä½æ–¹å·®ï¼Œé«˜åå·®ï¼‰
- `lambda=1`: ç”¨å®Œæ•´ Monte Carloï¼ˆé«˜æ–¹å·®ï¼Œæ— åï¼‰
- `lambda=0.95`ï¼ˆé»˜è®¤ï¼‰ï¼šæŠ˜ä¸­é€‰æ‹©

### Q5: è®­ç»ƒä¸ç¨³å®šæ€ä¹ˆåŠï¼Ÿ

**GRPO ä¸ç¨³å®š**ï¼š
- å¢å¤§ `rollout.n`ï¼ˆæ›´å¤šæ ·æœ¬ï¼‰
- å‡å° `clip_ratio`ï¼ˆæ›´ä¿å®ˆçš„æ›´æ–°ï¼‰
- ä½¿ç”¨ DrGRPOï¼ˆ`loss_agg_mode="seq-mean-token-sum-norm"`ï¼‰

**PPO ä¸ç¨³å®š**ï¼š
- è°ƒæ•´ `gamma` å’Œ `lam`
- å¢åŠ  `ppo_epochs`
- ä½¿ç”¨ Dual-clip PPO

---

## ğŸ”— ç›¸å…³èµ„æº

### æœ¬åœ°æ–‡ä»¶
- ç®—æ³•æ¦‚è§ˆ: `03_RLç®—æ³•æ¦‚è§ˆ.md`
- GRPO è¯¦è§£: `GRPO_è¯¦è§£.md`
- PPO è¯¦è§£: `PPO_è¯¦è§£.md`
- é¡¹ç›®æ¦‚è§ˆ: `../../CLAUDE.md`
- å®Œæ•´å­¦ä¹ è·¯çº¿: `../../LEARNING_GUIDE.md`

### å®˜æ–¹æ–‡æ¡£
- [GRPO æ–‡æ¡£](https://verl.readthedocs.io/en/latest/algo/grpo.html)
- [PPO æ–‡æ¡£](https://verl.readthedocs.io/en/latest/algo/ppo.html)
- [Baseline Performance](https://verl.readthedocs.io/en/latest/algo/baseline.html)

### ä»£ç ä½ç½®
- æ ¸å¿ƒç®—æ³•: `verl/trainer/ppo/core_algos.py`
  - GRPO: ç¬¬ 266-330 è¡Œ
  - GAE: ç¬¬ 210-262 è¡Œ
  - Policy Loss: ç¬¬ 450-550 è¡Œ
- é…ç½®æ–‡ä»¶: `verl/trainer/config/ppo_trainer.yaml`
- ç®—æ³•æ³¨å†Œ: `verl/trainer/ppo/core_algos.py:112-150`

### è®ºæ–‡å’Œå‚è€ƒ
- [DeepSeekMath (GRPO)](https://arxiv.org/pdf/2402.03300)
- [PPO è®ºæ–‡](https://arxiv.org/abs/1707.06347)
- [OpenAI Spinning Up - PPO](https://spinningup.openai.com/en/latest/algorithms/ppo.html)
- [DrGRPO è®ºæ–‡](https://arxiv.org/pdf/2503.20783)

---

## â­ï¸ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬ç« åï¼Œç»§ç»­å­¦ä¹ ï¼š
- **04 - Reward è®¾è®¡**: æ›´å¤šè‡ªå®šä¹‰ Reward å®ç°å’Œè°ƒä¼˜æŠ€å·§
- **05 - Agent RL**: å·¥å…·è°ƒç”¨å’Œå¤šè½®å¯¹è¯çš„ RL è®­ç»ƒ

---

*åˆ›å»ºæ—¶é—´: 2026-01-26*
*é¢„è®¡å®Œæˆæ—¶é—´: 3-4 å¤©*
