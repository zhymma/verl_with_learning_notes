# RL ç®—æ³•æ¦‚è§ˆ

> verl æ”¯æŒçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•å¯¹æ¯”ä¸é€‰æ‹©æŒ‡å—

---

## ğŸ“– ç›®å½•

1. [æ”¯æŒçš„ç®—æ³•åˆ—è¡¨](#1-æ”¯æŒçš„ç®—æ³•åˆ—è¡¨)
2. [GRPO vs PPO è¯¦ç»†å¯¹æ¯”](#2-grpo-vs-ppo-è¯¦ç»†å¯¹æ¯”)
3. [å…¶ä»–ç®—æ³•ç®€ä»‹](#3-å…¶ä»–ç®—æ³•ç®€ä»‹)
4. [ç®—æ³•é€‰æ‹©å†³ç­–æ ‘](#4-ç®—æ³•é€‰æ‹©å†³ç­–æ ‘)
5. [é…ç½®åˆ‡æ¢æ–¹æ³•](#5-é…ç½®åˆ‡æ¢æ–¹æ³•)
6. [æ€§èƒ½å¯¹æ¯”](#6-æ€§èƒ½å¯¹æ¯”)

---

## 1. æ”¯æŒçš„ç®—æ³•åˆ—è¡¨

verl æ”¯æŒ **12+ ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•**ï¼Œé€šè¿‡ `algorithm.adv_estimator` é…ç½®ï¼š

### 1.1 ä¸»æµç®—æ³•ï¼ˆæ¨èï¼‰

| ç®—æ³• | å…¨ç§° | Critic | é€‚ç”¨åœºæ™¯ | éš¾åº¦ |
|------|------|--------|----------|------|
| **GRPO** | Group Relative Policy Optimization | âŒ | æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆ | â­â­ |
| **PPO** | Proximal Policy Optimization | âœ… | é•¿æ–‡æœ¬ã€å¯¹è¯è´¨é‡ | â­â­â­ |
| **RLOO** | REINFORCE Leave-One-Out | âŒ | å¿«é€Ÿå®éªŒ | â­â­ |

### 1.2 æ‰©å±•ç®—æ³•

| ç®—æ³• | Critic | ç‰¹ç‚¹ |
|------|--------|------|
| **REINFORCE++** | âŒ | åŸºç¡€ç­–ç•¥æ¢¯åº¦å¢å¼ºç‰ˆ |
| **ReMax** | âŒ | æœ€å¤§åŒ–å¥–åŠ±åŸºçº¿ |
| **OPO** | âŒ | åœ¨çº¿ç­–ç•¥ä¼˜åŒ– |
| **GPG** | âŒ | å¹¿ä¹‰ç­–ç•¥æ¢¯åº¦ |
| **DrGRPO** | âŒ | GRPO å˜ä½“ï¼Œå‡å°‘é•¿åº¦åå·® |
| **GRPO_PASSK** | âŒ | Pass@k ä¼˜åŒ–çš„ GRPO |
| **Dual-clip PPO** | âœ… | PPO å˜ä½“ï¼ŒåŒé‡ clipping |

### 1.3 é…ç½®ç¤ºä¾‹

```yaml
# GRPO
algorithm:
  adv_estimator: grpo

# PPO
algorithm:
  adv_estimator: gae

# RLOO
algorithm:
  adv_estimator: rloo

# REINFORCE++
algorithm:
  adv_estimator: reinforce_plus_plus

# ReMax
algorithm:
  adv_estimator: remax
```

---

## 2. GRPO vs PPO è¯¦ç»†å¯¹æ¯”

### 2.1 æ ¸å¿ƒåŒºåˆ«

#### GRPOï¼ˆæ—  Criticï¼‰

```
å¯¹äºæ¯ä¸ª promptï¼Œç”Ÿæˆ n ä¸ªå“åº”:

responses = [resp_1, resp_2, ..., resp_n]
rewards = [r_1, r_2, ..., r_n]

è®¡ç®—ç»„å†…å‡å€¼å’Œæ ‡å‡†å·®:
mean = Î£(r_i) / n
std = âˆš(Î£(r_i - mean)Â² / n)

å½’ä¸€åŒ–ä¼˜åŠ¿å€¼:
advantage_i = (r_i - mean) / std

ä½¿ç”¨ advantage_i æ›´æ–°ç­–ç•¥
```

#### PPOï¼ˆæœ‰ Criticï¼‰

```
å¯¹äºæ¯ä¸ª promptï¼Œç”Ÿæˆ 1 ä¸ªå“åº”:

Critic é¢„æµ‹ä»·å€¼å‡½æ•°:
V(s_t) = critic_model(state_t)

è®¡ç®— TD-error:
Î´_t = r_t + Î³*V(s_{t+1}) - V(s_t)

è®¡ç®— GAE ä¼˜åŠ¿å€¼:
A_t = Î´_t + Î³Î»*A_{t+1}

ä½¿ç”¨ A_t æ›´æ–° Actor
ä½¿ç”¨ MSE æ›´æ–° Critic
```

### 2.2 è®­ç»ƒé€Ÿåº¦å¯¹æ¯”

**å‡è®¾ï¼š** Qwen2.5-7Bï¼Œ256 promptsï¼Œ500 tokens/response

| é˜¶æ®µ | GRPO (n=4) | PPO (n=1) |
|------|------------|-----------|
| **Rollout** | ~30sï¼ˆ256*4=1024 å“åº”ï¼‰ | ~8sï¼ˆ256 å“åº”ï¼‰ |
| **Reward** | ~2s | ~0.5s |
| **Critic Forward** | 0sï¼ˆæ—  Criticï¼‰ | ~3s |
| **Advantage** | ~0.1s | ~0.2sï¼ˆGAE æ›´å¤æ‚ï¼‰ |
| **Actor Update** | ~10sï¼ˆ1024 å“åº”ï¼‰ | ~3sï¼ˆ256 å“åº”ï¼‰ |
| **Critic Update** | 0sï¼ˆæ—  Criticï¼‰ | ~3s |
| **æ€»è®¡** | **~42s** | **~18s** |

**ç»“è®ºï¼š**
- PPO æ›´å¿«ï¼ˆæ¯ stepï¼‰
- GRPO éœ€è¦æ›´å¤š Rollout æ—¶é—´
- ä½† GRPO çœå» Critic è®­ç»ƒï¼Œæ€»ä½“å·®è·ä¸å¤§

### 2.3 GPU æ˜¾å­˜å¯¹æ¯”

**Qwen2.5-7B ç¤ºä¾‹ï¼š**

| ç»„ä»¶ | GRPO (n=4) | PPO (n=1) |
|------|------------|-----------|
| **Actor è®­ç»ƒ** | 30GB | 30GB |
| **Critic è®­ç»ƒ** | 0GB | 30GB âœ— |
| **Rollout æ¨ç†** | 20GB | 20GB |
| **æ€»è®¡** | **~50GB** | **~80GB** |

**ç»“è®ºï¼š**
- GRPO çœå» Critic æ˜¾å­˜ï¼ˆ~30GBï¼‰
- 1 å¼  A100 80GB å¯ä»¥è·‘ GRPO
- PPO é€šå¸¸éœ€è¦ 2 å¼  A100

### 2.4 æ”¶æ•›æ€§å¯¹æ¯”

| æŒ‡æ ‡ | GRPO | PPO |
|------|------|-----|
| **ç¨³å®šæ€§** | ä¸­ç­‰ï¼ˆä¾èµ– rollout.nï¼‰ | é«˜ï¼ˆGAE æ–¹å·®å°ï¼‰ |
| **æ”¶æ•›é€Ÿåº¦** | å¿«ï¼ˆå¤§ batch æ ·æœ¬ï¼‰ | ä¸­ç­‰ |
| **æœ€ç»ˆæ€§èƒ½** | ç›¸å½“ | ç›¸å½“ |
| **è¶…å‚æ•æ„Ÿåº¦** | å¯¹ rollout.n æ•æ„Ÿ | å¯¹ gamma/lam æ•æ„Ÿ |

**GSM8K è®­ç»ƒå¯¹æ¯”ï¼š**
```
GRPO (n=4):
  Epoch 1: 45% â†’ Epoch 2: 52% â†’ Epoch 3: 56.7%

PPO (n=1):
  Epoch 1: 42% â†’ Epoch 2: 50% â†’ Epoch 3: 56.7%
```

---

## 3. å…¶ä»–ç®—æ³•ç®€ä»‹

### 3.1 RLOO (REINFORCE Leave-One-Out)

**ç‰¹ç‚¹ï¼š**
- ç±»ä¼¼ GRPOï¼Œä½†ç”¨ "leave-one-out" æ–¹å¼è®¡ç®— baseline
- ä¸éœ€è¦ Critic
- æ¯” GRPO æ›´ç¨³å®šï¼Œä½†è®¡ç®—ç•¥æ…¢

**Baseline è®¡ç®—ï¼š**
```python
# å¯¹äºå“åº” iï¼Œbaseline æ˜¯å…¶ä»– n-1 ä¸ªå“åº”çš„å‡å€¼
baseline_i = (Î£(r_j) - r_i) / (n - 1)  # j â‰  i
advantage_i = r_i - baseline_i
```

**é…ç½®ï¼š**
```yaml
algorithm:
  adv_estimator: rloo

actor_rollout_ref:
  rollout:
    n: 4  # è‡³å°‘ 2
```

### 3.2 REINFORCE++

**ç‰¹ç‚¹ï¼š**
- ç»å…¸ REINFORCE çš„å¢å¼ºç‰ˆ
- ä½¿ç”¨ discounted returns è€Œéå•æ­¥ reward
- ä¸éœ€è¦ Critic

**Returns è®¡ç®—ï¼š**
```python
R_t = r_t + Î³*r_{t+1} + Î³Â²*r_{t+2} + ...
advantage_t = (R_t - mean(R)) / std(R)
```

**é…ç½®ï¼š**
```yaml
algorithm:
  adv_estimator: reinforce_plus_plus
  gamma: 0.99
```

### 3.3 ReMax

**ç‰¹ç‚¹ï¼š**
- ä½¿ç”¨å¤–éƒ¨ baseline æ¨¡å‹
- é€‚åˆæœ‰é¢„è®­ç»ƒ reward model çš„åœºæ™¯

**é…ç½®ï¼š**
```yaml
algorithm:
  adv_estimator: remax

# éœ€è¦æä¾› reward baseline
```

### 3.4 DrGRPO

**ç‰¹ç‚¹ï¼š**
- GRPO å˜ä½“ï¼Œè§£å†³é•¿åº¦åå·®é—®é¢˜
- ä¸é™¤ä»¥æ ‡å‡†å·®ï¼Œä½¿ç”¨å…¨å±€å½’ä¸€åŒ–

**é…ç½®ï¼š**
```yaml
algorithm:
  adv_estimator: grpo
  norm_adv_by_std_in_grpo: false  # å…³é”®ï¼

actor_rollout_ref:
  actor:
    loss_agg_mode: "seq-mean-token-sum-norm"
    loss_scale_factor: 512
```

**è¯¦è§ï¼š** `GRPO_è¯¦è§£.md` ç¬¬ 5 èŠ‚

---

## 4. ç®—æ³•é€‰æ‹©å†³ç­–æ ‘

```
ä½ çš„ä»»åŠ¡æ˜¯ä»€ä¹ˆï¼Ÿ
â”‚
â”œâ”€ ç»“æœå¯¼å‘ä»»åŠ¡ï¼ˆæ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆï¼‰
â”‚   â”‚
â”‚   â”œâ”€ GPU èµ„æºæœ‰é™ï¼ˆ< 2 å¼  A100ï¼‰
â”‚   â”‚   â””â”€ æ¨èï¼šGRPOï¼ˆrollout.n=4ï¼‰
â”‚   â”‚
â”‚   â””â”€ è¿½æ±‚æœ€ç¨³å®š
â”‚       â””â”€ æ¨èï¼šPPO æˆ– RLOO
â”‚
â”œâ”€ è¿‡ç¨‹å¯¼å‘ä»»åŠ¡ï¼ˆé•¿æ–‡æœ¬ã€å¯¹è¯è´¨é‡ï¼‰
â”‚   â”‚
â”‚   â”œâ”€ GPU èµ„æºå……è¶³ï¼ˆ>= 2 å¼  A100ï¼‰
â”‚   â”‚   â””â”€ æ¨èï¼šPPOï¼ˆéœ€è¦ Criticï¼‰
â”‚   â”‚
â”‚   â””â”€ GPU èµ„æºæœ‰é™
â”‚       â””â”€ æ¨èï¼šGRPOï¼ˆrollout.n=8ï¼Œæ›´å¤šæ ·æœ¬ï¼‰
â”‚
â””â”€ å¿«é€Ÿå®éªŒ/éªŒè¯æƒ³æ³•
    â”‚
    â”œâ”€ æœ€ç®€å•ä¸Šæ‰‹
    â”‚   â””â”€ æ¨èï¼šGRPOï¼ˆrollout.n=2ï¼‰
    â”‚
    â””â”€ æ›´ç¨³å®š
        â””â”€ æ¨èï¼šRLOOï¼ˆrollout.n=4ï¼‰
```

### 4.1 ä»»åŠ¡ç±»å‹åˆ¤æ–­

**ç»“æœå¯¼å‘ä»»åŠ¡ï¼š**
- æ•°å­¦æ¨ç†ï¼ˆGSM8K, MATHï¼‰
- ä»£ç ç”Ÿæˆï¼ˆHumanEval, MBPPï¼‰
- é€‰æ‹©é¢˜ï¼ˆMMLUï¼‰
- æœ‰æ˜ç¡®æ­£ç¡®ç­”æ¡ˆ

**è¿‡ç¨‹å¯¼å‘ä»»åŠ¡ï¼š**
- é•¿æ–‡æœ¬ç”Ÿæˆï¼ˆæ‘˜è¦ã€ç¿»è¯‘ï¼‰
- å¯¹è¯è´¨é‡ä¼˜åŒ–ï¼ˆå‹å¥½åº¦ã€è¿è´¯æ€§ï¼‰
- åˆ›æ„å†™ä½œ
- å¤šè½®å¯¹è¯

### 4.2 GPU èµ„æºè€ƒè™‘

**å•å¡åœºæ™¯ï¼ˆ1 å¼  A100 80GBï¼‰ï¼š**
- âœ… GRPOï¼ˆQwen2.5-7Bï¼‰
- âœ… RLOOï¼ˆQwen2.5-7Bï¼‰
- âŒ PPOï¼ˆQwen2.5-7Bï¼‰- æ˜¾å­˜ä¸å¤Ÿ

**å¤šå¡åœºæ™¯ï¼ˆ2+ å¼  A100ï¼‰ï¼š**
- âœ… GRPO
- âœ… PPO
- âœ… RLOO
- âœ… ä»»æ„ç®—æ³•

---

## 5. é…ç½®åˆ‡æ¢æ–¹æ³•

### 5.1 ä» GRPO åˆ‡æ¢åˆ° PPO

```bash
# GRPO é…ç½®
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    actor_rollout_ref.rollout.n=4 \
    actor_rollout_ref.actor.use_kl_loss=true

# â†“ åˆ‡æ¢åˆ° PPO â†“

# PPO é…ç½®
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=gae \              # æ”¹ä¸º gae
    algorithm.gamma=0.99 \                     # æ·»åŠ  gamma
    algorithm.lam=0.95 \                       # æ·»åŠ  lam
    actor_rollout_ref.rollout.n=1 \            # æ”¹ä¸º 1
    critic.model.path=Qwen/Qwen2.5-7B-Instruct \  # æ·»åŠ  Critic
    critic.ppo_epochs=2 \                      # Critic epochs
    critic.ppo_mini_batch_size=64              # Critic batch size
```

### 5.2 ä» PPO åˆ‡æ¢åˆ° RLOO

```bash
# PPO é…ç½®
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=gae \
    critic.model.path=Qwen/Qwen2.5-7B-Instruct

# â†“ åˆ‡æ¢åˆ° RLOO â†“

# RLOO é…ç½®
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=rloo \             # æ”¹ä¸º rloo
    actor_rollout_ref.rollout.n=4              # æ”¹ä¸º >= 2

# ç§»é™¤ critic ç›¸å…³é…ç½®ï¼ˆRLOO ä¸éœ€è¦ï¼‰
```

### 5.3 é…ç½®æ–‡ä»¶åˆ‡æ¢

åˆ›å»ºä¸åŒç®—æ³•çš„é…ç½®æ–‡ä»¶ï¼š

**grpo_config.yaml:**
```yaml
defaults:
  - ppo_trainer
  - _self_

algorithm:
  adv_estimator: grpo

actor_rollout_ref:
  rollout:
    n: 4
  actor:
    use_kl_loss: true
```

**ppo_config.yaml:**
```yaml
defaults:
  - ppo_trainer
  - _self_

algorithm:
  adv_estimator: gae
  gamma: 0.99
  lam: 0.95

critic:
  model:
    path: ${actor_rollout_ref.model.path}
```

**ä½¿ç”¨ï¼š**
```bash
# ä½¿ç”¨ GRPO
python3 -m verl.trainer.main_ppo --config-name grpo_config

# ä½¿ç”¨ PPO
python3 -m verl.trainer.main_ppo --config-name ppo_config
```

---

## 6. æ€§èƒ½å¯¹æ¯”

### 6.1 GSM8K åŸºå‡†æµ‹è¯•

| æ¨¡å‹ | ç®—æ³• | Pretrain | è®­ç»ƒå | æå‡ |
|------|------|----------|--------|------|
| **Qwen2.5-0.5B** | GRPO | 36.4% | 55.2% | +18.8% |
| **Qwen2.5-0.5B** | PPO | 36.4% | 56.7% | +20.3% |
| **Qwen2.5-0.5B** | RLOO | 36.4% | 54.8% | +18.4% |
| **Qwen2.5-7B** | GRPO | 82.1% | 89.3% | +7.2% |
| **Qwen2.5-7B** | PPO | 82.1% | 89.5% | +7.4% |

**ç»“è®ºï¼š**
- PPO ç•¥ä¼˜äº GRPOï¼ˆ~1-2%ï¼‰
- RLOO æ€§èƒ½ä¸ GRPO ç›¸å½“
- å¤§æ¨¡å‹æå‡å¹…åº¦å°äºå°æ¨¡å‹

### 6.2 è®­ç»ƒæ•ˆç‡å¯¹æ¯”

**Qwen2.5-7B on 8Ã—A100 80GB:**

| ç®—æ³• | æ¯ epoch æ—¶é—´ | GPU æ˜¾å­˜ | æ€»æˆæœ¬ |
|------|--------------|----------|--------|
| **GRPO** | ~30 min | ~50GB | ä½ |
| **PPO** | ~25 min | ~80GB | ä¸­ |
| **RLOO** | ~32 min | ~50GB | ä½ |

### 6.3 è¶…å‚æ•æ„Ÿåº¦

| ç®—æ³• | æ•æ„Ÿè¶…å‚ | æ¨èå€¼ | è°ƒå‚éš¾åº¦ |
|------|---------|--------|----------|
| **GRPO** | rollout.n | 4-8 | â­â­ |
| **PPO** | gamma, lam, lr | 0.99, 0.95, 1e-6 | â­â­â­ |
| **RLOO** | rollout.n | 4-8 | â­â­ |

---

## ğŸ“š æ¨èå­¦ä¹ é¡ºåº

### 1. å…¥é—¨ï¼šGRPOï¼ˆç¬¬ 1 å¤©ï¼‰
- é˜…è¯» `GRPO_è¯¦è§£.md`
- è¿è¡Œ GSM8K GRPO è®­ç»ƒ
- ç†è§£åˆ†ç»„æœºåˆ¶å’Œä¼˜åŠ¿è®¡ç®—

### 2. è¿›é˜¶ï¼šPPOï¼ˆç¬¬ 2 å¤©ï¼‰
- é˜…è¯» `PPO_è¯¦è§£.md`
- ç†è§£ GAE å’Œ Clipping
- è¿è¡Œ PPO è®­ç»ƒå¹¶å¯¹æ¯”

### 3. å®éªŒï¼šç®—æ³•å¯¹æ¯”ï¼ˆç¬¬ 3 å¤©ï¼‰
- åœ¨åŒä¸€ä»»åŠ¡ä¸Šæµ‹è¯• GRPOã€PPOã€RLOO
- å¯¹æ¯”è®­ç»ƒæ›²çº¿å’Œæœ€ç»ˆæ€§èƒ½
- è®°å½•ç»éªŒå’Œæœ€ä½³å®è·µ

---

## ğŸ”— ç›¸å…³èµ„æº

### æœ¬åœ°æ–‡ä»¶
- GRPO è¯¦è§£: `GRPO_è¯¦è§£.md`
- PPO è¯¦è§£: `PPO_è¯¦è§£.md`
- é¡¹ç›®æ¦‚è§ˆ: `../../CLAUDE.md`

### å®˜æ–¹æ–‡æ¡£
- [Algorithm Comparison](https://verl.readthedocs.io/en/latest/algo/baseline.html)
- [GRPO](https://verl.readthedocs.io/en/latest/algo/grpo.html)
- [PPO](https://verl.readthedocs.io/en/latest/algo/ppo.html)

### ä»£ç ä½ç½®
- ç®—æ³•æ³¨å†Œ: `verl/trainer/ppo/core_algos.py:112-150`
- GRPO: `verl/trainer/ppo/core_algos.py:266-330`
- PPO GAE: `verl/trainer/ppo/core_algos.py:214-262`
- RLOO: `verl/trainer/ppo/core_algos.py:430-520`

---

*æœ€åæ›´æ–°: 2026-01-26*
