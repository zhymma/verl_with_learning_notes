# GRPO ç®—æ³•è¯¦è§£

> Group Relative Policy Optimization - æ— éœ€ Critic çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•

---

## ğŸ“– ç›®å½•

1. [GRPO æ ¸å¿ƒæ€æƒ³](#1-grpo-æ ¸å¿ƒæ€æƒ³)
2. [æºç æ·±åº¦è§£æ](#2-æºç æ·±åº¦è§£æ)
3. [å®Œæ•´è®­ç»ƒæµç¨‹](#3-å®Œæ•´è®­ç»ƒæµç¨‹)
4. [é…ç½®å‚æ•°è¯¦è§£](#4-é…ç½®å‚æ•°è¯¦è§£)
5. [DrGRPO å˜ä½“](#5-drgrpo-å˜ä½“)
6. [è°ƒè¯•æŠ€å·§](#6-è°ƒè¯•æŠ€å·§)
7. [å¸¸è§é—®é¢˜](#7-å¸¸è§é—®é¢˜)

---

## 1. GRPO æ ¸å¿ƒæ€æƒ³

### 1.1 ä»€ä¹ˆæ˜¯ GRPOï¼Ÿ

**GRPO (Group Relative Policy Optimization)** æ˜¯ä¸€ç§ç®€åŒ–çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç”± DeepSeekMath è®ºæ–‡æå‡ºã€‚

**æ ¸å¿ƒç‰¹ç‚¹ï¼š**
- âœ… **æ— éœ€ Critic æ¨¡å‹**ï¼šä¸éœ€è¦è®­ç»ƒä»·å€¼å‡½æ•°ç½‘ç»œ
- âœ… **åŸºäºç»„ç›¸å¯¹å¥–åŠ±**ï¼šä½¿ç”¨åŒç»„æ ·æœ¬çš„å‡å€¼ä½œä¸º baseline
- âœ… **è®­ç»ƒé€Ÿåº¦å¿«**ï¼šçœå» Critic è®­ç»ƒæ—¶é—´å’Œæ˜¾å­˜
- âœ… **é€‚åˆç»“æœå¯¼å‘ä»»åŠ¡**ï¼šæ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆç­‰

### 1.2 GRPO vs PPO

| ç‰¹æ€§ | GRPO | PPO |
|------|------|-----|
| **Critic æ¨¡å‹** | âŒ ä¸éœ€è¦ | âœ… éœ€è¦ |
| **Baseline** | ç»„å†…æ ·æœ¬å‡å€¼ | Critic çš„ä»·å€¼å‡½æ•° |
| **ä¼˜åŠ¿ä¼°è®¡** | ç›¸å¯¹äºç»„å‡å€¼ | GAEï¼ˆæ—¶åºå·®åˆ†ï¼‰ |
| **GPU æ˜¾å­˜** | æ›´å°‘ï¼ˆåªè®­ç»ƒ Actorï¼‰ | æ›´å¤šï¼ˆActor + Criticï¼‰ |
| **è®­ç»ƒé€Ÿåº¦** | æ›´å¿« | è¾ƒæ…¢ |
| **é€‚ç”¨åœºæ™¯** | ç»“æœå¯¼å‘ä»»åŠ¡ | è¿‡ç¨‹å¯¼å‘ä»»åŠ¡ |

### 1.3 GRPO å·¥ä½œæµç¨‹

```
1. å¯¹äºæ¯ä¸ª promptï¼Œé‡‡æ · n ä¸ªå“åº”ï¼ˆå½¢æˆä¸€ä¸ª"ç»„"ï¼‰
   â†“
2. è®¡ç®—æ¯ä¸ªå“åº”çš„å¥–åŠ±ï¼ˆé€šè¿‡ RewardManagerï¼‰
   â†“
3. åœ¨ç»„å†…è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
   â†“
4. å½’ä¸€åŒ–ä¼˜åŠ¿å€¼ï¼š(reward - mean) / std
   â†“
5. ä½¿ç”¨å½’ä¸€åŒ–çš„ä¼˜åŠ¿å€¼æ›´æ–°ç­–ç•¥
```

**å…³é”®é…ç½®ï¼š**
- `actor_rollout_ref.rollout.n >= 2`ï¼ˆæ¯ä¸ª prompt é‡‡æ ·å¤šä¸ªå“åº”ï¼‰
- `algorithm.adv_estimator=grpo`
- `actor_rollout_ref.actor.use_kl_loss=true`ï¼ˆä½¿ç”¨ KL loss è€Œé KL rewardï¼‰

---

## 2. æºç æ·±åº¦è§£æ

### 2.1 å‡½æ•°ç­¾å

```python
# ä½ç½®: verl/trainer/ppo/core_algos.py:266-330

@register_adv_est(AdvantageEstimator.GRPO)
def compute_grpo_outcome_advantage(
    token_level_rewards: torch.Tensor,  # (bs, response_length)
    response_mask: torch.Tensor,        # (bs, response_length)
    index: np.ndarray,                  # (bs,) - åˆ†ç»„ç´¢å¼•
    epsilon: float = 1e-6,              # æ•°å€¼ç¨³å®šæ€§
    norm_adv_by_std_in_grpo: bool = True,  # æ˜¯å¦é™¤ä»¥æ ‡å‡†å·®
    config: Optional[AlgoConfig] = None,
) -> tuple[torch.Tensor, torch.Tensor]:
    """
    è®¡ç®— GRPO çš„ä¼˜åŠ¿å€¼ï¼ˆä»…ç”¨äºç»“æœå¥–åŠ±ï¼‰
    """
```

**å‚æ•°è¯´æ˜ï¼š**
- `token_level_rewards`: æ¯ä¸ª token çš„å¥–åŠ±ï¼ˆé€šå¸¸åªæœ‰æœ€åä¸€ä¸ª token æœ‰å¥–åŠ±ï¼‰
- `response_mask`: æ ‡è®°å“ªäº› token æ˜¯å“åº”éƒ¨åˆ†ï¼ˆ1ï¼‰è¿˜æ˜¯ prompt éƒ¨åˆ†ï¼ˆ0ï¼‰
- `index`: åˆ†ç»„ç´¢å¼•ï¼Œç›¸åŒç´¢å¼•çš„å“åº”å±äºåŒä¸€ä¸ª prompt
- `norm_adv_by_std_in_grpo`: True=åŸå§‹ GRPOï¼ŒFalse=DrGRPO

### 2.2 ç¬¬ 1 æ­¥ï¼šè®¡ç®—æ€»å¥–åŠ±

```python
# ä»£ç ä½ç½®: verl/trainer/ppo/core_algos.py:303
scores = token_level_rewards.sum(dim=-1)
```

**ä½œç”¨ï¼š**
å°† token çº§åˆ«çš„å¥–åŠ±æ±‚å’Œï¼Œå¾—åˆ°æ¯ä¸ªå“åº”çš„æ€»åˆ†ã€‚

**ç¤ºä¾‹ï¼š**
```python
# å‡è®¾æœ‰ 2 ä¸ª promptï¼Œæ¯ä¸ªé‡‡æ · 2 ä¸ªå“åº”
token_level_rewards = torch.tensor([
    [0, 0, 0, 1.0],  # prompt 0, response 0 â†’ score=1.0
    [0, 0, 0, 0.0],  # prompt 0, response 1 â†’ score=0.0
    [0, 0, 0, 0.5],  # prompt 1, response 0 â†’ score=0.5
    [0, 0, 0, 1.0],  # prompt 1, response 1 â†’ score=1.0
])

scores = torch.tensor([1.0, 0.0, 0.5, 1.0])
```

### 2.3 ç¬¬ 2 æ­¥ï¼šæŒ‰ç»„åˆ†ç»„

```python
# ä»£ç ä½ç½®: verl/trainer/ppo/core_algos.py:305-312
id2score = defaultdict(list)
id2mean = {}
id2std = {}

with torch.no_grad():
    bsz = scores.shape[0]
    for i in range(bsz):
        id2score[index[i]].append(scores[i])
```

**ä½œç”¨ï¼š**
å°†å±äºåŒä¸€ä¸ª prompt çš„å¤šä¸ªå“åº”åˆ†åˆ°åŒä¸€ç»„ã€‚

**ç¤ºä¾‹ï¼š**
```python
index = np.array([0, 0, 1, 1])  # å‰ 2 ä¸ªå±äº prompt 0ï¼Œå 2 ä¸ªå±äº prompt 1

# åˆ†ç»„å:
id2score = {
    0: [1.0, 0.0],  # prompt 0 çš„ 2 ä¸ªå“åº”
    1: [0.5, 1.0],  # prompt 1 çš„ 2 ä¸ªå“åº”
}
```

### 2.4 ç¬¬ 3 æ­¥ï¼šè®¡ç®—ç»„å†…ç»Ÿè®¡é‡

```python
# ä»£ç ä½ç½®: verl/trainer/ppo/core_algos.py:313-322
for idx in id2score:
    if len(id2score[idx]) == 1:
        # åªæœ‰ 1 ä¸ªæ ·æœ¬ï¼šå‡å€¼=0ï¼Œæ ‡å‡†å·®=1ï¼ˆä¿æŒåŸå€¼ï¼‰
        id2mean[idx] = torch.tensor(0.0)
        id2std[idx] = torch.tensor(1.0)
    elif len(id2score[idx]) > 1:
        # å¤šä¸ªæ ·æœ¬ï¼šè®¡ç®—çœŸå®çš„å‡å€¼å’Œæ ‡å‡†å·®
        scores_tensor = torch.stack(id2score[idx])
        id2mean[idx] = torch.mean(scores_tensor)
        id2std[idx] = torch.std(scores_tensor)
```

**ä½œç”¨ï¼š**
è®¡ç®—æ¯ä¸ªç»„çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼Œç”¨ä½œ baselineã€‚

**ç¤ºä¾‹ï¼š**
```python
# ç»§ç»­ä¸Šé¢çš„ä¾‹å­
id2mean = {
    0: (1.0 + 0.0) / 2 = 0.5,
    1: (0.5 + 1.0) / 2 = 0.75,
}

id2std = {
    0: std([1.0, 0.0]) = 0.5,
    1: std([0.5, 1.0]) = 0.25,
}
```

### 2.5 ç¬¬ 4 æ­¥ï¼šå½’ä¸€åŒ–ä¼˜åŠ¿å€¼

```python
# ä»£ç ä½ç½®: verl/trainer/ppo/core_algos.py:323-328
for i in range(bsz):
    if norm_adv_by_std_in_grpo:
        # åŸå§‹ GRPOï¼šé™¤ä»¥æ ‡å‡†å·®
        scores[i] = (scores[i] - id2mean[index[i]]) / (id2std[index[i]] + epsilon)
    else:
        # DrGRPOï¼šä¸é™¤ä»¥æ ‡å‡†å·®
        scores[i] = scores[i] - id2mean[index[i]]
```

**ä½œç”¨ï¼š**
å°†æ¯ä¸ªå“åº”çš„å¥–åŠ±å½’ä¸€åŒ–ä¸ºç›¸å¯¹äºç»„å†…å‡å€¼çš„ä¼˜åŠ¿å€¼ã€‚

**ç¤ºä¾‹ï¼ˆnorm_adv_by_std_in_grpo=Trueï¼‰ï¼š**
```python
# prompt 0, response 0
advantage[0] = (1.0 - 0.5) / (0.5 + 1e-6) â‰ˆ 1.0

# prompt 0, response 1
advantage[1] = (0.0 - 0.5) / (0.5 + 1e-6) â‰ˆ -1.0

# prompt 1, response 0
advantage[2] = (0.5 - 0.75) / (0.25 + 1e-6) â‰ˆ -1.0

# prompt 1, response 1
advantage[3] = (1.0 - 0.75) / (0.25 + 1e-6) â‰ˆ 1.0
```

**è§£é‡Šï¼š**
- é«˜äºç»„å‡å€¼çš„å“åº” â†’ æ­£ä¼˜åŠ¿å€¼ â†’ å¢å¼ºæ¦‚ç‡
- ä½äºç»„å‡å€¼çš„å“åº” â†’ è´Ÿä¼˜åŠ¿å€¼ â†’ é™ä½æ¦‚ç‡

### 2.6 ç¬¬ 5 æ­¥ï¼šå¹¿æ’­åˆ° token ç»´åº¦

```python
# ä»£ç ä½ç½®: verl/trainer/ppo/core_algos.py:328
scores = scores.unsqueeze(-1) * response_mask
```

**ä½œç”¨ï¼š**
å°†æ ‡é‡ä¼˜åŠ¿å€¼æ‰©å±•åˆ°æ¯ä¸ª tokenï¼Œå¹¶åªåœ¨å“åº”éƒ¨åˆ†ç”Ÿæ•ˆã€‚

**ç¤ºä¾‹ï¼š**
```python
# å‡è®¾ response_mask:
response_mask = torch.tensor([
    [0, 0, 1, 1],  # å‰ 2 ä¸ªæ˜¯ promptï¼Œå 2 ä¸ªæ˜¯ response
    [0, 0, 1, 1],
    [0, 0, 1, 1],
    [0, 0, 1, 1],
])

# å¹¿æ’­å:
advantages = torch.tensor([
    [0,  0,  1.0,  1.0],   # prompt 0, response 0
    [0,  0, -1.0, -1.0],   # prompt 0, response 1
    [0,  0, -1.0, -1.0],   # prompt 1, response 0
    [0,  0,  1.0,  1.0],   # prompt 1, response 1
])
```

### 2.7 å®Œæ•´ä»£ç æµç¨‹å›¾

```
è¾“å…¥:
  token_level_rewards: (4, 4) = [[0,0,0,1], [0,0,0,0], [0,0,0,0.5], [0,0,0,1]]
  response_mask: (4, 4)
  index: [0, 0, 1, 1]

ç¬¬ 1 æ­¥: æ±‚å’Œ
  scores: (4,) = [1.0, 0.0, 0.5, 1.0]

ç¬¬ 2 æ­¥: åˆ†ç»„
  id2score[0] = [1.0, 0.0]
  id2score[1] = [0.5, 1.0]

ç¬¬ 3 æ­¥: ç»Ÿè®¡
  id2mean[0] = 0.5,  id2std[0] = 0.5
  id2mean[1] = 0.75, id2std[1] = 0.25

ç¬¬ 4 æ­¥: å½’ä¸€åŒ–
  advantages: (4,) = [1.0, -1.0, -1.0, 1.0]

ç¬¬ 5 æ­¥: å¹¿æ’­
  advantages: (4, 4) = [[0,0,1,1], [0,0,-1,-1], [0,0,-1,-1], [0,0,1,1]]

è¾“å‡º:
  advantages: (4, 4)
  returns: (4, 4)  # GRPO ä¸­ returns == advantages
```

---

## 3. å®Œæ•´è®­ç»ƒæµç¨‹

### 3.1 ä»æ•°æ®åˆ°ä¼˜åŠ¿å€¼

```python
# 1. åŠ è½½æ•°æ®ï¼ˆåœ¨ RayPPOTrainer.fit ä¸­ï¼‰
batch = {
    'prompts': [...],  # 256 ä¸ª prompts
    'reward_model': [...],
}

# 2. Rollout ç”Ÿæˆï¼ˆåœ¨ _train_step ç¬¬ 1 é˜¶æ®µï¼‰
rollout_output = self.actor_rollout_wg.generate_sequences(batch)
# rollout_output.batch ç°åœ¨æœ‰:
#   'input_ids': (1024, seq_len)  # 256 * 4 = 1024 ä¸ªå“åº”
#   'responses': (1024, response_len)
#   'response_mask': (1024, response_len)

# 3. è®¡ç®— Rewardï¼ˆåœ¨ _train_step ç¬¬ 2 é˜¶æ®µï¼‰
rollout_output = self._compute_reward(rollout_output)
# rollout_output.batch ç°åœ¨æœ‰:
#   'token_level_rewards': (1024, response_len)
#   'rewards': (1024,)  # æ€»å¥–åŠ±

# 4. è®¡ç®—ä¼˜åŠ¿å€¼ï¼ˆåœ¨ _train_step ç¬¬ 5 é˜¶æ®µï¼‰
index = np.repeat(np.arange(256), 4)  # [0,0,0,0, 1,1,1,1, ..., 255,255,255,255]

advantages, returns = compute_grpo_outcome_advantage(
    token_level_rewards=rollout_output.batch['token_level_rewards'],
    response_mask=rollout_output.batch['response_mask'],
    index=index,
    norm_adv_by_std_in_grpo=True,
)

rollout_output.batch['advantages'] = advantages
```

### 3.2 GSM8K è®­ç»ƒç¤ºä¾‹è¿½è¸ª

**Prompt:**
```
"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning..."
```

**ç”Ÿæˆ 4 ä¸ªå“åº”ï¼š**
```python
responses = [
    "Let's solve step by step... #### 12",    # æ­£ç¡®ç­”æ¡ˆ
    "First, calculate... #### 15",           # é”™è¯¯ç­”æ¡ˆ
    "We need to find... #### 12",            # æ­£ç¡®ç­”æ¡ˆ
    "The answer is... #### 10",              # é”™è¯¯ç­”æ¡ˆ
]
```

**è®¡ç®— Rewardï¼ˆä½¿ç”¨ GSM8K rewardï¼‰ï¼š**
```python
# ground_truth = "12"
rewards = [1.0, 0.0, 1.0, 0.0]  # åªæœ‰ç¬¬ 0 å’Œç¬¬ 2 ä¸ªæ­£ç¡®
```

**è®¡ç®— GRPO ä¼˜åŠ¿å€¼ï¼š**
```python
# ç»„å†…ç»Ÿè®¡
mean = (1.0 + 0.0 + 1.0 + 0.0) / 4 = 0.5
std = std([1.0, 0.0, 1.0, 0.0]) = 0.5

# å½’ä¸€åŒ–ä¼˜åŠ¿
advantages = [
    (1.0 - 0.5) / 0.5 = 1.0,   # æ­£ç¡®å“åº” â†’ æ­£ä¼˜åŠ¿
    (0.0 - 0.5) / 0.5 = -1.0,  # é”™è¯¯å“åº” â†’ è´Ÿä¼˜åŠ¿
    (1.0 - 0.5) / 0.5 = 1.0,   # æ­£ç¡®å“åº” â†’ æ­£ä¼˜åŠ¿
    (0.0 - 0.5) / 0.5 = -1.0,  # é”™è¯¯å“åº” â†’ è´Ÿä¼˜åŠ¿
]
```

**ç­–ç•¥æ›´æ–°ï¼š**
- å¢å¼ºæ­£ç¡®å“åº”çš„ç”Ÿæˆæ¦‚ç‡
- é™ä½é”™è¯¯å“åº”çš„ç”Ÿæˆæ¦‚ç‡

---

## 4. é…ç½®å‚æ•°è¯¦è§£

### 4.1 æ ¸å¿ƒé…ç½®

```yaml
# ç®—æ³•é€‰æ‹©
algorithm:
  adv_estimator: grpo  # ä½¿ç”¨ GRPO ä¼˜åŠ¿ä¼°è®¡å™¨

# Rollout é…ç½®
actor_rollout_ref:
  rollout:
    n: 4  # æ¯ä¸ª prompt ç”Ÿæˆ 4 ä¸ªå“åº”ï¼ˆå¿…é¡» >= 2ï¼‰

  actor:
    # KL æ§åˆ¶ï¼ˆGRPO æ¨èä½¿ç”¨ KL lossï¼‰
    use_kl_loss: true
    kl_loss_coef: 0.001
    kl_loss_type: "k1"

    # PPO æ›´æ–°
    ppo_epochs: 2
    ppo_mini_batch_size: 64
    clip_ratio: 0.2

    # Loss èšåˆ
    loss_agg_mode: "token-mean"  # "token-mean" | "seq-mean-token-mean"

# æ•°æ®é…ç½®
data:
  train_batch_size: 256  # 256 ä¸ª prompts â†’ 256*4=1024 ä¸ªå“åº”
```

### 4.2 å‚æ•°è¯¦è§£

#### `rollout.n`ï¼ˆé‡è¦ï¼ï¼‰

**ä½œç”¨ï¼š**æ¯ä¸ª prompt é‡‡æ ·å¤šå°‘ä¸ªå“åº”

**æ¨èå€¼ï¼š**
- `n=4`: å¹³è¡¡è®¡ç®—æ•ˆç‡å’Œç»„å†…æ–¹å·®ä¼°è®¡ï¼ˆé»˜è®¤ï¼‰
- `n=2`: æœ€å°å€¼ï¼Œæ–¹å·®ä¼°è®¡ä¸å¤Ÿå‡†ç¡®
- `n=8`: æ›´å‡†ç¡®ï¼Œä½†è®¡ç®—å¼€é”€å¤§

**å½±å“ï¼š**
```
æ€»å“åº”æ•° = train_batch_size * n
æ˜¾å­˜å ç”¨ âˆ n
è®­ç»ƒæ—¶é—´ âˆ nï¼ˆRollout é˜¶æ®µï¼‰
```

#### `norm_adv_by_std_in_grpo`

**ä½œç”¨ï¼š**æ˜¯å¦é™¤ä»¥æ ‡å‡†å·®

**Trueï¼ˆé»˜è®¤ï¼‰ï¼š** åŸå§‹ GRPO
```python
advantage = (reward - mean) / (std + epsilon)
```

**Falseï¼š** DrGRPO å˜ä½“
```python
advantage = reward - mean
```

#### `use_kl_loss`

**GRPO æ¨è `true`**ï¼Œç›´æ¥åœ¨ loss ä¸­åŠ  KLï¼š
```python
total_loss = policy_loss + kl_loss_coef * kl_divergence
```

**vs PPO çš„ KL reward penaltyï¼š**
```python
reward = original_reward - kl_coef * kl_divergence
```

#### `loss_agg_mode`

**token-meanï¼ˆé»˜è®¤ï¼‰ï¼š**
```python
loss = mean(losses * response_mask)
```

**seq-mean-token-meanï¼š**
```python
loss = mean([mean(losses[i]) for i in range(bs)])
```

**seq-mean-token-sum-normï¼ˆDrGRPOï¼‰ï¼š**
```python
loss = mean([sum(losses[i]) / norm_factor for i in range(bs)])
```

### 4.3 å®Œæ•´è®­ç»ƒå‘½ä»¤

```bash
python3 -m verl.trainer.main_ppo \
    # æ•°æ®
    data.train_files=$HOME/data/gsm8k/train.parquet \
    data.val_files=$HOME/data/gsm8k/test.parquet \
    data.train_batch_size=256 \
    data.max_prompt_length=1024 \
    data.max_response_length=512 \
    \
    # æ¨¡å‹
    actor_rollout_ref.model.path=Qwen/Qwen2.5-7B-Instruct \
    actor_rollout_ref.model.enable_gradient_checkpointing=true \
    \
    # Rolloutï¼ˆå…³é”®ï¼ï¼‰
    actor_rollout_ref.rollout.n=4 \
    actor_rollout_ref.rollout.temperature=1.0 \
    actor_rollout_ref.rollout.top_p=1.0 \
    actor_rollout_ref.rollout.tensor_model_parallel_size=2 \
    \
    # ç®—æ³•ï¼ˆGRPO æ ¸å¿ƒï¼‰
    algorithm.adv_estimator=grpo \
    algorithm.norm_adv_by_std_in_grpo=true \
    \
    # Actor è®­ç»ƒ
    actor_rollout_ref.actor.ppo_epochs=2 \
    actor_rollout_ref.actor.ppo_mini_batch_size=64 \
    actor_rollout_ref.actor.ppo_micro_batch_size=2 \
    actor_rollout_ref.actor.clip_ratio=0.2 \
    actor_rollout_ref.actor.optim.lr=1e-6 \
    actor_rollout_ref.actor.use_kl_loss=true \
    actor_rollout_ref.actor.kl_loss_coef=0.001 \
    actor_rollout_ref.actor.loss_agg_mode=token-mean \
    \
    # è®­ç»ƒ
    trainer.total_epochs=3 \
    trainer.logger=tensorboard \
    trainer.n_gpus_per_node=8
```

---

## 5. DrGRPO å˜ä½“

### 5.1 DrGRPO æ˜¯ä»€ä¹ˆï¼Ÿ

**è®ºæ–‡ï¼š** [Understanding R1-Zero-Like Training](https://arxiv.org/pdf/2503.20783)

**æ ¸å¿ƒå‘ç°ï¼š**
GRPO åŸå§‹å®ç°æœ‰"é•¿åº¦åå·®"ï¼š
- é”™è¯¯ç­”æ¡ˆå¾€å¾€æ›´é•¿ï¼ˆæ¨¡å‹"èƒ¡ç¼–ä¹±é€ "ï¼‰
- é™¤ä»¥æ ‡å‡†å·®ä¼šæ”¾å¤§è¿™ç§åå·®

**DrGRPO æ”¹è¿›ï¼š**
1. ä¸é™¤ä»¥æ ‡å‡†å·®ï¼ˆ`norm_adv_by_std_in_grpo=false`ï¼‰
2. ä½¿ç”¨å…¨å±€å½’ä¸€åŒ–ï¼ˆ`loss_agg_mode="seq-mean-token-sum-norm"`ï¼‰

### 5.2 DrGRPO é…ç½®

```yaml
actor_rollout_ref:
  actor:
    # æ ¸å¿ƒå˜åŒ–
    loss_agg_mode: "seq-mean-token-sum-norm"  # å…¨å±€å½’ä¸€åŒ–
    loss_scale_factor: 512  # å¯é€‰ï¼šå›ºå®šå½’ä¸€åŒ–å› å­
    use_kl_loss: false  # DrGRPO ä¸ç”¨ KL loss

algorithm:
  norm_adv_by_std_in_grpo: false  # ä¸é™¤ä»¥æ ‡å‡†å·®
```

### 5.3 GRPO vs DrGRPO

| ç‰¹æ€§ | GRPO | DrGRPO |
|------|------|---------|
| **æ ‡å‡†å·®å½’ä¸€åŒ–** | âœ… ä½¿ç”¨ | âŒ ä¸ä½¿ç”¨ |
| **ä¼˜åŠ¿å…¬å¼** | `(r - Î¼) / Ïƒ` | `r - Î¼` |
| **Loss èšåˆ** | token-mean | seq-mean-token-sum-norm |
| **KL æ§åˆ¶** | KL loss | KL reward penalty |
| **é•¿åº¦åå·®** | å¯èƒ½å­˜åœ¨ | å‡è½» |
| **é€‚ç”¨åœºæ™¯** | ä¸€èˆ¬ä»»åŠ¡ | é•¿ CoT ä»»åŠ¡ |

---

## 6. è°ƒè¯•æŠ€å·§

### 6.1 æ·»åŠ ä¼˜åŠ¿è®¡ç®—æ—¥å¿—

```python
# åœ¨ verl/trainer/ppo/core_algos.py:303 ä¹‹åæ·»åŠ 

scores = token_level_rewards.sum(dim=-1)

# æ·»åŠ è°ƒè¯•è¾“å‡º
print(f"\n[GRPO Debug] Batch info:")
print(f"  Batch size: {scores.shape[0]}")
print(f"  Unique prompts: {len(np.unique(index))}")
print(f"  Samples per prompt: {len(index) // len(np.unique(index))}")
print(f"  Scores: mean={scores.mean():.4f}, std={scores.std():.4f}")
print(f"  Scores range: [{scores.min():.4f}, {scores.max():.4f}]")
```

### 6.2 æ£€æŸ¥åˆ†ç»„æ­£ç¡®æ€§

```python
# åœ¨ verl/trainer/ppo/core_algos.py:313 ä¹‹åæ·»åŠ 

for idx in id2score:
    scores_list = [s.item() for s in id2score[idx]]
    print(f"  Group {idx}: scores={scores_list}, mean={id2mean[idx]:.4f}, std={id2std[idx]:.4f}")
```

### 6.3 æŸ¥çœ‹ä¼˜åŠ¿åˆ†å¸ƒ

```python
# åœ¨ verl/trainer/ppo/core_algos.py:328 ä¹‹åæ·»åŠ 

print(f"\n[GRPO Debug] Advantages:")
print(f"  Mean: {scores.mean():.4f}")
print(f"  Std: {scores.std():.4f}")
print(f"  Min: {scores.min():.4f}, Max: {scores.max():.4f}")
print(f"  Positive ratio: {(scores > 0).float().mean():.2%}")
```

### 6.4 TensorBoard ç›‘æ§

å…³é”®æŒ‡æ ‡ï¼š
```python
# åœ¨ RayPPOTrainer ä¸­è®°å½•
metrics = {
    'grpo/mean_reward': rewards.mean(),
    'grpo/std_reward': rewards.std(),
    'grpo/mean_advantage': advantages.mean(),
    'grpo/positive_ratio': (advantages > 0).float().mean(),
    'grpo/group_size': rollout_n,
}
```

---

## 7. å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆ GRPO éœ€è¦ `rollout.n >= 2`ï¼Ÿ

**åŸå› ï¼š**
GRPO éœ€è¦å¤šä¸ªæ ·æœ¬æ¥ä¼°è®¡ç»„å†…æ–¹å·®ã€‚

**å¦‚æœ n=1ï¼š**
```python
id2mean[idx] = torch.tensor(0.0)
id2std[idx] = torch.tensor(1.0)
# ä¼˜åŠ¿å€¼ = reward / 1.0 = rewardï¼ˆæ²¡æœ‰å½’ä¸€åŒ–æ•ˆæœï¼‰
```

**æ¨è n >= 4ï¼š**
- n=2: æ–¹å·®ä¼°è®¡ä¸ç¨³å®š
- n=4: å¹³è¡¡ç‚¹
- n=8: æ›´å‡†ç¡®ï¼Œä½†æ…¢ 2 å€

### Q2: GRPO è®­ç»ƒä¸æ”¶æ•›æ€ä¹ˆåŠï¼Ÿ

**å¯èƒ½åŸå›  1ï¼š** `rollout.n` å¤ªå°
```yaml
# å¢å¤§é‡‡æ ·æ•°
actor_rollout_ref.rollout.n: 8
```

**å¯èƒ½åŸå›  2ï¼š** å­¦ä¹ ç‡å¤ªå¤§
```yaml
actor_rollout_ref.actor.optim.lr: 5e-7  # ä» 1e-6 é™ä½
```

**å¯èƒ½åŸå›  3ï¼š** Clipping å¤ªå®½æ¾
```yaml
actor_rollout_ref.actor.clip_ratio: 0.1  # ä» 0.2 é™ä½
```

### Q3: æ˜¾å­˜ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ

**æ–¹æ³• 1ï¼š** å‡å° `rollout.n`
```yaml
actor_rollout_ref.rollout.n: 2  # ä» 4 é™åˆ° 2
```

**æ–¹æ³• 2ï¼š** å‡å° batch size
```yaml
data.train_batch_size: 128  # ä» 256 é™ä½
```

**æ–¹æ³• 3ï¼š** ä½¿ç”¨ Gradient Checkpointing
```yaml
actor_rollout_ref.model.enable_gradient_checkpointing: true
```

### Q4: GRPO vs PPO å“ªä¸ªæ›´å¥½ï¼Ÿ

**GRPO æ›´é€‚åˆï¼š**
- âœ… æ•°å­¦æ¨ç†ï¼ˆGSM8K, MATHï¼‰
- âœ… ä»£ç ç”Ÿæˆï¼ˆHumanEvalï¼‰
- âœ… å¿«é€Ÿå®éªŒ
- âœ… GPU èµ„æºæœ‰é™

**PPO æ›´é€‚åˆï¼š**
- âœ… é•¿æ–‡æœ¬ç”Ÿæˆ
- âœ… å¯¹è¯è´¨é‡ä¼˜åŒ–
- âœ… éœ€è¦ç»†ç²’åº¦ä»·å€¼ä¼°è®¡
- âœ… è¿½æ±‚è®­ç»ƒç¨³å®šæ€§

---

## ğŸ“š å‚è€ƒèµ„æº

### è®ºæ–‡
- [DeepSeekMath (GRPO åŸå§‹è®ºæ–‡)](https://arxiv.org/pdf/2402.03300)
- [DrGRPO è®ºæ–‡](https://arxiv.org/pdf/2503.20783)

### ä»£ç ä½ç½®
- GRPO å®ç°: `verl/trainer/ppo/core_algos.py:266-330`
- å‘é‡åŒ– GRPO: `verl/trainer/ppo/core_algos.py:333-357`
- Pass@k GRPO: `verl/trainer/ppo/core_algos.py:360-430`

### å®˜æ–¹æ–‡æ¡£
- [GRPO æ–‡æ¡£](https://verl.readthedocs.io/en/latest/algo/grpo.html)
- [Baseline Performance](https://verl.readthedocs.io/en/latest/algo/baseline.html)

### ç¤ºä¾‹è„šæœ¬
- `examples/grpo_trainer/run_qwen3-8b.sh`
- `examples/grpo_trainer/run_gemma-2-9b.sh`

---

*æœ€åæ›´æ–°: 2026-01-26*
