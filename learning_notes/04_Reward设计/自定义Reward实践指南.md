# è‡ªå®šä¹‰ Reward å®è·µæŒ‡å—

> ä»é›¶å¼€å§‹æŒæ¡ Reward å‡½æ•°çš„è®¾è®¡ã€å®ç°å’Œä¼˜åŒ–

---

## ğŸ“– ç›®å½•

1. [Reward å‡½æ•°åŸºç¡€](#1-reward-å‡½æ•°åŸºç¡€)
2. [RewardManager è°ƒç”¨æµç¨‹](#2-rewardmanager-è°ƒç”¨æµç¨‹)
3. [3 ç§ Reward ç±»å‹è¯¦è§£](#3-3-ç§-reward-ç±»å‹è¯¦è§£)
4. [å®æˆ˜ç¤ºä¾‹ 1-5ï¼šåŸºç¡€ Reward](#4-å®æˆ˜ç¤ºä¾‹-1-5åŸºç¡€-reward)
5. [å®æˆ˜ç¤ºä¾‹ 6-10ï¼šé«˜çº§ Reward](#5-å®æˆ˜ç¤ºä¾‹-6-10é«˜çº§-reward)
6. [Reward Shaping æŠ€å·§](#6-reward-shaping-æŠ€å·§)
7. [è°ƒè¯•å’ŒéªŒè¯](#7-è°ƒè¯•å’ŒéªŒè¯)
8. [æ€§èƒ½ä¼˜åŒ–](#8-æ€§èƒ½ä¼˜åŒ–)
9. [æœ€ä½³å®è·µ](#9-æœ€ä½³å®è·µ)

---

## 1. Reward å‡½æ•°åŸºç¡€

### 1.1 ä»€ä¹ˆæ˜¯ Reward å‡½æ•°ï¼Ÿ

åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œ**Reward å‡½æ•°**å®šä¹‰äº†"ä»€ä¹ˆæ˜¯å¥½çš„è¡Œä¸º"ã€‚å®ƒæ¥æ”¶æ¨¡å‹çš„è¾“å‡ºï¼Œè¿”å›ä¸€ä¸ªåˆ†æ•°ï¼ŒæŒ‡å¯¼æ¨¡å‹æœç€æœŸæœ›çš„æ–¹å‘å­¦ä¹ ã€‚

```
è®­ç»ƒæµç¨‹ä¸­çš„ Reward:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Prompt  â”‚ -> â”‚  Model   â”‚ -> â”‚  Response   â”‚ -> â”‚  Reward  â”‚
â”‚          â”‚    â”‚ Generate â”‚    â”‚ "#### 42"   â”‚    â”‚  Score   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                           â†“
                                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                     â”‚ 1.0 or 0 â”‚
                                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ¸å¿ƒä½œç”¨ï¼š**
- å®šä¹‰"æ­£ç¡®"çš„æ ‡å‡†
- å¼•å¯¼æ¨¡å‹ä¼˜åŒ–æ–¹å‘
- å†³å®šè®­ç»ƒæ•ˆæœçš„ä¸Šé™

### 1.2 Reward å‡½æ•°çš„ç­¾å

verl ä¸­çš„ Reward å‡½æ•°å¿…é¡»éµå¾ªå›ºå®šçš„ç­¾åï¼š

```python
def compute_score(
    data_source: str,        # æ•°æ®æ¥æºï¼ˆå¦‚ "gsm8k", "my_task"ï¼‰
    solution_str: str,       # æ¨¡å‹ç”Ÿæˆçš„å®Œæ•´å“åº”
    ground_truth: str,       # æ­£ç¡®ç­”æ¡ˆï¼ˆä»æ•°æ®çš„ reward_model ä¸­è·å–ï¼‰
    extra_info: dict = None  # é¢å¤–ä¿¡æ¯ï¼ˆå¯é€‰ï¼Œç”¨äºä¼ é€’å…¶ä»–å‚æ•°ï¼‰
) -> float:                  # è¿”å›å¥–åŠ±åˆ†æ•°ï¼ˆé€šå¸¸ 0-1ï¼‰
    """
    è®¡ç®—å•ä¸ªå“åº”çš„å¥–åŠ±åˆ†æ•°

    Args:
        data_source: ç”¨äºåŒºåˆ†ä¸åŒæ•°æ®é›†ï¼Œå¯ä»¥åœ¨ä¸€ä¸ªå‡½æ•°ä¸­å¤„ç†å¤šä¸ªæ•°æ®é›†
        solution_str: æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ï¼Œå·²ç»è¿‡ detokenize
        ground_truth: æ­£ç¡®ç­”æ¡ˆï¼Œä»æ•°æ®ä¸­çš„ reward_model å­—æ®µæå–
        extra_info: å¯é€‰çš„é¢å¤–ä¿¡æ¯å­—å…¸

    Returns:
        float: å¥–åŠ±åˆ†æ•°ï¼Œæ¨èèŒƒå›´ 0-1
               1.0 = å®Œå…¨æ­£ç¡®
               0.0 = å®Œå…¨é”™è¯¯
               0-1 ä¹‹é—´ = éƒ¨åˆ†æ­£ç¡®
    """
    pass
```

**å‚æ•°è¯¦è§£ï¼š**

**data_source:**
- ç”¨é€”ï¼šåŒºåˆ†ä¸åŒçš„ä»»åŠ¡/æ•°æ®é›†
- ç¤ºä¾‹ï¼š`"gsm8k"`, `"math"`, `"code_generation"`
- ä½¿ç”¨åœºæ™¯ï¼šä¸€ä¸ª Reward å‡½æ•°å¤„ç†å¤šç§ä»»åŠ¡

```python
def compute_score(data_source, solution_str, ground_truth, extra_info=None):
    if data_source == "gsm8k":
        return gsm8k_reward(solution_str, ground_truth)
    elif data_source == "math":
        return math_reward(solution_str, ground_truth)
    else:
        raise ValueError(f"Unknown data_source: {data_source}")
```

**solution_str:**
- ç±»å‹ï¼š`str`ï¼ˆå·²ç» detokenizeï¼‰
- å†…å®¹ï¼šæ¨¡å‹ç”Ÿæˆçš„å®Œæ•´å“åº”æ–‡æœ¬
- ç¤ºä¾‹ï¼š`"Let's solve step by step. First, ... Therefore, the answer is #### 42"`

**ground_truth:**
- ç±»å‹ï¼š`str`ï¼ˆä»æ•°æ®ä¸­æå–ï¼‰
- å†…å®¹ï¼šæ­£ç¡®ç­”æ¡ˆæˆ–å‚è€ƒç­”æ¡ˆ
- æ¥æºï¼šæ•°æ®çš„ `reward_model` å­—æ®µä¸­çš„å‚æ•°
- ç¤ºä¾‹ï¼š`"42"`, `"\\frac{1}{2}"`, `"def solution():\n    return 42"`

**extra_info:**
- ç±»å‹ï¼š`dict` æˆ– `None`
- ç”¨é€”ï¼šä¼ é€’é¢å¤–å‚æ•°ï¼ˆå¦‚é…ç½®ã€å…ƒæ•°æ®ï¼‰
- ç¤ºä¾‹ï¼š`{"method": "strict", "format_score": 0.1}`

### 1.3 Reward çš„æ•°å€¼è®¾è®¡

#### æ¨èèŒƒå›´ï¼š0-1

**åŸå› ï¼š**
1. **æ ‡å‡†åŒ–**ï¼šä¾¿äºä¸åŒ Reward çš„å¯¹æ¯”å’Œç»„åˆ
2. **æ•°å€¼ç¨³å®š**ï¼šé¿å…æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±
3. **å¯è§£é‡Šæ€§**ï¼š0=é”™è¯¯ï¼Œ1=æ­£ç¡®ï¼Œä¸­é—´å€¼=éƒ¨åˆ†æ­£ç¡®

#### Binary Rewardï¼ˆäºŒå…ƒå¥–åŠ±ï¼‰

```python
def binary_reward(solution_str, ground_truth):
    """
    åªæœ‰ 0 æˆ– 1 ä¸¤ç§å¯èƒ½

    ä¼˜ç‚¹ï¼šç®€å•ã€æ˜ç¡®
    ç¼ºç‚¹ï¼šéš¾ä»¥å­¦ä¹ ã€æ ·æœ¬æ•ˆç‡ä½
    """
    answer = extract_answer(solution_str)
    return 1.0 if answer == ground_truth else 0.0
```

**é€‚ç”¨åœºæ™¯ï¼š**
- æ˜ç¡®çš„å¯¹é”™åˆ¤æ–­ï¼ˆå¦‚æ•°å­¦é¢˜ï¼‰
- æ•°æ®é›†è¶³å¤Ÿå¤§
- ä¸éœ€è¦ç»†ç²’åº¦åé¦ˆ

#### Graded Rewardï¼ˆåˆ†çº§å¥–åŠ±ï¼‰

```python
def graded_reward(solution_str, ground_truth):
    """
    å¤šä¸ªç¦»æ•£çš„å¥–åŠ±ç­‰çº§

    ä¼˜ç‚¹ï¼šæä¾›ä¸­é—´åé¦ˆ
    ç¼ºç‚¹ï¼šéœ€è¦äººå·¥å®šä¹‰ç­‰çº§
    """
    answer = extract_answer(solution_str)

    if answer == ground_truth:
        return 1.0  # å®Œå…¨æ­£ç¡®
    elif has_correct_format(solution_str):
        return 0.3  # æ ¼å¼æ­£ç¡®
    elif has_reasoning_steps(solution_str):
        return 0.1  # æœ‰æ¨ç†è¿‡ç¨‹
    else:
        return 0.0  # å®Œå…¨é”™è¯¯
```

**é€‚ç”¨åœºæ™¯ï¼š**
- æœ‰æ˜ç¡®çš„è¯„åˆ†æ ‡å‡†
- éœ€è¦é¼“åŠ±éƒ¨åˆ†æ­£ç¡®
- æ•°æ®é›†è¾ƒå°ï¼Œéœ€è¦å¯†é›†åé¦ˆ

#### Continuous Rewardï¼ˆè¿ç»­å¥–åŠ±ï¼‰

```python
def continuous_reward(solution_str, ground_truth):
    """
    0-1 ä¹‹é—´çš„è¿ç»­å€¼

    ä¼˜ç‚¹ï¼šæœ€å¤§åŒ–ä¿¡æ¯åˆ©ç”¨
    ç¼ºç‚¹ï¼šå¯èƒ½éš¾ä»¥è®¾è®¡åˆç†çš„è¿ç»­å‡½æ•°
    """
    # æ–¹æ³• 1ï¼šç›¸ä¼¼åº¦
    similarity = compute_similarity(solution_str, ground_truth)
    return similarity  # 0-1 ä¹‹é—´

    # æ–¹æ³• 2ï¼šå½’ä¸€åŒ–æŒ‡æ ‡
    metric = compute_metric(solution_str)
    max_metric = 100
    return min(1.0, metric / max_metric)

    # æ–¹æ³• 3ï¼šå¤šæŒ‡æ ‡åŠ æƒ
    accuracy = compute_accuracy(solution_str, ground_truth)
    length_score = compute_length_score(solution_str)
    format_score = compute_format_score(solution_str)

    return 0.6 * accuracy + 0.2 * length_score + 0.2 * format_score
```

**é€‚ç”¨åœºæ™¯ï¼š**
- æ–‡æœ¬ç”Ÿæˆè´¨é‡è¯„ä¼°
- å¤šç›®æ ‡ä¼˜åŒ–
- éœ€è¦ç»†ç²’åº¦åé¦ˆ

---

## 2. RewardManager è°ƒç”¨æµç¨‹

### 2.1 RewardManager æ¶æ„

**ä½ç½®ï¼š** `verl/trainer/ppo/reward.py`

```python
class RewardManager:
    """
    è´Ÿè´£è®¡ç®— batch ä¸­æ‰€æœ‰å“åº”çš„å¥–åŠ±åˆ†æ•°

    æ ¸å¿ƒæ–¹æ³•ï¼š
    - __call__(batch): è®¡ç®—æ•´ä¸ª batch çš„ reward
    - _call_single(data_item): è®¡ç®—å•ä¸ªæ ·æœ¬çš„ reward
    """

    def __init__(self, tokenizer, num_examine: int = 0):
        """
        Args:
            tokenizer: ç”¨äº detokenize
            num_examine: æ‰“å°å‰ N ä¸ªæ ·æœ¬ï¼ˆè°ƒè¯•ç”¨ï¼‰
        """
        self.tokenizer = tokenizer
        self.num_examine = num_examine

    def __call__(self, batch: DataProto) -> DataProto:
        """
        è®¡ç®— batch ä¸­æ‰€æœ‰æ ·æœ¬çš„ reward

        æµç¨‹ï¼š
        1. æå– responses å’Œ reward_model é…ç½®
        2. Detokenize responses
        3. è°ƒç”¨ compute_score_fn
        4. è¿”å› token-level rewards
        """
        pass
```

### 2.2 å®Œæ•´è°ƒç”¨æµç¨‹

```python
# ==================== åœ¨ RayPPOTrainer._train_step ä¸­ ====================

# é˜¶æ®µ 1: Rollout - ç”Ÿæˆå“åº”
rollout_output = self.actor_rollout_wg.generate_sequences(batch)
# rollout_output.batch åŒ…å«:
#   'responses': (bs, response_len) - token IDs
#   'response_mask': (bs, response_len)

# é˜¶æ®µ 2: Reward - è®¡ç®—å¥–åŠ±ï¼ˆè°ƒç”¨ RewardManagerï¼‰
rollout_output = self._compute_reward(rollout_output)

# ==================== _compute_reward å®ç° ====================

def _compute_reward(self, rollout_output: DataProto):
    # è°ƒç”¨ RewardManager
    rollout_output = self.reward_manager(rollout_output.batch)

    return rollout_output

# ==================== RewardManager.__call__ å®ç° ====================

def __call__(self, batch: DataProto) -> DataProto:
    # 1. æå–å¿…è¦ä¿¡æ¯
    responses = batch["responses"]           # (bs, response_len)
    reward_models = batch["reward_model"]    # List[dict]
    data_sources = batch["data_source"]      # List[str]

    # 2. Detokenize
    response_strs = self.tokenizer.batch_decode(
        responses,
        skip_special_tokens=True
    )

    # 3. é€ä¸ªè®¡ç®— reward
    scores = []
    for i in range(len(response_strs)):
        reward_config = reward_models[i]
        data_source = data_sources[i]
        solution_str = response_strs[i]

        # æå– ground_truth
        ground_truth = reward_config.get("ground_truth", "")

        # è·å– compute_score å‡½æ•°
        compute_fn = self._get_compute_fn(reward_config)

        # è®¡ç®—åˆ†æ•°
        score = compute_fn(
            data_source=data_source,
            solution_str=solution_str,
            ground_truth=ground_truth,
            extra_info=reward_config.get("extra_info")
        )

        scores.append(score)

    # 4. è½¬æ¢ä¸º token-level rewards
    token_level_rewards = self._to_token_level(scores, batch)

    # 5. å­˜å› batch
    batch["token_level_rewards"] = token_level_rewards
    batch["rewards"] = torch.tensor(scores)

    return batch

# ==================== _get_compute_fn å®ç° ====================

def _get_compute_fn(self, reward_config):
    style = reward_config["style"]

    if style == "rule":
        # Rule-based: å¯¼å…¥ Python æ¨¡å—
        module_path = reward_config["module"]
        function_name = reward_config.get("function", "compute_score")

        module = importlib.import_module(module_path)
        compute_fn = getattr(module, function_name)

    elif style == "model":
        # Model-based: åŠ è½½ Reward Model
        model_path = reward_config["path"]
        compute_fn = self._load_reward_model(model_path)

    elif style == "sandbox":
        # Sandbox: ä»£ç æ‰§è¡Œ
        compute_fn = self._create_sandbox_fn(reward_config)

    else:
        raise ValueError(f"Unknown style: {style}")

    return compute_fn
```

### 2.3 æ•°æ®æµç¤ºæ„å›¾

```
è¾“å…¥ batch:
{
    "responses": [[101, 2023, 2003, ...], [...]],  # token IDs
    "reward_model": [
        {
            "style": "rule",
            "module": "verl.utils.reward_score.gsm8k",
            "ground_truth": "42"
        },
        ...
    ],
    "data_source": ["gsm8k", ...]
}

    â†“ Detokenize

response_strs: [
    "Let's solve step by step. ... #### 42",
    ...
]

    â†“ For each response

compute_score(
    data_source="gsm8k",
    solution_str="Let's solve step by step. ... #### 42",
    ground_truth="42"
)

    â†“ Extract answer "42"

answer == ground_truth  â†’ score = 1.0

    â†“ Convert to token-level

token_level_rewards: [
    [0, 0, 0, ..., 1.0],  # åªæœ‰æœ€åä¸€ä¸ª token æœ‰å¥–åŠ±
    ...
]

è¾“å‡º batch:
{
    ...(åŸæœ‰å­—æ®µ),
    "token_level_rewards": tensor([[0,0,...,1], [...]]),
    "rewards": tensor([1.0, ...])
}
```

---

## 3. 3 ç§ Reward ç±»å‹è¯¦è§£

### 3.1 Rule-based Reward

**å®šä¹‰ï¼š** åŸºäºè§„åˆ™å’Œæ¨¡å¼åŒ¹é…çš„ Reward å‡½æ•°

**ä¼˜ç‚¹ï¼š**
- âœ… å®ç°ç®€å•
- âœ… è®¡ç®—å¿«é€Ÿ
- âœ… å®Œå…¨å¯æ§ã€å¯è§£é‡Š
- âœ… ä¸éœ€è¦é¢å¤–æ¨¡å‹

**ç¼ºç‚¹ï¼š**
- âŒ éœ€è¦äººå·¥è®¾è®¡è§„åˆ™
- âŒ æ³›åŒ–èƒ½åŠ›æœ‰é™
- âŒ è§„åˆ™å¯èƒ½è¿‡äºä¸¥æ ¼æˆ–å®½æ¾

**é…ç½®ï¼š**
```python
reward_model = {
    "style": "rule",
    "module": "verl.utils.reward_score.gsm8k",  # Python æ¨¡å—è·¯å¾„
    "function": "compute_score",                 # å‡½æ•°åï¼ˆé»˜è®¤ compute_scoreï¼‰
    "ground_truth": "42",                        # ä¼ é€’ç»™å‡½æ•°çš„å‚æ•°
    "method": "strict"                           # extra_info ä¸­çš„å…¶ä»–å‚æ•°
}
```

**ç¤ºä¾‹ 1ï¼šGSM8K Reward**

**ä½ç½®ï¼š** `verl/utils/reward_score/gsm8k.py`

```python
def compute_score(
    data_source,
    solution_str,
    ground_truth,
    method="strict",
    format_score=0.0,
    score=1.0
):
    """
    GSM8K Reward: æå– #### åçš„æ•°å­—å¹¶æ¯”è¾ƒ

    Args:
        method: "strict" æˆ– "flexible"
        format_score: æ ¼å¼æ­£ç¡®ä½†ç­”æ¡ˆé”™è¯¯çš„åˆ†æ•°
        score: ç­”æ¡ˆæ­£ç¡®çš„åˆ†æ•°

    Returns:
        float: 0.0, format_score, æˆ– score
    """
    # 1. æå–ç­”æ¡ˆ
    answer = extract_solution(solution_str, method=method)

    # 2. åˆ¤æ–­
    if answer is None:
        # æ²¡æœ‰æ‰¾åˆ°ç­”æ¡ˆï¼ˆæ ¼å¼é”™è¯¯ï¼‰
        return 0.0
    elif answer == ground_truth:
        # ç­”æ¡ˆæ­£ç¡®
        return score
    else:
        # æ ¼å¼æ­£ç¡®ï¼Œç­”æ¡ˆé”™è¯¯
        return format_score


def extract_solution(solution_str, method="strict"):
    """
    ä»å“åº”ä¸­æå–ç­”æ¡ˆ

    strict: åŒ¹é… "#### number" æ ¼å¼
    flexible: æå–æœ€åä¸€ä¸ªæ•°å­—
    """
    if method == "strict":
        # ä¸¥æ ¼æ¨¡å¼ï¼šå¿…é¡»æœ‰ "####"
        solutions = re.findall(r"#### (\-?[0-9\.\,]+)", solution_str)
        if solutions:
            # å–æœ€åä¸€ä¸ªåŒ¹é…
            return solutions[-1].replace(",", "").replace("$", "")
        else:
            return None

    elif method == "flexible":
        # å®½æ¾æ¨¡å¼ï¼šæå–æœ€åä¸€ä¸ªæ•°å­—
        numbers = re.findall(r"(\-?[0-9\.\,]+)", solution_str)
        if numbers:
            return numbers[-1].replace(",", "").replace("$", "")
        else:
            return None
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# æµ‹è¯•
solution_1 = "Let's solve step by step. ... Therefore, #### 42"
solution_2 = "The answer is 42."
solution_3 = "I don't know."

print(compute_score("gsm8k", solution_1, "42", method="strict"))
# è¾“å‡º: 1.0 (æ ¼å¼å’Œç­”æ¡ˆéƒ½æ­£ç¡®)

print(compute_score("gsm8k", solution_2, "42", method="strict"))
# è¾“å‡º: 0.0 (æ²¡æœ‰ "####", æ ¼å¼é”™è¯¯)

print(compute_score("gsm8k", solution_2, "42", method="flexible"))
# è¾“å‡º: 1.0 (flexible æ¨¡å¼æå–åˆ° "42")

print(compute_score("gsm8k", solution_3, "42", method="strict"))
# è¾“å‡º: 0.0 (æ²¡æœ‰æ•°å­—)
```

**ç¤ºä¾‹ 2ï¼šMATH Reward**

**ä½ç½®ï¼š** `verl/utils/reward_score/math_reward.py`

```python
def compute_score(data_source, solution_str, ground_truth):
    """
    MATH Reward: æå– \\boxed{answer} å¹¶æ¯”è¾ƒ

    æ­¥éª¤ï¼š
    1. æ‰¾åˆ°æœ€åä¸€ä¸ª \\boxed{...}
    2. æå–å…¶ä¸­çš„å†…å®¹
    3. æ ‡å‡†åŒ–ï¼ˆå»é™¤ç©ºæ ¼ã€ç‰¹æ®Šç¬¦å·ç­‰ï¼‰
    4. æ¯”è¾ƒ
    """
    try:
        # 1. æå– boxed å†…å®¹
        boxed_str = last_boxed_only_string(solution_str)

        if boxed_str is None:
            return 0.0

        # 2. å»é™¤ \boxed{ å’Œ }
        answer = remove_boxed(boxed_str)

        # 3. æ ‡å‡†åŒ–å¹¶æ¯”è¾ƒ
        if is_equiv(answer, ground_truth):
            return 1.0
        else:
            return 0.0

    except Exception as e:
        print(f"Error in compute_score: {e}")
        return 0.0


def last_boxed_only_string(string):
    """
    æå–æœ€åä¸€ä¸ª \\boxed{...} çš„å®Œæ•´å­—ç¬¦ä¸²

    ç¤ºä¾‹ï¼š
    "The answer is \\boxed{42} and \\boxed{43}"
    â†’ "\\boxed{43}"
    """
    idx = string.rfind("\\boxed")
    if idx < 0:
        # å°è¯• \\fbox
        idx = string.rfind("\\fbox")
        if idx < 0:
            return None

    # æ‰¾åˆ°åŒ¹é…çš„å³æ‹¬å·
    i = idx
    num_left_braces = 0
    while i < len(string):
        if string[i] == "{":
            num_left_braces += 1
        if string[i] == "}":
            num_left_braces -= 1
            if num_left_braces == 0:
                return string[idx : i + 1]
        i += 1

    return None


def is_equiv(str1, str2):
    """
    åˆ¤æ–­ä¸¤ä¸ªæ•°å­¦è¡¨è¾¾å¼æ˜¯å¦ç­‰ä»·

    åŒ…å«å¤§é‡çš„æ ‡å‡†åŒ–æ“ä½œï¼š
    - å»é™¤ç©ºæ ¼
    - æ ‡å‡†åŒ–åˆ†æ•°è¡¨ç¤º
    - å¤„ç† LaTeX ç¬¦å·
    - ...
    """
    str1 = strip_string(str1)
    str2 = strip_string(str2)
    return str1 == str2


def strip_string(string):
    """
    æ ‡å‡†åŒ–å­—ç¬¦ä¸²

    æ“ä½œï¼š
    - å»é™¤æ¢è¡Œ
    - å»é™¤åæ–œæ 
    - æ ‡å‡†åŒ–åˆ†æ•°ï¼ˆ\\frac{a}{b}ï¼‰
    - å»é™¤å•ä½
    - å»é™¤ç™¾åˆ†å·
    - ...
    """
    # 1. å»é™¤æ¢è¡Œ
    string = string.replace("\n", "")

    # 2. å»é™¤ \\ å’Œç‰¹æ®Šç¬¦å·
    string = string.replace("\\\\", "\\")
    string = string.replace("\\left", "")
    string = string.replace("\\right", "")

    # 3. æ ‡å‡†åŒ–åˆ†æ•°
    string = fix_fracs(string)

    # 4. å»é™¤ç©ºæ ¼
    string = string.replace(" ", "")

    # ... æ›´å¤šæ ‡å‡†åŒ–æ“ä½œ

    return string
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
solution_1 = "The solution is \\boxed{\\frac{1}{2}}"
solution_2 = "Answer: \\boxed{0.5}"
solution_3 = "I think it's \\boxed{42}"

print(compute_score("math", solution_1, "\\frac{1}{2}"))
# è¾“å‡º: 1.0

print(compute_score("math", solution_2, "\\frac{1}{2}"))
# è¾“å‡º: 1.0 (0.5 ä¼šè¢«è½¬æ¢ä¸º \\frac{1}{2})

print(compute_score("math", solution_3, "\\frac{1}{2}"))
# è¾“å‡º: 0.0
```

### 3.2 Model-based Reward

**å®šä¹‰ï¼š** ä½¿ç”¨è®­ç»ƒå¥½çš„ Reward Model æ¥è¯„åˆ†

**ä¼˜ç‚¹ï¼š**
- âœ… è‡ªåŠ¨å­¦ä¹ å¤æ‚çš„è¯„åˆ†æ ‡å‡†
- âœ… æ³›åŒ–èƒ½åŠ›å¼º
- âœ… é€‚åˆä¸»è§‚è¯„ä»·ï¼ˆå¦‚å¯¹è¯è´¨é‡ï¼‰

**ç¼ºç‚¹ï¼š**
- âŒ éœ€è¦è®­ç»ƒ Reward Model
- âŒ è®¡ç®—æ…¢ï¼ˆéœ€è¦å‰å‘ä¼ æ’­ï¼‰
- âŒ å¯èƒ½å­¦ä¹ åˆ°é”™è¯¯çš„åå¥½

**é…ç½®ï¼š**
```python
reward_model = {
    "style": "model",
    "path": "path/to/reward_model",       # Reward Model è·¯å¾„
    "model_type": "sequence_classification",  # æ¨¡å‹ç±»å‹
    "device": "cuda:0"                    # è¿è¡Œè®¾å¤‡
}
```

**å®ç°ç¤ºä¾‹ï¼š**

```python
class ModelBasedRewardManager:
    def __init__(self, model_path, device="cuda"):
        # åŠ è½½ Reward Model
        from transformers import AutoModelForSequenceClassification, AutoTokenizer

        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
        self.model.to(device)
        self.model.eval()
        self.device = device

    def compute_score(self, data_source, solution_str, ground_truth, extra_info=None):
        """
        ä½¿ç”¨ Reward Model è¯„åˆ†

        è¾“å…¥ï¼šprompt + response
        è¾“å‡ºï¼š0-1 ä¹‹é—´çš„åˆ†æ•°
        """
        # 1. æ„é€ è¾“å…¥ï¼ˆå¯èƒ½éœ€è¦ promptï¼‰
        if extra_info and "prompt" in extra_info:
            text = extra_info["prompt"] + "\n" + solution_str
        else:
            text = solution_str

        # 2. Tokenize
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=512
        ).to(self.device)

        # 3. å‰å‘ä¼ æ’­
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits  # (1, 2) for binary classification

        # 4. è®¡ç®—åˆ†æ•°
        # å‡è®¾ label 0 = bad, label 1 = good
        probs = torch.softmax(logits, dim=-1)
        score = probs[0, 1].item()  # å–"good"çš„æ¦‚ç‡

        return score
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# åˆ›å»º Reward Manager
reward_mgr = ModelBasedRewardManager("OpenAssistant/reward-model-deberta-v3-large")

# è®¡ç®—åˆ†æ•°
score = reward_mgr.compute_score(
    data_source="dialog",
    solution_str="Thank you for your question! The answer is 42.",
    ground_truth="",
    extra_info={"prompt": "What is the meaning of life?"}
)

print(f"Score: {score:.4f}")
# è¾“å‡º: Score: 0.8523 (ç¤ºä¾‹)
```

### 3.3 Sandbox Rewardï¼ˆä»£ç æ‰§è¡Œï¼‰

**å®šä¹‰ï¼š** åœ¨æ²™ç®±ç¯å¢ƒä¸­æ‰§è¡Œä»£ç ï¼Œæ ¹æ®æµ‹è¯•ç»“æœè¯„åˆ†

**ä¼˜ç‚¹ï¼š**
- âœ… å‡†ç¡®ï¼ˆå®é™…æ‰§è¡Œï¼‰
- âœ… é€‚åˆä»£ç ç”Ÿæˆä»»åŠ¡
- âœ… å¯ä»¥æµ‹è¯•åŠŸèƒ½æ­£ç¡®æ€§

**ç¼ºç‚¹ï¼š**
- âŒ è®¡ç®—æ…¢
- âŒ éœ€è¦å®‰å…¨éš”ç¦»ï¼ˆé˜²æ­¢æ¶æ„ä»£ç ï¼‰
- âŒ å¯èƒ½æœ‰è¶…æ—¶ã€é”™è¯¯ç­‰é—®é¢˜

**é…ç½®ï¼š**
```python
reward_model = {
    "style": "sandbox",
    "language": "python",              # æ‰§è¡Œè¯­è¨€
    "timeout": 5,                      # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    "test_cases": [                    # æµ‹è¯•ç”¨ä¾‹
        {"input": [1, 2], "expected": 3},
        {"input": [5, 3], "expected": 8}
    ]
}
```

**å®ç°ç¤ºä¾‹ï¼š**

```python
import subprocess
import tempfile
import os

def sandbox_compute_score(data_source, solution_str, ground_truth, extra_info=None):
    """
    åœ¨æ²™ç®±ä¸­æ‰§è¡Œä»£ç å¹¶è¯„åˆ†

    æµç¨‹ï¼š
    1. æå–ä»£ç 
    2. å†™å…¥ä¸´æ—¶æ–‡ä»¶
    3. åœ¨éš”ç¦»ç¯å¢ƒä¸­æ‰§è¡Œ
    4. æ£€æŸ¥æµ‹è¯•ç”¨ä¾‹
    5. è¿”å›é€šè¿‡ç‡
    """
    # 1. æå–ä»£ç 
    code = extract_code(solution_str)

    # 2. è·å–æµ‹è¯•ç”¨ä¾‹
    test_cases = extra_info.get("test_cases", [])

    # 3. æ‰§è¡Œæµ‹è¯•
    passed = 0
    for test in test_cases:
        try:
            result = execute_code_safely(
                code,
                test["input"],
                timeout=extra_info.get("timeout", 5)
            )

            if result == test["expected"]:
                passed += 1

        except Exception as e:
            # æ‰§è¡Œå¤±è´¥ï¼ˆè¶…æ—¶ã€é”™è¯¯ç­‰ï¼‰
            continue

    # 4. è®¡ç®—é€šè¿‡ç‡
    if len(test_cases) == 0:
        return 0.0

    pass_rate = passed / len(test_cases)
    return pass_rate


def extract_code(solution_str):
    """
    ä»å“åº”ä¸­æå–ä»£ç å—

    æ”¯æŒæ ¼å¼ï¼š
    - ```python ... ```
    - ```\n...\n```
    """
    # åŒ¹é…ä»£ç å—
    pattern = r"```(?:python)?\n(.*?)\n```"
    matches = re.findall(pattern, solution_str, re.DOTALL)

    if matches:
        return matches[0]
    else:
        # å‡è®¾æ•´ä¸ªå“åº”å°±æ˜¯ä»£ç 
        return solution_str


def execute_code_safely(code, inputs, timeout=5):
    """
    åœ¨éš”ç¦»ç¯å¢ƒä¸­å®‰å…¨æ‰§è¡Œä»£ç 

    ä½¿ç”¨ subprocess + timeout éš”ç¦»
    """
    # 1. åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(
        mode='w',
        suffix='.py',
        delete=False
    ) as f:
        # å†™å…¥ä»£ç 
        f.write(code)
        f.write("\n\n")
        # å†™å…¥æµ‹è¯•
        f.write(f"result = solution({inputs})\n")
        f.write("print(result)\n")
        temp_file = f.name

    try:
        # 2. æ‰§è¡Œ
        result = subprocess.run(
            ["python", temp_file],
            capture_output=True,
            text=True,
            timeout=timeout
        )

        # 3. è§£æè¾“å‡º
        output = result.stdout.strip()
        return eval(output)  # æ³¨æ„ï¼šä¸å®‰å…¨ï¼ç”Ÿäº§ç¯å¢ƒéœ€è¦æ›´å®‰å…¨çš„è§£æ

    except subprocess.TimeoutExpired:
        raise TimeoutError("Code execution timeout")

    except Exception as e:
        raise RuntimeError(f"Code execution failed: {e}")

    finally:
        # 4. æ¸…ç†
        os.remove(temp_file)
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
solution = """
```python
def solution(a, b):
    return a + b
```
"""

test_cases = [
    {"input": [1, 2], "expected": 3},
    {"input": [5, 3], "expected": 8},
    {"input": [-1, 1], "expected": 0}
]

score = sandbox_compute_score(
    data_source="code",
    solution_str=solution,
    ground_truth="",
    extra_info={"test_cases": test_cases, "timeout": 5}
)

print(f"Pass rate: {score:.2%}")
# è¾“å‡º: Pass rate: 100.00% (æ‰€æœ‰æµ‹è¯•é€šè¿‡)
```

---

## 4. å®æˆ˜ç¤ºä¾‹ 1-5ï¼šåŸºç¡€ Reward

### ç¤ºä¾‹ 1ï¼šé•¿åº¦å¥–åŠ±

**ç›®æ ‡ï¼š** é¼“åŠ±ç”Ÿæˆç‰¹å®šé•¿åº¦çš„å“åº”

```python
def length_reward(data_source, solution_str, ground_truth, extra_info=None):
    """
    é•¿åº¦å¥–åŠ±ï¼šé¼“åŠ±åœ¨ç›®æ ‡é•¿åº¦é™„è¿‘çš„å“åº”

    Args:
        extra_info: {"target_length": 100, "tolerance": 20}
    """
    target = extra_info.get("target_length", 100) if extra_info else 100
    tolerance = extra_info.get("tolerance", 20) if extra_info else 20

    actual = len(solution_str)

    # åœ¨ [target - tolerance, target + tolerance] èŒƒå›´å†…å¾—æ»¡åˆ†
    if abs(actual - target) <= tolerance:
        return 1.0
    else:
        # è¶…å‡ºèŒƒå›´ï¼Œçº¿æ€§æƒ©ç½š
        penalty = abs(actual - target) / target
        return max(0.0, 1.0 - penalty)
```

**æµ‹è¯•ï¼š**
```python
assert length_reward("", "x" * 90, "", {"target_length": 100, "tolerance": 20}) == 1.0
assert length_reward("", "x" * 150, "", {"target_length": 100, "tolerance": 20}) < 1.0
```

### ç¤ºä¾‹ 2ï¼šæ ¼å¼æ£€æŸ¥å¥–åŠ±

**ç›®æ ‡ï¼š** æ£€æŸ¥å“åº”æ˜¯å¦åŒ…å«å¿…éœ€çš„æ ¼å¼å…ƒç´ 

```python
def format_check_reward(data_source, solution_str, ground_truth, extra_info=None):
    """
    æ ¼å¼æ£€æŸ¥å¥–åŠ±

    æ£€æŸ¥é¡¹ï¼š
    - æ˜¯å¦æœ‰æ ‡é¢˜
    - æ˜¯å¦æœ‰æ¨ç†æ­¥éª¤
    - æ˜¯å¦æœ‰æœ€ç»ˆç­”æ¡ˆæ ‡è®°
    """
    score = 0.0

    # æ£€æŸ¥ 1ï¼šæœ‰"Let's solve"æˆ–ç±»ä¼¼å¼€å¤´
    if any(phrase in solution_str.lower() for phrase in [
        "let's solve",
        "let us solve",
        "to solve this"
    ]):
        score += 0.3

    # æ£€æŸ¥ 2ï¼šæœ‰æ­¥éª¤æ ‡è®°ï¼ˆ"Step 1", "First", etc.ï¼‰
    if any(phrase in solution_str for phrase in [
        "Step 1",
        "First,",
        "1.",
        "Firstly,"
    ]):
        score += 0.3

    # æ£€æŸ¥ 3ï¼šæœ‰æœ€ç»ˆç­”æ¡ˆæ ‡è®°
    if "####" in solution_str or "Therefore" in solution_str:
        score += 0.4

    return score
```

### ç¤ºä¾‹ 3ï¼šå…³é”®è¯å¥–åŠ±

**ç›®æ ‡ï¼š** é¼“åŠ±åŒ…å«ç‰¹å®šå…³é”®è¯

```python
def keyword_reward(data_source, solution_str, ground_truth, extra_info=None):
    """
    å…³é”®è¯å¥–åŠ±

    Args:
        extra_info: {"keywords": ["important", "key phrase"], "weights": [0.6, 0.4]}
    """
    keywords = extra_info.get("keywords", []) if extra_info else []
    weights = extra_info.get("weights", [1.0 / len(keywords)] * len(keywords)) if extra_info else []

    if len(keywords) == 0:
        return 0.0

    # ç¡®ä¿ weights é•¿åº¦åŒ¹é…
    if len(weights) != len(keywords):
        weights = [1.0 / len(keywords)] * len(keywords)

    score = 0.0
    for keyword, weight in zip(keywords, weights):
        if keyword.lower() in solution_str.lower():
            score += weight

    return min(1.0, score)  # Cap at 1.0
```

**æµ‹è¯•ï¼š**
```python
extra_info = {
    "keywords": ["reasoning", "step by step"],
    "weights": [0.6, 0.4]
}

text = "Let me explain with step by step reasoning."
score = keyword_reward("", text, "", extra_info)
print(score)  # 1.0 (both keywords present)
```

### ç¤ºä¾‹ 4ï¼šç¦è¯æƒ©ç½š

**ç›®æ ‡ï¼š** æƒ©ç½šåŒ…å«ç¦ç”¨è¯çš„å“åº”

```python
def forbidden_word_penalty(data_source, solution_str, ground_truth, extra_info=None):
    """
    ç¦è¯æƒ©ç½š

    Args:
        extra_info: {"forbidden": ["bad word", "inappropriate"], "penalty_per_word": 0.2}
    """
    forbidden = extra_info.get("forbidden", []) if extra_info else []
    penalty_per_word = extra_info.get("penalty_per_word", 0.2) if extra_info else 0.2

    solution_lower = solution_str.lower()

    penalty = 0.0
    for word in forbidden:
        if word.lower() in solution_lower:
            penalty += penalty_per_word

    return max(0.0, 1.0 - penalty)
```

### ç¤ºä¾‹ 5ï¼šç»„åˆå¥–åŠ±

**ç›®æ ‡ï¼š** ç»“åˆå¤šä¸ª Reward å‡½æ•°

```python
def combined_reward(data_source, solution_str, ground_truth, extra_info=None):
    """
    ç»„åˆå¥–åŠ±ï¼šå‡†ç¡®æ€§ + æ ¼å¼ + é•¿åº¦

    æƒé‡ï¼š
    - å‡†ç¡®æ€§ï¼š60%
    - æ ¼å¼ï¼š20%
    - é•¿åº¦ï¼š20%
    """
    # 1. å‡†ç¡®æ€§ï¼ˆBinaryï¼‰
    answer = extract_answer(solution_str)
    accuracy = 1.0 if answer == ground_truth else 0.0

    # 2. æ ¼å¼
    format_score = format_check_reward(data_source, solution_str, ground_truth)

    # 3. é•¿åº¦
    length_score = length_reward(
        data_source,
        solution_str,
        ground_truth,
        {"target_length": 200, "tolerance": 50}
    )

    # 4. åŠ æƒç»„åˆ
    total_score = (
        0.6 * accuracy +
        0.2 * format_score +
        0.2 * length_score
    )

    return total_score
```

**æµ‹è¯•ï¼š**
```python
# å®Œå…¨æ­£ç¡®ï¼Œæ ¼å¼å¥½ï¼Œé•¿åº¦åˆé€‚
solution_good = "Let's solve step by step. ... Therefore, #### 42"
print(combined_reward("gsm8k", solution_good, "42"))  # æ¥è¿‘ 1.0

# ç­”æ¡ˆé”™è¯¯ï¼Œä½†æ ¼å¼å’Œé•¿åº¦å¥½
solution_wrong = "Step by step. ... Therefore, #### 43"
print(combined_reward("gsm8k", solution_wrong, "42"))  # çº¦ 0.4

# ç­”æ¡ˆæ­£ç¡®ï¼Œä½†æ ¼å¼å·®
solution_bad_format = "42"
print(combined_reward("gsm8k", solution_bad_format, "42"))  # çº¦ 0.6
```

---

*ï¼ˆç”±äºç¯‡å¹…é™åˆ¶ï¼Œç»§ç»­åœ¨ä¸‹ä¸€éƒ¨åˆ†ï¼‰*
