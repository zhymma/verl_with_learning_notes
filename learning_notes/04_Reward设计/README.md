# 04 - Reward è®¾è®¡

> ç¬¬å››éƒ¨åˆ†ï¼šæ·±å…¥ç†è§£ Reward ç³»ç»Ÿå’Œå®ç°è‡ªå®šä¹‰ Reward å‡½æ•°

---

## ğŸ“š æœ¬ç« å†…å®¹

### ğŸ“– å­¦ä¹ ç¬”è®°

#### **è‡ªå®šä¹‰Rewardå®è·µæŒ‡å—.md** - å®Œæ•´å®æˆ˜æ•™ç¨‹ï¼ˆæ–°ï¼ï¼‰
- Reward å‡½æ•°åŸºç¡€æ¦‚å¿µ
- RewardManager è°ƒç”¨æµç¨‹è¯¦è§£
- 3 ç§ Reward ç±»å‹å¯¹æ¯”ï¼ˆRule-based, Model-based, Sandboxï¼‰
- 10+ ä¸ªå®æˆ˜ç¤ºä¾‹
  - æ•°å­¦æ¨ç† Rewardï¼ˆGSM8K, MATHï¼‰
  - ä»£ç ç”Ÿæˆ Rewardï¼ˆHumanEval, MBPPï¼‰
  - æ–‡æœ¬è´¨é‡ Rewardï¼ˆé•¿åº¦ã€æ ¼å¼ã€å¤šæ ·æ€§ï¼‰
  - å¤šç›®æ ‡ Rewardï¼ˆå‡†ç¡®æ€§ + ç®€æ´æ€§ï¼‰
  - Reward Shaping æŠ€å·§
- Reward è°ƒè¯•å®Œæ•´æµç¨‹
- æœ€ä½³å®è·µå’Œå¸¸è§é”™è¯¯
- æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### ğŸ› ï¸ å®æˆ˜ä»£ç 

æœ¬éƒ¨åˆ†æä¾›**æºç çº§åˆ«çš„ç¤ºä¾‹åˆ†æ**ï¼Œæ‰€æœ‰ç¤ºä¾‹éƒ½å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®æ‰¾åˆ°ï¼š
- å†…ç½® Reward: `verl/utils/reward_score/`
- ç¤ºä¾‹æ•°æ®: `examples/data_preprocess/`

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ­¥éª¤ 1ï¼šç†è§£ Reward ç±»å‹

```python
# ç±»å‹ 1: Rule-based Rewardï¼ˆè§„åˆ™åŒ¹é…ï¼‰
def gsm8k_reward(solution_str, ground_truth):
    # æå–ç­”æ¡ˆ
    answer = extract_solution(solution_str)
    # æ¯”è¾ƒ
    return 1.0 if answer == ground_truth else 0.0

# ç±»å‹ 2: Model-based Rewardï¼ˆä½¿ç”¨ Reward Modelï¼‰
reward_model = AutoModelForSequenceClassification.from_pretrained("...")
score = reward_model(prompt, response)

# ç±»å‹ 3: Sandbox Rewardï¼ˆä»£ç æ‰§è¡Œï¼‰
def code_reward(code_str, test_cases):
    # æ‰§è¡Œä»£ç 
    result = execute_code(code_str, test_cases)
    # è¿”å›é€šè¿‡ç‡
    return result.pass_rate
```

### æ­¥éª¤ 2ï¼šå®ç°ä½ çš„ç¬¬ä¸€ä¸ª Reward

åˆ›å»º `my_reward.py`:
```python
def compute_score(data_source, solution_str, ground_truth, extra_info=None):
    """
    è‡ªå®šä¹‰ Reward å‡½æ•°

    Args:
        data_source (str): æ•°æ®æ¥æºï¼ˆå¦‚ "my_task"ï¼‰
        solution_str (str): æ¨¡å‹ç”Ÿæˆçš„å“åº”
        ground_truth (str): æ­£ç¡®ç­”æ¡ˆ
        extra_info (dict): é¢å¤–ä¿¡æ¯

    Returns:
        float: å¥–åŠ±åˆ†æ•°ï¼ˆé€šå¸¸ 0-1ï¼‰
    """
    # ç¤ºä¾‹ï¼šé•¿åº¦å¥–åŠ±
    target_length = 100
    actual_length = len(solution_str)

    # å¥–åŠ±åœ¨ 80-120 å­—ä¹‹é—´çš„å“åº”
    if 80 <= actual_length <= 120:
        return 1.0
    else:
        # è¶…å‡ºèŒƒå›´ï¼Œçº¿æ€§æƒ©ç½š
        penalty = abs(actual_length - target_length) / target_length
        return max(0.0, 1.0 - penalty)
```

### æ­¥éª¤ 3ï¼šé…ç½®ä½¿ç”¨è‡ªå®šä¹‰ Reward

```bash
python3 -m verl.trainer.main_ppo \
    data.train_files=my_data.parquet \
    custom_reward_function.path=/path/to/my_reward.py \
    custom_reward_function.name=compute_score
```

æˆ–è€…åœ¨æ•°æ®ä¸­é…ç½®ï¼š
```python
# æ•°æ®å‡†å¤‡æ—¶
data = {
    "data_source": "my_task",
    "prompt": "Write a summary...",
    "reward_model": {
        "style": "rule",
        "module": "my_reward",
        "function": "compute_score"
    }
}
```

---

## ğŸ“– æ¨èå­¦ä¹ è·¯å¾„

### ç¬¬ 1 å¤©ï¼šReward åŸºç¡€

1. **é˜…è¯»** `è‡ªå®šä¹‰Rewardå®è·µæŒ‡å—.md` ç¬¬ 1-3 èŠ‚ï¼ˆ2 å°æ—¶ï¼‰
   - ç†è§£ Reward å‡½æ•°çš„ä½œç”¨
   - æŒæ¡ RewardManager è°ƒç”¨æµç¨‹
   - äº†è§£ 3 ç§ Reward ç±»å‹

2. **å®è·µ** æŸ¥çœ‹å†…ç½® Reward å®ç°
   ```bash
   # GSM8K Reward
   cat verl/utils/reward_score/gsm8k.py

   # MATH Reward
   cat verl/utils/reward_score/math_reward.py
   ```

3. **è¿è¡Œ** GSM8K è®­ç»ƒï¼Œæ·»åŠ  Reward æ—¥å¿—
   ```python
   # åœ¨ verl/trainer/ppo/reward.py çš„ RewardManager.__call__ ä¸­
   print(f"[Reward Debug] Batch size: {len(batch)}")
   print(f"  data_source: {batch['data_source'][0]}")
   print(f"  reward_model: {batch['reward_model'][0]}")
   print(f"  scores: {scores[:5]}")
   ```

### ç¬¬ 2 å¤©ï¼šå®ç°è‡ªå®šä¹‰ Reward

1. **é˜…è¯»** `è‡ªå®šä¹‰Rewardå®è·µæŒ‡å—.md` ç¬¬ 4-6 èŠ‚ï¼ˆ2 å°æ—¶ï¼‰
   - å­¦ä¹  10+ ä¸ªå®æˆ˜ç¤ºä¾‹
   - ç†è§£ Reward Shaping æŠ€å·§
   - æŒæ¡è°ƒè¯•æ–¹æ³•

2. **å®è·µ** å®ç°ä½ çš„ç¬¬ä¸€ä¸ª Reward
   ```python
   # ç¤ºä¾‹ï¼šç®€æ´æ€§å¥–åŠ±
   def brevity_reward(solution_str, ground_truth, target_length=100):
       length = len(solution_str)
       if length <= target_length:
           return 1.0
       else:
           return max(0.0, 1.0 - (length - target_length) / target_length)
   ```

3. **æµ‹è¯•** åœ¨å°æ•°æ®é›†ä¸Šæµ‹è¯•
   ```bash
   # åˆ›å»ºæµ‹è¯•æ•°æ®ï¼ˆ10 ä¸ªæ ·æœ¬ï¼‰
   python prepare_test_data.py

   # è¿è¡Œè®­ç»ƒ
   python3 -m verl.trainer.main_ppo \
       data.train_files=test_data.parquet \
       custom_reward_function.path=my_reward.py \
       trainer.total_epochs=1
   ```

### ç¬¬ 3 å¤©ï¼šé«˜çº§æŠ€å·§å’Œè°ƒä¼˜

1. **é˜…è¯»** `è‡ªå®šä¹‰Rewardå®è·µæŒ‡å—.md` ç¬¬ 7-9 èŠ‚ï¼ˆ2 å°æ—¶ï¼‰
   - å­¦ä¹ å¤šç›®æ ‡ Reward
   - æŒæ¡æ€§èƒ½ä¼˜åŒ–æŠ€å·§
   - ç†è§£æœ€ä½³å®è·µ

2. **å®è·µ** å¤šç›®æ ‡ Reward
   ```python
   def multi_objective_reward(solution_str, ground_truth):
       # ç›®æ ‡ 1ï¼šå‡†ç¡®æ€§ï¼ˆæƒé‡ 0.6ï¼‰
       accuracy = compute_accuracy(solution_str, ground_truth)

       # ç›®æ ‡ 2ï¼šç®€æ´æ€§ï¼ˆæƒé‡ 0.2ï¼‰
       brevity = compute_brevity(solution_str)

       # ç›®æ ‡ 3ï¼šå¯è¯»æ€§ï¼ˆæƒé‡ 0.2ï¼‰
       readability = compute_readability(solution_str)

       return 0.6 * accuracy + 0.2 * brevity + 0.2 * readability
   ```

3. **å¯¹æ¯”å®éªŒ** æµ‹è¯•ä¸åŒ Reward è®¾è®¡
   - Sparse Reward vs Dense Reward
   - Binary Reward vs Continuous Reward
   - Single Objective vs Multi-Objective

---

## ğŸ“‹ å­¦ä¹ æ£€æŸ¥æ¸…å•

### Reward åŸºç¡€ç†è§£ âœ“
- [ ] ç†è§£ Reward åœ¨ RL è®­ç»ƒä¸­çš„ä½œç”¨
- [ ] æŒæ¡ RewardManager è°ƒç”¨æµç¨‹
- [ ] äº†è§£ 3 ç§ Reward ç±»å‹çš„åŒºåˆ«
- [ ] ç†è§£ reward_model é…ç½®æ ¼å¼
- [ ] çŸ¥é“å¦‚ä½•æŸ¥çœ‹ Reward è®¡ç®—æ—¥å¿—

### è‡ªå®šä¹‰ Reward å®ç° âœ“
- [ ] å®ç°è¿‡ç®€å•çš„ Rule-based Reward
- [ ] ç†è§£ compute_score å‡½æ•°ç­¾å
- [ ] èƒ½å¤Ÿé…ç½® custom_reward_function
- [ ] çŸ¥é“å¦‚ä½•è°ƒè¯• Reward è®¡ç®—
- [ ] ç†è§£ Reward Shaping çš„ä½œç”¨

### é«˜çº§æŠ€å·§æŒæ¡ âœ“
- [ ] å®ç°è¿‡å¤šç›®æ ‡ Reward
- [ ] ç†è§£ Sparse vs Dense Reward
- [ ] æŒæ¡ Reward æ€§èƒ½ä¼˜åŒ–
- [ ] èƒ½å¤Ÿåˆ†æ Reward åˆ†å¸ƒ
- [ ] çŸ¥é“å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ³•

---

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬ç« åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

âœ… æ·±å…¥ç†è§£ Reward å‡½æ•°çš„è®¾è®¡åŸç†
âœ… ç†Ÿç»ƒå®ç°å„ç§ç±»å‹çš„è‡ªå®šä¹‰ Reward
âœ… æŒæ¡ Reward Shaping æŠ€å·§
âœ… èƒ½å¤Ÿè°ƒè¯•å’Œä¼˜åŒ– Reward è®¡ç®—
âœ… è®¾è®¡å¤šç›®æ ‡ Reward å‡½æ•°
âœ… åˆ†æ Reward å¯¹è®­ç»ƒçš„å½±å“

---

## ğŸ’¡ é‡ç‚¹å†…å®¹

### Reward å‡½æ•°ç­¾å

```python
def compute_score(
    data_source: str,        # æ•°æ®æ¥æºï¼ˆå¦‚ "gsm8k"ï¼‰
    solution_str: str,       # æ¨¡å‹ç”Ÿæˆçš„å®Œæ•´å“åº”
    ground_truth: str,       # æ­£ç¡®ç­”æ¡ˆï¼ˆä»æ•°æ®ä¸­è·å–ï¼‰
    extra_info: dict = None  # é¢å¤–ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
) -> float:                  # è¿”å› 0-1 ä¹‹é—´çš„åˆ†æ•°
    """
    è®¡ç®—å•ä¸ªå“åº”çš„å¥–åŠ±åˆ†æ•°
    """
    pass
```

### Reward é…ç½®çš„ 3 ä¸ª style

```yaml
# Style 1: rule-based
reward_model:
  style: "rule"
  module: "verl.utils.reward_score.gsm8k"
  function: "compute_score"  # å¯é€‰

# Style 2: model-based
reward_model:
  style: "model"
  path: "path/to/reward_model"
  model_type: "sequence_classification"

# Style 3: sandbox
reward_model:
  style: "sandbox"
  language: "python"
  test_cases: [...]
```

### Reward ç±»å‹å¯¹æ¯”

| ç±»å‹ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| **Rule-based** | å¿«é€Ÿã€å¯è§£é‡Š | éœ€è¦äººå·¥è®¾è®¡è§„åˆ™ | æ•°å­¦æ¨ç†ã€æ ¼å¼æ£€æŸ¥ |
| **Model-based** | è‡ªåŠ¨å­¦ä¹  | éœ€è¦è®­ç»ƒ Reward Model | å¯¹è¯è´¨é‡ã€æ–‡æœ¬ç”Ÿæˆ |
| **Sandbox** | å‡†ç¡®ï¼ˆå¯æ‰§è¡Œï¼‰ | æ…¢ã€éœ€è¦å®‰å…¨éš”ç¦» | ä»£ç ç”Ÿæˆ |

### Sparse vs Dense Reward

**Sparse Rewardï¼ˆç¨€ç–å¥–åŠ±ï¼‰ï¼š**
```python
# åªæœ‰æœ€ç»ˆç»“æœæœ‰å¥–åŠ±
def sparse_reward(solution_str, ground_truth):
    return 1.0 if solution_str == ground_truth else 0.0
```
âœ… ä¼˜ç‚¹ï¼šç®€å•ã€æ˜ç¡®
âŒ ç¼ºç‚¹ï¼šéš¾ä»¥å­¦ä¹ ã€æ ·æœ¬æ•ˆç‡ä½

**Dense Rewardï¼ˆå¯†é›†å¥–åŠ±ï¼‰ï¼š**
```python
# æ¯ä¸€æ­¥éƒ½æœ‰å¥–åŠ±
def dense_reward(solution_str, ground_truth):
    # æ­£ç¡®æ€§
    accuracy = compute_accuracy(solution_str, ground_truth)

    # ä¸­é—´æ­¥éª¤å¥–åŠ±
    step_rewards = 0.0
    if "Let's solve step by step" in solution_str:
        step_rewards += 0.1
    if "####" in solution_str:
        step_rewards += 0.1

    return accuracy * 0.8 + step_rewards * 0.2
```
âœ… ä¼˜ç‚¹ï¼šå®¹æ˜“å­¦ä¹ ã€æ ·æœ¬æ•ˆç‡é«˜
âŒ ç¼ºç‚¹ï¼šå¯èƒ½è¿‡æ‹Ÿåˆè§„åˆ™

---

## â“ å¸¸è§é—®é¢˜

### Q1: Reward åˆ†æ•°çš„èŒƒå›´æ˜¯ä»€ä¹ˆï¼Ÿ

**æ¨èèŒƒå›´ï¼š0-1**
```python
# å¥½çš„è®¾è®¡
return 1.0  # å®Œå…¨æ­£ç¡®
return 0.5  # éƒ¨åˆ†æ­£ç¡®
return 0.0  # å®Œå…¨é”™è¯¯

# é¿å…
return 100  # å¤ªå¤§ï¼Œå¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®š
return -1.0  # è´Ÿæ•°ï¼Œä¸æ¨èï¼ˆè™½ç„¶æŠ€æœ¯ä¸Šå¯è¡Œï¼‰
```

**ä¸ºä»€ä¹ˆ 0-1ï¼Ÿ**
- ä¾¿äºä¸åŒ Reward çš„å¯¹æ¯”
- é¿å…æ•°å€¼ä¸ç¨³å®š
- æ˜“äºç†è§£å’Œè°ƒè¯•

### Q2: å¦‚ä½•è®¾è®¡ Reward çš„ä¸­é—´å€¼ï¼Ÿ

**æ–¹æ³• 1ï¼šåˆ†çº§å¥–åŠ±**
```python
def graded_reward(solution_str, ground_truth):
    answer = extract_answer(solution_str)

    if answer == ground_truth:
        return 1.0  # å®Œå…¨æ­£ç¡®
    elif has_correct_format(solution_str):
        return 0.3  # æ ¼å¼æ­£ç¡®
    else:
        return 0.0  # å®Œå…¨é”™è¯¯
```

**æ–¹æ³• 2ï¼šè¿ç»­å¥–åŠ±**
```python
def continuous_reward(solution_str, ground_truth):
    # è®¡ç®—ç›¸ä¼¼åº¦
    similarity = compute_similarity(solution_str, ground_truth)
    return similarity  # 0-1 ä¹‹é—´çš„è¿ç»­å€¼
```

### Q3: Reward è®¡ç®—æ…¢æ€ä¹ˆåŠï¼Ÿ

**ä¼˜åŒ–æ–¹æ³• 1ï¼šæ‰¹é‡è®¡ç®—**
```python
# ä¸å¥½ï¼šé€ä¸ªè®¡ç®—
for solution in solutions:
    score = compute_score(solution, ground_truth)

# å¥½ï¼šæ‰¹é‡è®¡ç®—
scores = batch_compute_score(solutions, ground_truths)
```

**ä¼˜åŒ–æ–¹æ³• 2ï¼šç¼“å­˜ç»“æœ**
```python
from functools import lru_cache

@lru_cache(maxsize=10000)
def compute_score_cached(solution_str, ground_truth):
    return compute_score(solution_str, ground_truth)
```

**ä¼˜åŒ–æ–¹æ³• 3ï¼šå¹¶è¡Œè®¡ç®—**
```python
from multiprocessing import Pool

def compute_scores_parallel(solutions, ground_truths, n_workers=4):
    with Pool(n_workers) as pool:
        scores = pool.starmap(compute_score, zip(solutions, ground_truths))
    return scores
```

### Q4: Reward åˆ†å¸ƒä¸å‡è¡¡æ€ä¹ˆåŠï¼Ÿ

**é—®é¢˜ï¼š** å¤§éƒ¨åˆ†æ ·æœ¬éƒ½æ˜¯ 0 æˆ– 1

**è§£å†³æ–¹æ³• 1ï¼šReward Shaping**
```python
def shaped_reward(solution_str, ground_truth):
    # åŸå§‹ Rewardï¼ˆBinaryï¼‰
    raw_reward = 1.0 if solution_str == ground_truth else 0.0

    # Shaping: æ·»åŠ ä¸­é—´å¥–åŠ±
    length_reward = compute_length_reward(solution_str)
    format_reward = compute_format_reward(solution_str)

    # ç»„åˆ
    return raw_reward * 0.7 + length_reward * 0.15 + format_reward * 0.15
```

**è§£å†³æ–¹æ³• 2ï¼šReward Normalization**
```python
def normalized_reward(rewards):
    # æ ‡å‡†åŒ–åˆ° [-1, 1]
    mean = np.mean(rewards)
    std = np.std(rewards) + 1e-8
    return (rewards - mean) / std
```

### Q5: å¦‚ä½•éªŒè¯ Reward å‡½æ•°çš„æ­£ç¡®æ€§ï¼Ÿ

**æ­¥éª¤ 1ï¼šå•å…ƒæµ‹è¯•**
```python
def test_reward_function():
    # æµ‹è¯•æ­£ç¡®ç­”æ¡ˆ
    assert compute_score("#### 42", "42") == 1.0

    # æµ‹è¯•é”™è¯¯ç­”æ¡ˆ
    assert compute_score("#### 43", "42") == 0.0

    # æµ‹è¯•æ ¼å¼é”™è¯¯
    assert compute_score("42", "42") < 1.0
```

**æ­¥éª¤ 2ï¼šæ‰‹åŠ¨æ£€æŸ¥**
```python
# æ‰“å°å‰ 10 ä¸ªæ ·æœ¬çš„ Reward
for i in range(10):
    solution = solutions[i]
    ground_truth = ground_truths[i]
    score = compute_score(solution, ground_truth)
    print(f"Sample {i}:")
    print(f"  Solution: {solution[:50]}...")
    print(f"  Ground Truth: {ground_truth}")
    print(f"  Score: {score}")
```

**æ­¥éª¤ 3ï¼šç»Ÿè®¡åˆ†æ**
```python
import matplotlib.pyplot as plt

scores = [compute_score(s, gt) for s, gt in zip(solutions, ground_truths)]

plt.hist(scores, bins=20)
plt.xlabel("Reward Score")
plt.ylabel("Count")
plt.title("Reward Distribution")
plt.show()

print(f"Mean: {np.mean(scores):.4f}")
print(f"Std: {np.std(scores):.4f}")
print(f"Min: {np.min(scores):.4f}, Max: {np.max(scores):.4f}")
```

---

## ğŸ”— ç›¸å…³èµ„æº

### æœ¬åœ°æ–‡ä»¶
- è¯¦ç»†æ•™ç¨‹: `è‡ªå®šä¹‰Rewardå®è·µæŒ‡å—.md`
- Reward ç³»ç»Ÿæ·±åº¦è§£æ: `../02_æ•°æ®å‡†å¤‡/reward_ç³»ç»Ÿè¯¦è§£.md`
- é¡¹ç›®æ¦‚è§ˆ: `../../CLAUDE.md`
- å®Œæ•´å­¦ä¹ è·¯çº¿: `../../LEARNING_GUIDE.md`

### å®˜æ–¹æ–‡æ¡£
- [Reward Function](https://verl.readthedocs.io/en/latest/preparation/reward_function.html)
- [Prepare Data](https://verl.readthedocs.io/en/latest/preparation/prepare_data.html)

### ä»£ç ä½ç½®
- RewardManager: `verl/trainer/ppo/reward.py`
- å†…ç½® Reward: `verl/utils/reward_score/`
  - GSM8K: `gsm8k.py`
  - MATH: `math_reward.py`
  - Geo3K: `geo3k.py`
- æ•°æ®é¢„å¤„ç†ç¤ºä¾‹: `examples/data_preprocess/`

### è®ºæ–‡å‚è€ƒ
- [RLHF with Reward Models](https://arxiv.org/abs/2203.02155)
- [Reward Shaping](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf)

---

## â­ï¸ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬ç« åï¼Œç»§ç»­å­¦ä¹ ï¼š
- **05 - Agent RL**: å·¥å…·è°ƒç”¨å’Œå¤šè½®å¯¹è¯çš„ RL è®­ç»ƒ
- **è¿›é˜¶**: å®ç°æ›´å¤æ‚çš„ Reward Model

---

*åˆ›å»ºæ—¶é—´: 2026-01-26*
*é¢„è®¡å®Œæˆæ—¶é—´: 2-3 å¤©*
